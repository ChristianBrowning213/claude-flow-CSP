# PROJECT CONTEXT DUMP
# Root: F:\Github\CASCADE_CSP
# Generated: 2026-02-11 20:35:15
# Notes: Hidden paths and common junk/binaries are skipped. Sizes > max-size are skipped.
#
# Included files will follow with clear section headers.



================================================================================
=== FILE: docker-compose.yml ===
================================================================================

```yaml
services:
  # MLflow Tracking Server for observability
  mlflow:
    image: ghcr.io/mlflow/mlflow:v3.3.2
    container_name: cascade_mlflow
    network_mode: host  # Use host network for localhost access
    user: "${UID:-1000}:${GID:-1000}"  # Run as current user to match permissions
    environment:
      - MLFLOW_HOST=0.0.0.0
    volumes:
      # Mount local mlruns directory for persistence across rebuilds
      - ./conversational_system/mlruns:/mlruns
    # Use file-based backend (same as non-Docker mlflow ui) for consistent trace display
    command: mlflow server --host 0.0.0.0 --port 5001 --backend-store-uri file:///mlruns --default-artifact-root file:///mlruns
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:5001/')"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  # CASCADE Conversational System
  # NOTE: Neo4j is NOT included - uses your local Neo4j instance for data sharing
  cascade:
    build:
      context: .
      dockerfile: docker/Dockerfile
    container_name: cascade_app
    network_mode: host  # Use host network for localhost access to Neo4j
    user: "${UID:-1000}:${GID:-1000}"  # Run as current user to match permissions
    environment:
      # Home directory for user-writable cache (crawl4ai, etc.)
      - HOME=/app/.home
      # Playwright browsers are installed in cascade_user's home during build
      - PLAYWRIGHT_BROWSERS_PATH=/home/cascade_user/.cache/ms-playwright
      # API Keys
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - TAVILY_API_KEY=${TAVILY_API_KEY}
      - MP_API_KEY=${MP_API_KEY}
      # Supabase
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_SERVICE_KEY=${SUPABASE_SERVICE_KEY}
      - SUPABASE_DATABASE_URL=${SUPABASE_DATABASE_URL}
      # Neo4j
      - NEO4J_URI=${NEO4J_URI:-bolt://localhost:7687}
      - NEO4J_USER=${NEO4J_USER:-neo4j}
      - NEO4J_PASSWORD=${NEO4J_PASSWORD}
      # Memory configuration
      - MEM0_USE_LLM=${MEM0_USE_LLM:-true}
      - MEM0_LLM_MODEL=${MEM0_LLM_MODEL:-gpt-4o-mini}
      - ENABLE_GRAPH_MEMORY=${ENABLE_GRAPH_MEMORY:-true}
      # Knowledge graph for code parsing (requires Neo4j)
      - USE_KNOWLEDGE_GRAPH=${USE_KNOWLEDGE_GRAPH:-true}
      # Generate AI summaries for extracted code (increases API usage)
      - GENERATE_CODE_SUMMARY=${GENERATE_CODE_SUMMARY:-false}
      # Agent model configuration
      - AGENT_MODEL_NAME=${AGENT_MODEL_NAME:-o3}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL:-https://api.openai.com/v1}
      - USE_ADAPTIVE_AGENTS=${USE_ADAPTIVE_AGENTS:-false}
      - REQUIRE_OPENAI_API_KEY=${REQUIRE_OPENAI_API_KEY:-true}
      # Application paths
      - PROJECT_ROOT=${PROJECT_ROOT:-/app}
      - CODE_STORAGE_DIR=${CODE_STORAGE_DIR:-/app/temp_code}
      - SAVED_FILES_DIR=${SAVED_FILES_DIR:-/app/saved_code}
      # MLflow tracking
      - MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI:-http://localhost:5001}
      # Streamlit settings
      - STREAMLIT_BROWSER_GATHER_USAGE_STATS=false
      # Database path (mounted in /app/data for write access)
      - CONVERSATIONS_DB_PATH=/app/data/conversations.db
    volumes:
      # Mount local directories for persistence across rebuilds
      - ./conversational_system/conversations.db:/app/data/conversations.db
      - ./conversational_system/saved_code:/app/saved_code
      - ./conversational_system/mlruns:/mlruns  # Same path as MLflow server for trace data
    depends_on:
      mlflow:
        condition: service_healthy
    restart: unless-stopped

```


================================================================================
=== FILE: LICENSE ===
================================================================================

```
MIT License

Copyright (c) 2025 Xu Huang

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

```


================================================================================
=== FILE: project_context.txt ===
================================================================================

```

```


================================================================================
=== FILE: project_to_txt.py ===
================================================================================

```python
#!/usr/bin/env python3
from __future__ import annotations   # â† must be here (first statement)

import argparse, os, sys, time, io
from pathlib import Path
from typing import Optional

DEFAULT_EXCLUDED_DIRS = {
    ".git", ".hg", ".svn", ".idea", ".vscode", ".venv",
    "__pycache__", "node_modules", "dist", "build", ".cache",
    ".mypy_cache", ".pytest_cache", ".next", ".turbo", ".parcel-cache"
}
DEFAULT_EXCLUDED_EXTS = {
    # archives & binaries
    ".zip", ".gz", ".bz2", ".xz", ".7z", ".rar", ".tar",
    ".exe", ".dll", ".so", ".dylib", ".bin", ".dat", ".lock",
    # media
    ".png", ".jpg", ".jpeg", ".gif", ".webp", ".ico", ".svg",
    ".mp4", ".mov", ".avi", ".mkv", ".mp3", ".wav", ".flac",
    # docs likely huge or non-text
    ".pdf", ".psd", ".ai"
}
DEFAULT_EXCLUDED_FILES = {
    # huge / noisy lock or cache files (still text, but not helpful)
    "package-lock.json", "pnpm-lock.yaml", "yarn.lock",
    "poetry.lock", "pipfile.lock", ".DS_Store", "Thumbs.db"
}

LANG_BY_EXT = {
    ".py": "python", ".js": "javascript", ".ts": "typescript",
    ".tsx": "tsx", ".jsx": "jsx", ".json": "json", ".yml": "yaml",
    ".yaml": "yaml", ".md": "markdown", ".toml": "toml",
    ".ini": "", ".cfg": "", ".conf": "", ".txt": "",
    ".html": "html", ".css": "css", ".scss": "scss", ".sass": "sass",
    ".sh": "bash", ".ps1": "powershell", ".sql": "sql", ".xml": "xml",
    ".java": "java", ".c": "c", ".h": "c", ".cpp": "cpp", ".hpp": "cpp",
    ".rs": "rust", ".go": "go", ".rb": "ruby", ".php": "php",
    ".ipynb": "json"
}

def looks_binary(sample: bytes) -> bool:
    if b"\x00" in sample:
        return True
    # Heuristic: if >30% bytes are non-text-ish, treat as binary
    text_bytes = b"\t\n\r\f\b" + bytes(range(32, 127))
    if not sample:
        return False
    nontext = sum(ch not in text_bytes for ch in sample)
    return (nontext / len(sample)) > 0.30


def read_text_file(path: Path, max_bytes: int) -> Optional[str]:  # ðŸ‘ˆ change here
    try:
        with path.open("rb") as f:
            sample = f.read(min(max_bytes, 4096))
            if looks_binary(sample):
                return None
        with path.open("rb") as f:
            data = f.read(max_bytes)
        text = data.decode("utf-8", errors="replace")
        return text.replace("\r\n", "\n").replace("\r", "\n")
    except Exception:
        return None


def is_hidden(p: Path) -> bool:
    # Treat names starting with '.' as hidden (works cross-platform)
    return any(part.startswith(".") and part not in {".", ".."} for part in p.parts)

def should_skip_file(p: Path, args, rel: Path) -> bool:
    name = p.name
    if not args.include_dotfiles and (name.startswith(".") or is_hidden(rel)):
        return True
    if name in args.exclude_files or name in DEFAULT_EXCLUDED_FILES:
        return True
    ext = p.suffix.lower()
    if ext in DEFAULT_EXCLUDED_EXTS or ext in args.exclude_exts:
        return True
    if args.include_exts and ext not in args.include_exts:
        return True
    try:
        size = p.stat().st_size
        if size > args.max_size_mb * 1024 * 1024:
            return True
    except Exception:
        return True
    return False

def should_skip_dir(dirpath: Path, args, rel: Path) -> bool:
    name = dirpath.name
    if not args.include_dotfiles and (name.startswith(".") or is_hidden(rel)):
        return True
    if name in DEFAULT_EXCLUDED_DIRS or name in args.exclude_dirs:
        return True
    return False

def main():
    ap = argparse.ArgumentParser(
        description="Concatenate a projectâ€™s text source into one TXT for agent context."
    )
    ap.add_argument("root", help="Project root directory.")
    ap.add_argument("-o", "--output", default="project_context.txt",
                    help="Output TXT path (default: project_context.txt)")
    ap.add_argument("--max-size-mb", type=int, default=2,
                    help="Skip files larger than this many MB (default: 2)")
    ap.add_argument("--include-dotfiles", action="store_true",
                    help="Include dotfiles and hidden paths (default: skip)")
    ap.add_argument("--include-exts", default="", help="Comma-separated whitelist of extensions (e.g., .py,.md)")
    ap.add_argument("--exclude-exts", default="", help="Comma-separated extra excluded extensions (e.g., .log,.csv)")
    ap.add_argument("--exclude-dirs", default="",
                    help="Comma-separated extra excluded dir names (exact match)")
    ap.add_argument("--exclude-files", default="",
                    help="Comma-separated extra excluded file names (exact match)")
    ap.add_argument("--no-fences", action="store_true",
                    help="Do not wrap contents in Markdown code fences.")
    args = ap.parse_args()

    root = Path(args.root).resolve()
    if not root.exists() or not root.is_dir():
        print(f"Error: {root} is not a directory.", file=sys.stderr)
        sys.exit(1)

    # Parse lists
    args.include_exts = {e.strip().lower() for e in args.include_exts.split(",") if e.strip()} if args.include_exts else set()
    args.exclude_exts = {e.strip().lower() for e in args.exclude_exts.split(",") if e.strip()} if args.exclude_exts else set()
    args.exclude_dirs = {d.strip() for d in args.exclude_dirs.split(",") if d.strip()} if args.exclude_dirs else set()
    args.exclude_files = {f.strip() for f in args.exclude_files.split(",") if f.strip()} if args.exclude_files else set()

    included_files = []
    skipped = {"dir": 0, "hidden": 0, "size": 0, "binary": 0, "ext": 0, "other": 0}

    out_path = Path(args.output).resolve()
    out_path.parent.mkdir(parents=True, exist_ok=True)

    with io.open(out_path, "w", encoding="utf-8", newline="\n") as out:
        # Header / manifest preface
        ts = time.strftime("%Y-%m-%d %H:%M:%S")
        out.write(f"# PROJECT CONTEXT DUMP\n")
        out.write(f"# Root: {root}\n# Generated: {ts}\n")
        out.write("# Notes: Hidden paths and common junk/binaries are skipped. Sizes > max-size are skipped.\n")
        out.write("#\n# Included files will follow with clear section headers.\n\n")

        # Walk
        for dirpath, dirnames, filenames in os.walk(root):
            dirpath = Path(dirpath)
            rel_dir = dirpath.relative_to(root)

            # Filter directories in-place (os.walk respects modifications)
            keep_dirs = []
            for d in dirnames:
                dpath = dirpath / d
                if should_skip_dir(dpath, args, rel_dir / d):
                    skipped["dir"] += 1
                    continue
                keep_dirs.append(d)
            dirnames[:] = keep_dirs

            # Files
            for fn in filenames:
                fpath = dirpath / fn
                rel = fpath.relative_to(root)
                if should_skip_file(fpath, args, rel):
                    # heuristic reason counting
                    name = fpath.name
                    if not args.include_dotfiles and (name.startswith(".") or is_hidden(rel)):
                        skipped["hidden"] += 1
                    elif fpath.suffix.lower() in DEFAULT_EXCLUDED_EXTS or fpath.suffix.lower() in args.exclude_exts:
                        skipped["ext"] += 1
                    else:
                        try:
                            if fpath.stat().st_size > args.max_size_mb * 1024 * 1024:
                                skipped["size"] += 1
                            else:
                                skipped["other"] += 1
                        except Exception:
                            skipped["other"] += 1
                    continue

                # Read and binary check
                with fpath.open("rb") as fb:
                    sample = fb.read(4096)
                if looks_binary(sample):
                    skipped["binary"] += 1
                    continue

                text = read_text_file(fpath, max_bytes=args.max_size_mb * 1024 * 1024)
                if text is None:
                    skipped["other"] += 1
                    continue

                included_files.append(str(rel))

                # Write section
                lang = LANG_BY_EXT.get(fpath.suffix.lower(), "")
                out.write("\n\n" + "=" * 80 + "\n")
                out.write(f"=== FILE: {rel} ===\n")
                out.write("=" * 80 + "\n\n")
                if args.no_fences:
                    out.write(text)
                else:
                    fence = lang if lang is not None else ""
                    out.write(f"```{fence}\n{text}\n```\n")

        # Manifest footer
        out.write("\n\n" + "#" * 80 + "\n")
        out.write("# MANIFEST\n")
        out.write("# Included files:\n")
        for p in included_files:
            out.write(f"#  - {p}\n")
        out.write("#\n# Skips summary:\n")
        for k, v in skipped.items():
            out.write(f"#  {k}: {v}\n")
        out.write("# END\n")

    print(f"Done. Wrote: {out_path}")
    print(f"Included files: {len(included_files)} | Skips: {skipped}")
    return 0

if __name__ == "__main__":
    sys.exit(main())

```


================================================================================
=== FILE: README.md ===
================================================================================

```markdown
# CASCADE

CASCADE (Cumulative Agentic Skill Creation through Autonomous Development and Evolution) is a multi-agent system for materials science and chemistry research.

## Quick Start (Docker)

The fastest way to get started is using Docker.

### Prerequisites

- [Docker](https://docs.docker.com/get-docker/) and [Docker Compose](https://docs.docker.com/compose/install/)
- [Git](https://git-scm.com/downloads)
- A [Supabase](https://supabase.com) account
- Neo4j database (local installation)

### 1. Clone the Repository

```bash
git clone https://github.com/CederGroupHub/CASCADE.git
cd CASCADE
```

### 2. Set Up Supabase

1. Go to [supabase.com](https://supabase.com) and create a new project
2. In your project dashboard, go to **Project Settings**:
   - **Data API** â†’ Copy **Project URL** â†’ `SUPABASE_URL`
   - **API Keys** â†’ Click **Legacy anon, service_role API keys** â†’ Click **Reveal** on service_role â†’ `SUPABASE_SERVICE_KEY`
   - Click **Connect** (top center) â†’ Change Method to **Session pooler** â†’ Copy and replace `[YOUR-PASSWORD]` with your password â†’ `SUPABASE_DATABASE_URL`
3. Go to **SQL Editor** and run the schema from `mcp_servers_and_tools/research_server/extracted_code.sql`

*Note: Supabase UI may change over time. Look for API keys and connection strings in Project Settings. Free-tier Supabase projects may be paused after inactivity - check your project status and resume if needed before running the system.*

### 3. Set Up Neo4j

#### Option A: Neo4j Desktop 
1. Download from [neo4j.com/download](https://neo4j.com/download/)
2. Create a new project and database
3. Set a password and start the database

#### Option B: Command Line (Linux)

**Prerequisite:** Java 21 is required

```bash
# Download and extract
wget https://dist.neo4j.org/neo4j-community-2025.08.0-unix.tar.gz
tar -xzf neo4j-community-2025.08.0-unix.tar.gz
mv neo4j-community-2025.08.0 ~/neo4j

# Configure to bind to localhost only
echo "server.bolt.listen_address=localhost:7687" >> ~/neo4j/conf/neo4j.conf
echo "server.http.listen_address=localhost:7474" >> ~/neo4j/conf/neo4j.conf

# Start Neo4j (JAVA_HOME path may vary on your system)
# Find yours: find /usr/lib/jvm -name "java-21*" -type d 2>/dev/null | head -1
# Tip: Add "export JAVA_HOME=/usr/lib/jvm/java-21-openjdk-amd64" to ~/.bashrc
~/neo4j/bin/neo4j start

# Set initial password (default password is 'neo4j', then set your new password)
~/neo4j/bin/cypher-shell -u neo4j -p neo4j
```

### 4. Configure Environment Variables

```bash
cp .env.example .env
nano .env  # Fill in your API keys and credentials from steps 2-3

# Set your user ID for Docker (required for correct file permissions)
echo "UID=$(id -u)" >> .env
echo "GID=$(id -g)" >> .env
```

*Priority: `.env` file takes precedence. If a variable is not set in `.env`, it falls back to shell environment variables.*

### 5. Start the Services

```bash
# Make sure Supabase and Neo4j are running first. For Neo4j on Linux:
~/neo4j/bin/neo4j status

# Create the SQLite database file if it doesn't exist (prevents Docker from mounting it as a directory)
touch conversational_system/conversations.db

# Start CASCADE with Docker (open cascade.log to view real-time output)
docker compose up > cascade.log 2>&1 &
```

### 6. Access the Application

- **CASCADE UI**: http://localhost:8501
- **MLflow UI**: http://localhost:5001 (traces and experiments)
- **Neo4j Browser**: http://localhost:7474 (optional)

**Remote access:** If running on a remote server, set up SSH tunnels first:
```bash
ssh -f -N -L 8501:localhost:8501 -L 5001:localhost:5001 -L 7474:localhost:7474 username@your_remote_server
```
Then access the URLs above on your local machine.

---

## Using Local Models (vLLM)

CASCADE supports local models via vLLM or other OpenAI-compatible servers.

### Environment Variables

Add these to your `.env` file:

```bash
OPENAI_BASE_URL=http://localhost:8000/v1
AGENT_MODEL_NAME=your-model-name
REQUIRE_OPENAI_API_KEY=false
USE_ADAPTIVE_AGENTS=true
```

### Setup Guide

For vLLM deployment, SDK modifications, and detailed configuration, see [Adaptive Agents Documentation](conversational_system/ADAPTIVE_AGENTS.md).

---

## Management Commands

```bash
# Stop services
docker compose down

# Rebuild after code changes
docker compose up --build > cascade.log 2>&1 &
```

---

## Data Persistence

Data is stored on your local filesystem and persists across container restarts:
- `./conversational_system/conversations.db` - Conversation history
- `./conversational_system/saved_code/` - Saved code files
- `./conversational_system/mlruns/` - MLflow tracking data

Neo4j data is stored in your local Neo4j installation.

To reset all data, manually delete these files/directories.

---

## Project Structure

```
CASCADE/
â”œâ”€â”€ conversational_system/           # Main conversational agent system
â”‚   â”œâ”€â”€ frontend/                    # Streamlit web interface
â”‚   â”œâ”€â”€ core/                        # Orchestrator and DeepSolver agents
â”‚   â”œâ”€â”€ deep_solver_with_memory/     # Multi-agent workflow (4-agent architecture)
â”‚   â”œâ”€â”€ launch.sh                    # Launch script for local development
â”‚   â”œâ”€â”€ conversations.db             # SQLite database (created after first conversation)
â”‚   â”œâ”€â”€ ADAPTIVE_AGENTS.md           # vLLM and non-OpenAI model configuration
â”‚   â””â”€â”€ MLFLOW_TRACING.md            # MLflow tracing guide
â”œâ”€â”€ deep_solver_benchmark/           # Benchmark suite and testing
â”‚   â”œâ”€â”€ deep_solver/                 # Main agent implementations
â”‚   â”œâ”€â”€ deep_solver_free_form/       # Free-form output variant
â”‚   â”œâ”€â”€ baselines/                   # Baseline implementations (including Claude Code)
â”‚   â”œâ”€â”€ ablation_studies/            # Ablation study variants
â”‚   â”œâ”€â”€ data_for_demonstration/      # Data files for free-form demonstrations
â”‚   â”œâ”€â”€ README.md                    # Benchmark overview and quick start
â”‚   â”œâ”€â”€ DEVELOPMENT.md               # Local development and benchmark guide
â”‚   â””â”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ mcp_servers_and_tools/           # MCP servers and direct tools
â”‚   â”œâ”€â”€ research_server/             # Code extraction and knowledge graph
â”‚   â”œâ”€â”€ memory_server/               # Memory consolidation
â”‚   â”œâ”€â”€ workspace_server/            # Code execution environment
â”‚   â””â”€â”€ direct_tools/                # Direct tool implementations (non-MCP)
â”œâ”€â”€ benchmark_tasks_and_results/     # Benchmark questions, answers, and results
â”‚   â”œâ”€â”€ questions_and_answers/       # Benchmark JSON files (download required)
â”‚   â”œâ”€â”€ demonstration/               # Free-form demonstration examples
â”‚   â””â”€â”€ evaluation/                  # Evaluation scripts
â”œâ”€â”€ docker/                          # Docker configuration
â”œâ”€â”€ utils/                           # Shared utilities
â””â”€â”€ .env.example                     # Environment variable template
```

*Note: Benchmark data files require separate download. See [benchmark_tasks_and_results/README.md](benchmark_tasks_and_results/README.md) for instructions.*

---

## Additional Documentation

| Document | Description |
|----------|-------------|
| [Development Guide](deep_solver_benchmark/DEVELOPMENT.md) | Local development setup (No Docker) for conversational system and benchmarks |
| [Adaptive Agents](conversational_system/ADAPTIVE_AGENTS.md) | Using vLLM and non-OpenAI models |
| [Benchmark README](deep_solver_benchmark/README.md) | Benchmark structure and variants |

```


================================================================================
=== FILE: conversational_system\ADAPTIVE_AGENTS.md ===
================================================================================

```markdown
# Adaptive Agents for vLLM Models

This guide explains how to use adaptive agents with vLLM models like Qwen3-Coder-30B-A3B-Instruct-FP8.

## Required SDK Modification

**Note**: This modification is compatible with both vLLM and OpenAI models. You don't need to revert these changes when switching back to OpenAI models.

### 1. Locate and Modify the SDK File

File: `.venv/lib/python3.12/site-packages/agents/_run_impl.py`

Find the `execute_tools_and_side_effects` method, locate the elif branch around line 348 (openai-agents==0.2.8), and replace the whole elif branch with:

```python
    elif (
        not output_schema or output_schema.is_plain_text()
    ) and not processed_response.has_tools_or_approvals_to_run():
        # Check if agent has a target_output_type to inject
        if hasattr(agent, 'target_output_type') and agent.output_type is None:
            # Agent started with output_type=None and now has target to set
            agent.output_type = agent.target_output_type
            
            if agent.target_output_type is not None:
                # Has structured output target, run again to get output_schema
                return SingleStepResult(
                    original_input=original_input,
                    model_response=new_response,
                    pre_step_items=pre_step_items,
                    new_step_items=new_step_items,
                    next_step=NextStepRunAgain(),
                )
        
        # Normal path or target_output_type is None
        return await cls.execute_final_output(
            agent=agent,
            original_input=original_input,
            new_response=new_response,
            pre_step_items=pre_step_items,
            new_step_items=new_step_items,
            final_output=potential_final_output_text or "",
            hooks=hooks,
            context_wrapper=context_wrapper,
        )
```

## 2. Start vLLM Server

For example: 
```bash
source .venv/bin/activate
nohup env CUDA_VISIBLE_DEVICES=0,1,2,3 VLLM_USE_DEEP_GEMM=1 vllm serve open_source_models/Qwen3-Coder-30B-A3B-Instruct-FP8 \
    --max-model-len 262144 \
    --enable-expert-parallel \
    --data-parallel-size 4 \
    --enable-auto-tool-choice \
    --tool-call-parser qwen3_coder \
    --served-model-name Qwen3-Coder-30B-A3B-Instruct-FP8 \
    --port 8000 \
    --host 127.0.0.1 > vllm_qwen3_coder_30b.log 2>&1 &
```

## 3. Set Environment Variables

Set these environment variables to use adaptive agents with vLLM:

```bash
export USE_ADAPTIVE_AGENTS=true
export OPENAI_BASE_URL=http://localhost:8000/v1
export AGENT_MODEL_NAME=Qwen3-Coder-30B-A3B-Instruct-FP8
export REQUIRE_OPENAI_API_KEY=false
```

Or add them to your `.env` file in the project root:

```bash
USE_ADAPTIVE_AGENTS=true
OPENAI_BASE_URL=http://localhost:8000/v1
AGENT_MODEL_NAME=Qwen3-Coder-30B-A3B-Instruct-FP8
REQUIRE_OPENAI_API_KEY=false
```

## 4. Usage

After setting the environment variables above, all CASCADE components will automatically use adaptive agents with vLLM.

For running benchmarks or tests, see [DEVELOPMENT.md](../deep_solver_benchmark/DEVELOPMENT.md).
```


================================================================================
=== FILE: conversational_system\launch.sh ===
================================================================================

```bash
#!/bin/bash

# Launch script for Conversational Materials Science Assistant
# This script activates the environment and launches the Streamlit app

# Get the workspace root directory
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
WORKSPACE_ROOT="$(dirname "$SCRIPT_DIR")"

echo "ðŸš€ Launching Conversational Materials Science Assistant..."
echo "ðŸ“ Workspace: $WORKSPACE_ROOT"

# Check if virtual environment exists
VENV_PATH="$WORKSPACE_ROOT/.venv"
if [ ! -d "$VENV_PATH" ]; then
    echo "âŒ Virtual environment not found at $VENV_PATH"
    echo "Please create it first: python -m venv .venv"
    exit 1
fi

# Activate virtual environment
echo "ðŸ”§ Activating virtual environment..."
source "$VENV_PATH/bin/activate"

# Check if required environment variables are set
if [ -z "$OPENAI_API_KEY" ]; then
    echo "âš ï¸  Warning: OPENAI_API_KEY not set"
    echo "   Please set it in your .env file"
fi

if [ -z "$SUPABASE_URL" ]; then
    echo "âš ï¸  Warning: SUPABASE_URL not set"
    echo "   Please set it in your .env file"
fi

# Change to the conversational_system directory
cd "$SCRIPT_DIR"

# Launch Streamlit
echo "ðŸŒ Starting Streamlit app..."
echo "   Open your browser at: http://localhost:8501"
echo ""
echo "Press Ctrl+C to stop the server"
echo ""

streamlit run frontend/streamlit_app.py

```


================================================================================
=== FILE: conversational_system\MLFLOW_TRACING.md ===
================================================================================

```markdown
# MLflow Tracing Guide

This system has integrated MLflow tracing to automatically record the complete agent execution process, including:
- Orchestrator tool calls (search_memory, solve_with_deep_solver, execute_code, etc.)
- DeepSolver internal 4-agent workflow (Solution Researcher, Code Agent, Debug Agents, Output Processor)
- Each agent's inputs, outputs, tool calls, and execution time
- Complete hierarchy of LLM calls and tool usage

## How to View Traces

### 1. Launch the System

**Option A: Using Docker (Recommended)**

```bash
docker compose up -d
```

This automatically starts:
- **Streamlit frontend** (port 8501): Web interface for user interaction
- **MLflow tracking server** (port 5001): Tracing and experiment tracking UI

**Option B: Manual Setup**

1. First, start the MLflow tracking server:

```bash
nohup mlflow server --host 127.0.0.1 --port 5001 --backend-store-uri ./mlruns --default-artifact-root ./mlruns > mlflow.log 2>&1 &
```

2. Then, launch the Streamlit app:

```bash
cd conversational_system
./launch.sh
```

**What happens:**
1. The Streamlit app launches in your browser (or navigate to http://localhost:8501)
2. Register a new account or log in with existing credentials
3. Start asking questions - each conversation is automatically traced
4. MLflow tracing runs in the background, recording all agent executions
5. **After each conversation turn**, check the terminal output and you'll see a direct link to the trace

**View Traces Anytime:**
- Click the trace URL printed in the terminal for immediate access to that specific trace
- Access the MLflow UI at http://localhost:5001 to browse all traces, compare runs, or review historical traces

**Security Note:** Both servers bind to localhost (127.0.0.1) by default. If running on a remote server, use SSH tunnels:

```bash
# Tunnel for Streamlit frontend and MLflow UI
ssh -f -N -L 8501:localhost:8501 -L 5001:localhost:5001 username@your_remote_server_ip
```

### 2. Navigate to Experiment

1. In the MLflow UI left sidebar, find and click the **`conversational_system`** experiment
2. You will see a list of all runs, each run corresponds to one conversation turn

### 3. View Complete Agent Trace

Each conversation produces multiple traces:
- **Individual API calls** (embedding/completion): Single span, shorter execution time
- **Complete agent trace**: Multiple spans, contains full tool call hierarchy â­

**How to quickly find the complete agent trace:**

1. Click on a run to enter the run details page
2. Switch to the **`Traces`** tab
3. **Sort**: Click the `Execution Time` column header, select **descending order** (longest execution time on top)
4. **Identify the correct trace**: Find the trace whose Request starts with `[SYSTEM: user_id=...`
   - This is the agent trace containing the complete conversation logic
   - Usually has the longest execution time and most spans
5. Click to enter the trace and view the complete execution hierarchy

### 4. Trace Content Explanation

The complete agent trace contains the following information:

**Orchestrator Level:**
- `AgentRunner.run` - Top-level agent execution
- `MaterialsScienceAssistant` - Orchestrator agent
- Tool call spans:
  - `search_memory` - Memory search (input: query + user_id, output: relevant memories)
  - `solve_with_deep_solver` - Call DeepSolver (if used)
  - `check_installed_packages`, `install_dependencies`, `execute_code` - Direct execution (if used)
  - `save_to_memory` - Save successful solution (if user satisfied)

**DeepSolver Internal Levels (if solve_with_deep_solver was used):**
- `SolutionResearcherAgent` - Phase 1: Research
- `CodeExecutionAgent` - Phase 2: Code execution
- `DebugAgent1/2/3` - Phase 3: Parallel debugging (if needed)
- `OutputProcessorAgent` - Phase 4: Output processing

Each span contains:
- **Inputs**: Tool input parameters
- **Outputs**: Tool return results
- **Execution time**: Time taken

```


================================================================================
=== FILE: conversational_system\__init__.py ===
================================================================================

```python
"""
Conversational System with Memory Integration

A multi-turn conversational agent system that integrates:
- Orchestrator: Main conversation controller
- DeepSolver: Deep problem-solving workflow (wraps 4-agent system)
- Memory: User preferences and successful solutions storage
"""

__version__ = "1.0.0"

```


================================================================================
=== FILE: conversational_system\config\prompts.py ===
================================================================================

```python
"""
Prompts for Conversational System Agents

Contains system prompts for:
- Orchestrator: Main conversation controller
- DeepSolver: Deep problem-solving workflow
"""

from datetime import datetime


def get_orchestrator_prompt() -> str:
    """
    Generate Orchestrator prompt with current date.

    Returns:
        Orchestrator system prompt string
    """
    return f"""
ROLE: Materials Science and Chemistry Research Assistant (Orchestrator)

You are an intelligent assistant specialized in materials science and chemistry.
You help users solve computational problems in materials science, chemistry, and related fields efficiently.

Current date: {datetime.now().strftime("%Y-%m-%d")}

# USER_ID AND QUERY HANDLING (CRITICAL)

IMPORTANT: Every message you receive will start with lines in this format:
[SYSTEM: user_id=xxx]
[CRITICAL REMINDER: Your FIRST action must be to call search_memory(query, user_id). After reviewing memory results, call solve_with_deep_solver(query, user_id) unless the problem is simple OR you have relevant memories and feel confident you can adapt them.]
[ORIGINAL_QUERY: yyy]  (present for improvements, continue, or save requests)

**user_id handling:**
- Required for: search_memory(query, user_id), solve_with_deep_solver(query, user_id), save_to_memory(content, user_id)
- Extract from "[SYSTEM: user_id=xxx]" at the start of each message
- Pass this user_id to the tools mentioned above

**CRITICAL REMINDER handling:**
- This reminder contains TWO mandatory steps that you MUST follow
- Step 1: ALWAYS call search_memory(query, user_id) as your FIRST action - this is non-negotiable
- Step 2: After reviewing memory results, choose PATH A or PATH B
- Do NOT skip search_memory - it is required for every single query
- Do NOT include this reminder when saving to memory - it's for your decision-making only

**ORIGINAL_QUERY handling:**
- Present in these scenarios:
  1. User requested improvements (ðŸ”§): Original query unchanged
  2. User added details (âž• Continue): Original query WITH appended details (separated by "; ")
  3. User clicked save (âœ…): Original query (possibly with appended details from Continue)
- When saving to memory (user satisfied), use this ORIGINAL_QUERY as the "User Query" field
- If not present, the current user message is the original query
- For Continue scenarios: ORIGINAL_QUERY may contain multiple parts separated by "; " (original question + supplementary details)

The actual user query/request starts AFTER these system lines.

# WORKFLOW (CRITICAL)

## STEP 1: UNDERSTAND USER NEEDS
- Handle user_id as stated above in the USER_ID HANDLING section
- Analyze the user's question and requirements
- **IMPORTANT**: If the user's question appears unclear, ambiguous, or potentially problematic, politely ask for clarification before proceeding. It's better to confirm understanding than to solve the wrong problem
- **ALSO IMPORTANT**: If you discover any issues, inconsistencies, or concerns during the problem-solving process (at any step), feel free to pause and ask the user for confirmation or clarification. Don't hesitate to communicate with the user when needed

## STEP 2: SEARCH MEMORY (MANDATORY - DO NOT SKIP)
- **CRITICAL**: Your FIRST action MUST be calling search_memory(query, user_id)
- This is mandatory for EVERY single query without exception
- Look for:
  * User's saved API keys and preferences
  * Similar problems and their working solutions
  * Relevant approaches and classes/functions/methods/tools/packages/software/etc.
- **DO NOT proceed to STEP 3 until you have called search_memory**

## STEP 3: CHECK QUERY COMPLETENESS
- Common missing info: API keys, specific parameters, tool/package/software preferences
- Call search_memory tool to check if you have relevant memories and if the info is indeed missing. For example, in your memory, you may get the memory that uses os.getenv("MP_API_KEY") to get the Materials Project API key, and in this way you should NOT ask users to provide the API key
- Stop here and output directly to ask politely for clarification only if the info is indeed missing
- **IMPORTANT**: If you found relevant memories with specific tools/packages/software but the user did NOT explicitly specify which tools to use, briefly confirm with the user first (e.g., "I found we previously solved similar problems using [tool/package]. Would you like me to use the same approach?"). Wait for user confirmation before proceeding.

## STEP 4: INTELLIGENT PATH SELECTION

**PREREQUISITE**: You MUST have already called search_memory in STEP 2. If you haven't, go back and call it now.

**DEFAULT: Use PATH B for most problems using solve_with_deep_solver tool.** Only use PATH A if the problem is simple and you feel very confident that you can solve it without any research, OR you have relevant memory and feel confident you can adapt them.

### PATH B - Deep Solver (DEFAULT - Use this for most problems):
**When to use**
- Memory has no similar solutions (MOST COMMON CASE)
- **Problem requires online search/deep research**
- **Problem needs investigation** (exploring packages, libraries, methods, or unfamiliar domains)
- You're unsure about the approach
- User requested improvements or deep analysis or iterations
- **When in doubt, ALWAYS use PATH B**

**Steps**: Call solve_with_deep_solver(query, user_id) â†’ Full research-code-debug workflow with online search capabilities

**IMPORTANT**: When formulating the query for solve_with_deep_solver:
- If the problem only needs research/search without code execution, add "No code or debugging needed. Just answer this question with online search and research."
- Most of the time, the problem needs code solution to give the user the right answer

**Why PATH B**: The DeepSolver has dedicated research agents with access to online search, code extraction from documentation, and iterative debugging capabilities that you don't have direct access to.

### PATH A - Quick Solution (Only for very simple cases):
**When to use**:
- The problem is very simple/straightforward
- Memory has relevant working solutions and you feel 100% confident you can adapt them without research
- **NO need for online search or documentation lookup**

**Steps**:
1. Adapt code from memory (or write simple code if no memory)
2. Check/install packages: check_installed_packages â†’ install_dependencies
3. Execute: execute_code
4. **CRITICAL - Evaluate result**:
   - âœ… Success â†’ move to STEP 5
   - âŒ Code execution failed â†’ **IMMEDIATELY use PATH B** (do NOT try to fix unless you are 100% confident you can fix it at once, do NOT output error to user)

**âš ï¸ CRITICAL RULES**:
- If execute_code fails, IMMEDIATELY switch to PATH B - do NOT try to debug yourself unless you are 100% confident you can fix it at once
- Do NOT output errors directly to users - use PATH B to get a working solution
- **If you need to search for documentation, examples, or any information online â†’ use PATH B**
- **When in doubt between PATH A and PATH B, ALWAYS use PATH B**

## STEP 5: PRESENT RESULTS
- Explain the solution in natural language to clearly answer the user's query
- Show the code clearly
- Present execution results
- If what you received from the DeepSolver is not enough to answer the user's query (e.g., the deep solver failed), you should just admit the failure and tell the user that here is the current code with some errors, and give some potential reasons for the failure and your suggestions for the next steps. You should not try to fix the errors
- Ask user to provide feedback

## STEP 6: HANDLE USER FEEDBACK
User has four options:

### Option 1: âœ… Satisfied
When the user is satisfied with the solution, you MUST call save_to_memory to preserve this successful solution for future reference.

**CRITICAL - Content Format for save_to_memory:**

The content parameter should be a well-structured string containing ALL of the following:
1. **User Query**: The ORIGINAL user question (even if there were improvement iterations, save the initial problem)
2. **Solution Code**: The final working code
3. **Execution Results**: The actual execution output (not just "success", but the real output)
4. **Explanation**: Clear explanation of how the solution works

**How to extract this information:**

**If you used PATH B (DeepSolver):**
- DeepSolver returns a dict with: original_user_query, final_code, execution_results, explanation
- Use ALL these fields to construct the content string

**If you used PATH A (Quick Solution):**
- User Query: Check if [ORIGINAL_QUERY: xxx] was in the message; if yes, use that; if not, extract the user's question from the CURRENT problem-solving session (the question that led to this solution you're about to save, NOT old questions from earlier in conversation history)
- Solution Code: The code you wrote and executed
- Execution Results: The output from execute_code tool
- Explanation: Your own explanation of the solution

**Content format example:**
```
User Query: [The original question from the user]

Solution Code:
[The complete working code]

Execution Results:
[The actual output when running the code]

Explanation:
[Clear explanation of the solution approach and results]
```

**Important notes:**
- Even if the user requested improvements, save the ORIGINAL query, not the improvement requests
- The code and results should be from the FINAL version that satisfied the user
- Include the actual execution output, not just "success"
- After saving, tell the user you've saved the solution and ask if they need help with anything else

### Option 2: ðŸ”§ Needs Improvement
- Listen carefully to user's specific concerns
- Understand what needs to be changed
- Re-solve with improvements (prefer PATH B for reliability unless it's a trivial fix)
- Can reference previous solution code from conversation history and improve it
- ORIGINAL_QUERY remains unchanged (improvements don't modify the core question)

### Option 3: âž• Continue (Add Details)
- User wants to add supplementary information to the original question
- The additional details are appended to ORIGINAL_QUERY (separated by "; ")
- Re-solve the problem with the updated, more complete query
- This is for refining requirements, not changing the core problem
- Each Continue adds more context: [original question] â†’ [original question; additional detail 1] â†’ [original question; additional detail 1; additional detail 2]

### Option 4: âŒ Exit
- User chose not to save the current solution
- Acknowledge gracefully without saving to memory
- Ask if they have other questions or need help with something else
- Be ready to help with a new problem

# CONVERSATION STYLE
- Be concise but thorough
- Explain technical concepts clearly
- Show enthusiasm for materials science and chemistry
- Be patient with clarifications
- Acknowledge when you're uncertain
- Celebrate successful solutions

Start each conversation by understanding the user's needs and searching memory for relevant context.
"""


# Keep a default instance for backward compatibility, but it will have static timestamp
ORCHESTRATOR_PROMPT = get_orchestrator_prompt()


# Export prompts
__all__ = [
    'get_orchestrator_prompt',
    'ORCHESTRATOR_PROMPT',  # Backward compatibility (static timestamp)
]

```


================================================================================
=== FILE: conversational_system\config\__init__.py ===
================================================================================

```python
"""
Configuration for conversational system.
"""

from .prompts import get_orchestrator_prompt, ORCHESTRATOR_PROMPT

__all__ = [
    'get_orchestrator_prompt',
    'ORCHESTRATOR_PROMPT',
]

```


================================================================================
=== FILE: conversational_system\core\deep_solver.py ===
================================================================================

```python
"""
DeepSolver - Deep Problem-Solving Workflow Wrapper

Wraps the existing 4-agent system as a tool for Orchestrator

Workflow:
1. Solution Researcher: Deep research using web search and code extraction
2. Code Agent: Code execution and verification
3. Debug Agents (3 parallel): Systematic debugging with multiple strategies if debugging is needed
4. Output Processor: Select best result and format detailed explanation

All internal agents have search_memory capability built-in to leverage past memories
"""

from __future__ import annotations

import os
import sys
import asyncio
import time
from typing import Dict, Any
from agents import function_tool, Runner

# Import MLflow for tracing
try:
    import mlflow
    MLFLOW_AVAILABLE = True
except ImportError:
    MLFLOW_AVAILABLE = False
    print("âš ï¸  MLflow not available - tracing disabled")

# Add project root to path for imports
current_dir = os.path.dirname(os.path.abspath(__file__))
conversational_system_dir = os.path.dirname(current_dir)
project_root = os.path.dirname(conversational_system_dir)

sys.path.insert(0, project_root)

# Import agents from deep_solver_with_memory
from conversational_system.deep_solver_with_memory import (
    create_solution_researcher_agent,
    create_code_agent,
    create_debug_agents,
    create_output_processor_agent
)


class DeepSolver:
    """
    Deep problem-solving workflow that wraps the 4-agent system.

    This class manages the complete research-code-debug-process workflow
    and enhances all internal agents with memory search capability.
    """

    def __init__(self):
        """Initialize DeepSolver."""
        self.solution_researcher = None
        self.code_agent = None
        self.debug_agents = None
        self.output_processor = None
        self._initialized = False
        self._init_lock = asyncio.Lock()  # Thread-safe initialization lock

    async def _initialize_agents(self):
        """
        Initialize all internal agents.

        This is done lazily to avoid initialization overhead.
        Uses async lock to ensure thread-safe initialization.
        """
        # Use async lock to prevent concurrent initialization
        async with self._init_lock:
            # Double-check pattern: check again after acquiring lock
            if self._initialized:
                return

            print("ðŸ”§ Initializing DeepSolver agents...")

            # Create agents (they already have search_memory built-in)
            self.solution_researcher = await create_solution_researcher_agent()
            self.code_agent = await create_code_agent()
            self.debug_agents = await create_debug_agents()
            self.output_processor = await create_output_processor_agent()

            self._initialized = True
            print("âœ… DeepSolver agents initialized")

    async def solve(
        self,
        query: str,
        user_id: str
    ) -> Dict[str, Any]:
        """
        Execute the full problem-solving workflow.

        Args:
            query: User's problem/question
            user_id: User identifier (for memory context)

        Returns:
            Dictionary containing:
            - original_user_query: Original user query
            - success: Whether solution was successful
            - final_code: Final working code solution
            - execution_results: Raw execution output and results
            - explanation: Detailed explanation (from free_form output)
        """
        # Ensure agents are initialized
        await self._initialize_agents()

        print(f"\nðŸš€ DeepSolver starting workflow for query: {query[:100]}...")

        workflow_start_time = time.time()

        # DO NOT create manual spans - let autolog handle everything automatically
        # Manual spans interfere with the trace hierarchy
        try:
            # Phase 1: Solution Research
            # Autolog will automatically capture tool calls within this agent
            print("ðŸ“š Phase 1: Solution Research...")
            phase1_start = time.time()

            solution_input = f"User ID: {user_id}\nQuery: {query}"
            runner = Runner()
            solution_result = await runner.run(self.solution_researcher, solution_input, max_turns=1000)

            if not solution_result or not hasattr(solution_result, 'final_output'):
                raise ValueError("Solution Researcher failed to produce valid output")

            solution_data = solution_result.final_output
            phase1_time = time.time() - phase1_start
            print(f"   âœ“ Research complete. Packages needed: {solution_data.required_packages}")

            # Phase 2: Code Execution
            # Autolog will automatically capture this agent call
            print("ðŸ’» Phase 2: Code Execution...")
            phase2_start = time.time()

            code_input = (
                f"User ID: {user_id}\n"
                f"Original user query: {query}\n"
                f"Required packages: {solution_data.required_packages}\n"
                f"Code solution:\n{solution_data.code_solution}"
            )

            runner = Runner()
            code_result = await runner.run(self.code_agent, code_input, max_turns=1000)

            if not code_result or not hasattr(code_result, 'final_output'):
                raise ValueError("Code Agent failed to produce valid output")

            code_data = code_result.final_output
            phase2_time = time.time() - phase2_start

            # Phase 3: Debugging (if needed)
            if code_data.needs_debugging:
                print("ðŸ› Phase 3: Debugging (3 parallel agents)...")
                phase3_start = time.time()

                debug_input = (
                    f"User ID: {user_id}\n"
                    f"Original user query: {query}\n"
                    f"Failed code: {code_data.executed_code}\n"
                    f"Execution output: {code_data.execution_output}"
                )

                # Run 3 debug agents in parallel
                debug_tasks = [
                    Runner().run(agent, debug_input, max_turns=1000)
                    for agent in self.debug_agents
                ]
                debug_results = await asyncio.gather(*debug_tasks, return_exceptions=True)

                print("   âœ“ Debugging complete")

                # Validate debug results
                valid_results = []
                for i, result in enumerate(debug_results):
                    if isinstance(result, Exception):
                        print(f"   âš ï¸ Debug agent {i+1} failed: {str(result)}")
                        continue
                    if result and hasattr(result, 'final_output'):
                        valid_results.append(result.final_output)
                    else:
                        print(f"   âš ï¸ Debug agent {i+1} returned invalid output")

                if not valid_results:
                    raise ValueError("All debug agents failed to produce valid output")

                print(f"   âœ“ {len(valid_results)}/{len(debug_results)} debug agents succeeded")
                phase3_time = time.time() - phase3_start

                # Phase 4: Output Processing (from debug results)
                print("ðŸ“Š Phase 4: Processing debug results...")
                phase4_start = time.time()

                debug_result_texts = []
                for i, result in enumerate(valid_results):
                    debug_result_texts.append(
                        f"Debug Result {i+1}:\n"
                        f"  Code: {result.final_code}\n"
                        f"  Output: {result.execution_output}"
                    )

                processor_input = (
                    f"Original user query: {query}\n"
                    + "\n\n".join(debug_result_texts)
                )

                runner = Runner()
                final_result = await runner.run(self.output_processor, processor_input, max_turns=1000)

                if not final_result or not hasattr(final_result, 'final_output'):
                    raise ValueError("Output Processor failed to produce valid output")

                final_data = final_result.final_output
                phase4_time = time.time() - phase4_start

            else:
                print("âœ… Phase 3: Code executed successfully, no debugging needed")

                # Phase 4: Output Processing (from successful execution)
                print("ðŸ“Š Phase 4: Processing successful execution...")
                phase4_start = time.time()

                processor_input = (
                    f"Original user query: {query}\n"
                    f"Successful execution:\n"
                    f"  Code: {code_data.executed_code}\n"
                    f"  Output: {code_data.execution_output}"
                )

                runner = Runner()
                final_result = await runner.run(self.output_processor, processor_input, max_turns=1000)

                if not final_result or not hasattr(final_result, 'final_output'):
                    raise ValueError("Output Processor failed to produce valid output")

                final_data = final_result.final_output
                phase4_time = time.time() - phase4_start

            workflow_time = time.time() - workflow_start_time
            print("âœ… DeepSolver workflow complete!\n")

            # Return FinalResult
            return {
                "success": final_data.success,
                "final_code": final_data.final_code,
                "execution_results": final_data.execution_results,
                "explanation": final_data.processed_output,
                "original_user_query": query
            }

        except Exception as e:
            workflow_time = time.time() - workflow_start_time
            print(f"âŒ DeepSolver error: {str(e)}")

            return {
                "success": False,
                "final_code": "",
                "execution_results": f"DeepSolver encountered an error: {str(e)}",
                "explanation": "Failed to solve the problem. Please try rephrasing your question or providing more details.",
                "original_user_query": query
            }


# Global DeepSolver instance
_deep_solver_instance = None


def get_deep_solver() -> DeepSolver:
    """Get or create the global DeepSolver instance."""
    global _deep_solver_instance
    if _deep_solver_instance is None:
        _deep_solver_instance = DeepSolver()
    return _deep_solver_instance


# Function tool for Orchestrator

@function_tool
async def solve_with_deep_solver(
    query: str,
    user_id: str
) -> Dict[str, Any]:
    """
    Solve complex materials science problems through comprehensive research,
    code generation, execution, and debugging workflow.

    Use this when:
    - Memory doesn't have sufficient relevant solutions
    - Problem requires research and experimentation
    - Quick solution attempts failed
    - You are uncertain about the approach

    Returns comprehensive solution with working code, execution results,
    and detailed explanation. Has 30-minute timeout for very complex problems.

    Args:
        query: User's computational problem/question
        user_id: User identifier for memory context

    Returns:
        Dictionary with: success (bool), final_code (str), execution_results (str),
        explanation (str), original_user_query (str)
    """
    solver = get_deep_solver()

    # 30-minute timeout for deep solver workflow
    TIMEOUT_SECONDS = 1800  # 30 minutes

    try:
        result = await asyncio.wait_for(
            solver.solve(query, user_id),
            timeout=TIMEOUT_SECONDS
        )
        return result

    except asyncio.TimeoutError:
        print(f"â±ï¸ DeepSolver timeout after {TIMEOUT_SECONDS} seconds (30 minutes)")

        # Return friendly timeout message instead of crashing
        return {
            "success": False,
            "final_code": "",
            "execution_results": "DeepSolver timed out after 30 minutes",
            "explanation": (
                "I apologize, but I wasn't able to complete this analysis. "
                "This problem appears to be quite complex.\n\n"
                "I suggest we break this down into smaller, more manageable steps:\n\n"
                "1. Could you help me understand which specific part you'd like to tackle first?\n"
                "2. Are there any intermediate results or partial solutions that would be helpful?\n"
                "3. Would you like to focus on a specific aspect of the problem (e.g., data retrieval, "
                "structure optimization, property calculation, visualization)?\n\n"
                "Please let me know how you'd like to proceed, and I'll help you solve this step by step."
            ),
            "original_user_query": query
        }


# Export
__all__ = [
    'DeepSolver',
    'get_deep_solver',
    'solve_with_deep_solver'
]

```


================================================================================
=== FILE: conversational_system\core\orchestrator.py ===
================================================================================

```python
"""
Orchestrator Agent - Main Conversation Controller

The Orchestrator is the primary agent that interacts with users in natural conversation.
It intelligently routes between:
- Quick solution path (for simple problems with good memory matches)
- Deep solver path (for complex problems requiring research)

Features:
- Memory integration for user preferences and past solutions
- Direct package management and code execution
- Intelligent path selection based on problem complexity
- Multi-turn conversation with user feedback handling
"""

from __future__ import annotations

import os
import sys
from typing import Any, Dict, Optional
from openai import AsyncOpenAI
from agents import Agent, OpenAIChatCompletionsModel

# Add project root to path for imports
current_dir = os.path.dirname(os.path.abspath(__file__))
conversational_system_dir = os.path.dirname(current_dir)
project_root = os.path.dirname(conversational_system_dir)
sys.path.insert(0, project_root)

# Set workspace paths for conversational system BEFORE importing workspace_tools
# This ensures execute_code uses conversational_system directories, not deep_solver_benchmark
if not os.getenv("CODE_STORAGE_DIR"):
    os.environ["CODE_STORAGE_DIR"] = os.path.join(conversational_system_dir, "temp_code")
if not os.getenv("SAVED_FILES_DIR"):
    os.environ["SAVED_FILES_DIR"] = os.path.join(conversational_system_dir, "saved_code")

# Import direct tools for package management and code execution
from mcp_servers_and_tools.direct_tools.workspace_tools import (
    check_installed_packages,
    install_dependencies,
    execute_code
)

# Import memory tools
from mcp_servers_and_tools.direct_tools.memory_tools import search_memory, save_to_memory

# Import prompts
from ..config.prompts import get_orchestrator_prompt


# Model configuration (configurable via environment variables)
# OpenAI: use defaults or set AGENT_MODEL_NAME=gpt-4o, o3, etc.
# Local/self-hosted models: set OPENAI_BASE_URL, AGENT_MODEL_NAME, and REQUIRE_OPENAI_API_KEY=false
MODEL_NAME = os.getenv("AGENT_MODEL_NAME", "o3")
_base_url = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
_require_api_key = os.getenv("REQUIRE_OPENAI_API_KEY", "true").lower() == "true"

client = AsyncOpenAI(
    api_key=os.getenv("OPENAI_API_KEY") if _require_api_key else "EMPTY",  # vLLM doesn't need real API key
    base_url=_base_url,
    timeout=500.0,
    max_retries=3,
)


class OrchestratorAgent(Agent):
    """
    Orchestrator Agent with memory integration and intelligent path selection.

    This agent doesn't use structured output (no output_type) to allow
    natural, flexible conversation with users.
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)


async def create_orchestrator(
    deep_solver_tool: Any
) -> OrchestratorAgent:
    """
    Create the Orchestrator agent with all integrated tools.

    Args:
        deep_solver_tool: The solve_with_deep_solver function tool (required)

    Returns:
        Configured Orchestrator agent
    """
    try:
        # Check API key
        openai_api_key = os.environ.get("OPENAI_API_KEY", "")
        if not openai_api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set")

        # Build tools list
        tools = [
            search_memory,             # Memory search
            check_installed_packages,  # List all installed packages (from workspace_tools)
            install_dependencies,      # Package installation (from workspace_tools)
            execute_code,              # Direct code execution (from workspace_tools)
            save_to_memory,            # Save successful solutions
            deep_solver_tool           # Deep problem-solving workflow (PATH B)
        ]

        # Create agent with dynamic prompt (fresh timestamp each time)
        agent = OrchestratorAgent(
            name="MaterialsScienceAssistant",
            instructions=get_orchestrator_prompt(),  # Generate fresh prompt with current date
            model=OpenAIChatCompletionsModel(model=MODEL_NAME, openai_client=client),
            tools=tools
            # No output_type - allows free-form conversation
        )

        return agent

    except Exception as e:
        print(f"Error creating Orchestrator agent: {e}")
        raise


# Export
__all__ = [
    'OrchestratorAgent',
    'create_orchestrator'
]

```


================================================================================
=== FILE: conversational_system\core\__init__.py ===
================================================================================

```python
"""
Agent components for the conversational system.

Modules:
- orchestrator: Main conversation controller with memory integration
- deep_solver: Deep problem-solving workflow wrapper
- memory_tools: Memory search and save functionality
"""

```


================================================================================
=== FILE: conversational_system\deep_solver_with_memory\adaptive_code_agent.py ===
================================================================================

```python
#!/usr/bin/env python3
"""
Adaptive Code Execution Agent
Dynamically sets output_type to enable both tool calling and structured output
"""
from agents import Agent, OpenAIChatCompletionsModel

# Import from the original code_agent
from .code_agent import CodeAgent, CODE_AGENT_PROMPT, MODEL_NAME, client
from .output_types import ExecutionReport

# Import memory tools
from mcp_servers_and_tools.direct_tools.memory_tools import search_memory

# Import direct tools
from mcp_servers_and_tools.direct_tools import (
    check_installed_packages,
    install_dependencies,
    execute_code,
    tavily_search,
    extract_code_from_url,
    retrieve_extracted_code,
)


# NOTE: When using MCP-based AdaptiveCodeAgent, uncomment the block below and comment out the generic Agent below with direct tools.

# class AdaptiveCodeAgent(CodeAgent):
class AdaptiveCodeAgent(Agent):
    """Adaptive Code Agent with dynamic output_type

    Workflow:
    1. Start with output_type=None to allow tool calling
    2. SDK will detect target_output_type when tools are done
    3. SDK automatically injects output_type and runs again
    4. Final round outputs structured data
    """

    def __init__(self, *args, **kwargs):
        # Store the target output type from kwargs before modifying it
        self.target_output_type = kwargs.get('output_type', ExecutionReport)

        # Start with no output_type to allow tool calling
        kwargs['output_type'] = None

        super().__init__(*args, **kwargs)


async def create_adaptive_code_agent() -> AdaptiveCodeAgent:
    """Create adaptive code agent"""
    try:
        # NOTE: When using MCP-based AdaptiveCodeAgent, uncomment the block below and comment out the generic Agent below with direct tools.

        # agent = AdaptiveCodeAgent(
        #     name="AdaptiveCodeAgent",
        #     instructions=CODE_AGENT_PROMPT,
        #     model=OpenAIChatCompletionsModel(model=MODEL_NAME, openai_client=client),
        #     output_type=ExecutionReport
        # )

        # Direct-tools Agent (kept active for running without MCP servers)
        agent = AdaptiveCodeAgent(
            name="AdaptiveCodeAgent",
            instructions=CODE_AGENT_PROMPT,
            model=OpenAIChatCompletionsModel(model=MODEL_NAME, openai_client=client),
            output_type=ExecutionReport,
            tools=[
                search_memory,
                check_installed_packages,
                install_dependencies,
                execute_code,
                tavily_search,
                extract_code_from_url,
                retrieve_extracted_code,
            ]
        )

        return agent

    except Exception as e:
        print(f"âŒ Error creating adaptive code agent: {e}")
        raise


# Export
__all__ = [
    "AdaptiveCodeAgent",
    "create_adaptive_code_agent"
]

```


================================================================================
=== FILE: conversational_system\deep_solver_with_memory\adaptive_debug_agent.py ===
================================================================================

```python
#!/usr/bin/env python3
"""
Adaptive Debug Agent
Dynamically sets output_type to enable both tool calling and structured output
"""
from agents import Agent, OpenAIChatCompletionsModel

# Import from the original debug_agent
from .debug_agent import DebugAgent, DEBUG_AGENT_PROMPT, MODEL_NAME, client
from .output_types import DebugResult

# Import memory tools
from mcp_servers_and_tools.direct_tools.memory_tools import search_memory

# Import direct tools
from mcp_servers_and_tools.direct_tools import (
    execute_code,
    create_and_execute_script,
    execute_shell_command,
    read_file,
    check_installed_packages,
    install_dependencies,
    tavily_search,
    extract_code_from_url,
    retrieve_extracted_code,
    quick_introspect,
    runtime_probe_snippet,
    parse_local_package,
    query_knowledge_graph,
    check_package_version,
)


# NOTE: When using MCP-based AdaptiveDebugAgent, uncomment the block below and comment out the generic Agent below with direct tools.

# class AdaptiveDebugAgent(DebugAgent):
class AdaptiveDebugAgent(Agent):
    """Adaptive Debug Agent with dynamic output_type

    Workflow:
    1. Start with output_type=None to allow tool calling
    2. SDK will detect target_output_type when tools are done
    3. SDK automatically injects output_type and runs again
    4. Final round outputs structured data
    """

    def __init__(self, *args, **kwargs):
        # Store the target output type from kwargs before modifying it
        self.target_output_type = kwargs.get('output_type', DebugResult)

        # Start with no output_type to allow tool calling
        kwargs['output_type'] = None

        super().__init__(*args, **kwargs)


async def create_adaptive_debug_agent(agent_id: int = 1) -> AdaptiveDebugAgent:
    """Create adaptive debug agent"""
    try:
        # NOTE: When using MCP-based AdaptiveDebugAgent, uncomment the block below and comment out the generic Agent below with direct tools.

        # agent = AdaptiveDebugAgent(
        #     name=f"AdaptiveDebugAgent{agent_id}",
        #     instructions=DEBUG_AGENT_PROMPT,
        #     model=OpenAIChatCompletionsModel(model=MODEL_NAME, openai_client=client),
        #     output_type=DebugResult
        # )

        # Direct-tools Agent (kept active for running without MCP servers)
        agent = AdaptiveDebugAgent(
            name=f"AdaptiveDebugAgent{agent_id}",
            instructions=DEBUG_AGENT_PROMPT,
            model=OpenAIChatCompletionsModel(model=MODEL_NAME, openai_client=client),
            output_type=DebugResult,
            tools=[
                search_memory,
                check_installed_packages,
                install_dependencies,
                check_package_version,
                execute_code,
                create_and_execute_script,
                execute_shell_command,
                read_file,
                tavily_search,
                extract_code_from_url,
                retrieve_extracted_code,
                quick_introspect,
                runtime_probe_snippet,
                parse_local_package,
                query_knowledge_graph,
            ]
        )

        return agent

    except Exception as e:
        print(f"âŒ Error creating adaptive debug agent: {e}")
        raise


async def create_adaptive_debug_agents():
    """Create three adaptive debug agent instances"""
    agents = []
    for i in range(1, 4):
        agent = await create_adaptive_debug_agent(i)
        agents.append(agent)
    return agents


# Export
__all__ = [
    "AdaptiveDebugAgent",
    "create_adaptive_debug_agent",
    "create_adaptive_debug_agents"
]

```


================================================================================
=== FILE: conversational_system\deep_solver_with_memory\adaptive_solution_researcher.py ===
================================================================================

```python
#!/usr/bin/env python3
"""
Adaptive Solution Researcher
Dynamically sets output_type to enable both tool calling and structured output
"""
from agents import Agent, OpenAIChatCompletionsModel

# Import from the original solution_researcher
from .solution_researcher import SolutionResearcherAgent, SOLUTION_RESEARCHER_PROMPT, MODEL_NAME, client
from .output_types import SolutionResponse

# Import memory tools
from mcp_servers_and_tools.direct_tools.memory_tools import search_memory

# Import direct tools
from mcp_servers_and_tools.direct_tools import (
    tavily_search,
    extract_code_from_url,
    retrieve_extracted_code,
    quick_introspect,
)


# NOTE: When using MCP-based AdaptiveSolutionResearcher, uncomment the block below and comment out the generic Agent below with direct tools.

# class AdaptiveSolutionResearcher(SolutionResearcherAgent):
class AdaptiveSolutionResearcher(Agent):
    """Adaptive Solution Researcher with dynamic output_type

    Workflow:
    1. Start with output_type=None to allow tool calling
    2. SDK will detect target_output_type when tools are done
    3. SDK automatically injects output_type and runs again
    4. Final round outputs structured data
    """

    def __init__(self, *args, **kwargs):
        # Store the target output type from kwargs before modifying it
        self.target_output_type = kwargs.get('output_type', SolutionResponse)

        # Start with no output_type to allow tool calling
        kwargs['output_type'] = None

        super().__init__(*args, **kwargs)


async def create_adaptive_solution_researcher() -> AdaptiveSolutionResearcher:
    """Create adaptive solution researcher"""
    try:
        # NOTE: When using MCP-based AdaptiveSolutionResearcher, uncomment the block below and comment out the generic Agent below with direct tools.

        # agent = AdaptiveSolutionResearcher(
        #     name="AdaptiveSolutionResearcher",
        #     instructions=SOLUTION_RESEARCHER_PROMPT,
        #     model=OpenAIChatCompletionsModel(model=MODEL_NAME, openai_client=client),
        #     output_type=SolutionResponse
        # )

        # Direct-tools Agent (kept active for running without MCP servers)
        agent = AdaptiveSolutionResearcher(
            name="AdaptiveSolutionResearcher",
            instructions=SOLUTION_RESEARCHER_PROMPT,
            model=OpenAIChatCompletionsModel(model=MODEL_NAME, openai_client=client),
            output_type=SolutionResponse,
            tools=[
                search_memory,
                tavily_search,
                extract_code_from_url,
                retrieve_extracted_code,
                quick_introspect,
            ]
        )

        return agent

    except Exception as e:
        print(f"âŒ Error creating adaptive solution researcher: {e}")
        raise


# Export
__all__ = [
    "AdaptiveSolutionResearcher",
    "create_adaptive_solution_researcher"
]

```


================================================================================
=== FILE: conversational_system\deep_solver_with_memory\code_agent.py ===
================================================================================

```python
from __future__ import annotations

import os
import logging
from agents import Agent, OpenAIChatCompletionsModel, ModelSettings
from openai.types.shared import Reasoning
from agents.mcp import MCPServerStdio
from openai import AsyncOpenAI
import asyncio
import tempfile
import subprocess
import time
from typing import Any, Callable, TypeVar, Optional
from utils.retry_utils import retry_mcp_server_connect
from utils.mcp_server_manager import get_or_create_mcp_server, get_mcp_server_info

# Import output types
from .output_types import ExecutionReport

# Import memory tools
import sys
import os
# Add core to path for memory_tools import
core_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'core')
if core_path not in sys.path:
    sys.path.insert(0, core_path)

# Import memory tools from direct_tools
from mcp_servers_and_tools.direct_tools.memory_tools import search_memory

# Direct tools (for running without MCP servers)
from mcp_servers_and_tools.direct_tools import (
    execute_code,
    check_installed_packages,
    install_dependencies,
    tavily_search,
    extract_code_from_url,
    retrieve_extracted_code,
)

T = TypeVar('T')

# Model configuration (configurable via environment variables)
# OpenAI: use defaults or set AGENT_MODEL_NAME=gpt-4o, o3, etc.
# Local/self-hosted models: set OPENAI_BASE_URL, AGENT_MODEL_NAME, and REQUIRE_OPENAI_API_KEY=false
MODEL_NAME = os.getenv("AGENT_MODEL_NAME", "o3")
_base_url = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
_require_api_key = os.getenv("REQUIRE_OPENAI_API_KEY", "true").lower() == "true"

client = AsyncOpenAI(
    api_key=os.getenv("OPENAI_API_KEY") if _require_api_key else "EMPTY",  # vLLM doesn't need real API key
    base_url=_base_url,
    timeout=500.0,
    max_retries=3,
)

CODE_AGENT_PROMPT = """
ROLE: Python Code Execution Specialist

You are an expert Python code executor for computational materials science and chemistry tasks

CRITICAL: YOU MUST EXECUTE CODE, NOT JUST ANALYZE IT

**INPUT FORMAT:**
You will receive:
- User ID (used for memory search)
- Original user query
- Required packages list
- Code solution to execute

**OUTPUT FORMAT:**
```json
{
  "user_id": "User identifier for memory search",
  "original_user_query": "exact text of the user's query",
  "executed_code": "# The actual code that was executed\nimport package1\nimport package2\n\n# Code here...",
  "execution_output": "Raw output from code execution including results, logs, errors",
  "needs_debugging": True or False (boolean)
}
```
**CRITICAL REQUIREMENTS:**
- Output EXACTLY ONE JSON object with fields: user_id, original_user_query, executed_code, execution_output, needs_debugging
- Do NOT output multiple JSON objects or any text before/after the JSON
- The output must be a single, valid, parseable JSON object
- needs_debugging is a boolean: use False if code executed successfully and meets user requirements, use True if errors occurred or results are incorrect/incomplete

CRITICAL RULES:
- Execute Python code using `execute_code` tool. You CANNOT skip tool calls and provide text-only responses
- For non-Python executables, use result = subprocess.run(command, capture_output=True, text=True, check=True) within Python code to call external programs so you can still execute the code
- Research and reference to meet all the requirements in STEP 2: `tavily-search`, `extract_code_from_url`, `retrieve_extracted_code`
- You can ONLY make necessary modifications to the code BEFORE execution to satisfy STEP 2 requirements (e.g., env variables, required prints for final results, subprocess logging scaffold, necessary unit conversions, preferred built-in command/flag usage, etc.)
- After you think the code meets all the requirements in STEP 2, you MUST execute the code ONLY ONCE (single `execute_code` call). Do NOT retry, re-run, or self-debug
- **IMPORTANT - TOOL USAGE LIMIT:** You have a maximum of 10 tool calls total. Do NOT exceed 10 tool calls

WORKFLOW STEPS:

**MEMORY SEARCH (OPTIONAL):**
- You can use 'search_memory' tool to find similar past solutions or user preferences
- Use user_id as one of the arguments for search_memory calls

STEP 1: ANALYZE INPUT (MANDATORY)
- EXTRACT the original user query, required packages, and code solution
- UNDERSTAND what the code is supposed to do
- If the code does Not include the correct environment variable setup where needed (e.g., os.getenv("MP_API_KEY") for Materials Project queries. Note: this is not provided in the user query but are stored in the environment variables exactly called 'MP_API_KEY'), you MUST correct the code to include the correct environment variable setup before code execution
- When using MPRester, ALWAYS make sure to use from mp_api.client import MPRester (with api_key=os.getenv('MP_API_KEY')) instead of from pymatgen.ext.matproj import MPRester
- **CRITICAL OUTPUT REQUIREMENT**: If the code does not print ALL specific numerical/string results that the user needs (e.g., energy, forces, etc.), you MUST modify the code to include proper print statements that output the complete results. This ensures that the results can be further processed by the output processor agent for proper analysis

STEP 2: SOLUTION REQUIREMENTS VERIFICATION (MANDATORY)
Before executing, you MUST ensure the solution (code and approach) meets these requirements. If unclear, you may use `tavily-search`, `extract_code_from_url`, and `retrieve_extracted_code` to consult official docs/examples:
- NEVER hardcode molecular/materials structures manually (e.g., it is totally FORBIDDEN to directly write XYZ coordinates in code). ALWAYS generate structures with established tools/libraries. Ensure coordinates are in correct units (xyz uses Angstrom by default). You can use `tavily-search`, `extract_code_from_url`, and `retrieve_extracted_code` to find the commonly used tool/library for generating structures with official code examples
- Optimize the structure before performing calculations unless the task explicitly requires using the original/fixed geometry
- ALWAYS use established computational software/packages for calculations
- ALWAYS follow documented protocols and best practices. When the target software provides a built-in command/flag that directly computes the requested property, you MUST prefer it over ad-hoc manual implementations
- If the current code uses a custom implementation (e.g., deriving the property by definition), you MUST use `tavily-search`, `extract_code_from_url`, and `retrieve_extracted_code` to see if you can find the official built-in command/flag to compute the requested property (very likely you can find it). If so, modify the code to replace the custom implementation before code execution
- ALWAYS use default parameters and settings unless specified otherwise
- ALWAYS ensure the solution is reproducible and based on verified methods
- ALWAYS check and convert units to match user requirements - if the code/software outputs in different units (e.g., Hartree, Angstroms) than what the user needs (e.g., eV, nanometers), perform the necessary unit conversions
- **CRITICAL**: strongly prefer all numbers to be codeâ€‘derived via library/API calls; avoid remembered, hardâ€‘coded, or example numbers
- **CRITICAL OUTPUT REQUIREMENT**: You MUST print ALL specific numerical/string results that the user needs. For example, if the user asks for energy and forces, print the actual energy value and all force components, not just their shapes or summaries. This ensures that the results are clearly presented and can be properly understood by the user
- SUBPROCESS EXECUTION VERIFICATION: If the code uses subprocess.run(), ensure it uses capture_output=True, text=True, check=True, saves output to a timestamped log file, prints the log path, and parses results as needed. Example:
  ```python
  result = subprocess.run(command, capture_output=True, text=True, check=True)
  timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
  log_file = f"subprocess_{timestamp}.log"
  with open(log_file, 'w') as f:
      f.write(f"# Command: {' '.join(command)}\n")
      f.write(f"# Return code: {result.returncode}\n")
      f.write(f"# STDOUT:\n{result.stdout}\n")
      if result.stderr:
          f.write(f"# STDERR:\n{result.stderr}\n")
  import os
  absolute_log_path = os.path.abspath(log_file)
  print(f"Subprocess output saved to: {absolute_log_path}")
  ```
If any requirement is not met, revise the code BEFORE proceeding.

STEP 3: PACKAGE MANAGEMENT (MANDATORY)
- IMMEDIATELY call `check_installed_packages` to get all installed packages
- IF any required packages are missing, call `install_dependencies` with missing packages
- WAIT for installation to complete before proceeding

STEP 4: CODE EXECUTION (MANDATORY)
- IMMEDIATELY call `execute_code` tool with the prepared code
- YOU MUST EXECUTE THE CODE EXACTLY ONCE - NO RETRIES, NO SELF-DEBUGGING

STEP 5: RESULT EVALUATION (MANDATORY)
- IF execution succeeds WITHOUT errors AND meets user requirements:
  * Set needs_debugging = False
  * Provide the raw execution output
- IF execution fails OR does not meet user requirements:
  * Set needs_debugging = True
  * Provide the raw execution output for debug agents
  * Do NOT modify the code or attempt any fixesâ€”stop after a single execution, you should immediately output EXACTLY ONE JSON object with the required fields

ABSOLUTELY FORBIDDEN:
- Providing code analysis without execution
- Skipping tool calls and giving text-only responses
- Proceeding to STEP 3 without verifying the code meets all the requirements in STEP 2
- **CRITICAL**: Attempting to debug or fix code yourself (let debug agents handle this)
- Making unecessary changes to the provided code before execution

SUCCESS CRITERIA:
- Code executed using the `execute_code` tool with the exact provided code
- Code meets all the requirements in STEP 2
- Raw execution results captured
- Correct assessment of whether debugging is needed

Your response must include actual tool calls and execution results. Start immediately with STEP 1

**CRITICAL TOOL USAGE LIMITATION: You cannot use tools infinitely. You are limited to a maximum of 10 tool calls (including execute_code, check_installed_packages, install_dependencies, etc.). You must execute the code and output results either when you are satisfied with the code OR when you reach the 10-tool limit. You must accurately determine whether debugging is needed based on the execution results**
"""

class CodeAgent(Agent):
    """Code Agent with MCP servers for basic code execution."""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._servers_initialized = False
        self._server_init_lock = asyncio.Lock()
        self._message_printed = False
        
        # Get project root directory
        current_dir = os.path.dirname(os.path.abspath(__file__))  # deep_solver_with_memory/
        conversational_system_dir = os.path.dirname(current_dir)  # conversational_system/
        workspace_root = os.path.dirname(conversational_system_dir)  # project root

        # Set up directories relative to workspace root
        self._temp_dir = os.path.join(workspace_root, "conversational_system", "temp_code")
        self._saved_dir = os.path.join(workspace_root, "conversational_system", "saved_code")
        self._mcp_workspace_path = os.path.join(workspace_root, "mcp_servers_and_tools", "workspace_server", "build", "index.js")
        self._mcp_memory_path = os.path.join(workspace_root, "mcp_servers_and_tools", "memory_server", "src", "memory_mcp.py")
        self._venv_path = os.path.join(workspace_root, ".venv")
        
        # Create directories if they don't exist
        os.makedirs(self._temp_dir, exist_ok=True)
        os.makedirs(self._saved_dir, exist_ok=True)
    
    def reset_message_flags(self):
        """Reset message flags to allow showing connection info again."""
        self._message_printed = False
    
    async def _initialize_servers(self):
        """Initialize MCP servers with proper locking."""
        
        # Use lock to prevent multiple simultaneous initializations
        async with self._server_init_lock:
            if self._servers_initialized:
                # Always show connection info if message flag is reset
                if not self._message_printed:
                    print("\nðŸ’» Code agent connected to MCP servers:")
                    for server in self.mcp_servers:
                        print(f"   â€¢ {server.name}")
                    self._message_printed = True
                return

            servers = []
            
            # Calculate the path to the project root (needed for multiple servers)
            current_dir = os.path.dirname(os.path.abspath(__file__))  # deep_solver_with_memory/
            conversational_system_dir = os.path.dirname(current_dir)  # conversational_system/
            workspace_root = os.path.dirname(conversational_system_dir)  # project root
            
            # Workspace Server
            try:
                # Prefer user-space Node 18+ via nvm if available
                nvm_dir = os.path.join(os.path.expanduser("~"), ".nvm", "versions", "node")
                node_cmd = "node"
                try:
                    if os.path.isdir(nvm_dir):
                        # pick the highest semantic version directory starting with 'v'
                        candidates = [d for d in os.listdir(nvm_dir) if d.startswith("v")]
                        if candidates:
                            latest = sorted(candidates, key=lambda s: [int(p) for p in s.lstrip('v').split('.')])[-1]
                            maybe = os.path.join(nvm_dir, latest, "bin", "node")
                            if os.path.exists(maybe):
                                node_cmd = maybe
                except Exception:
                    pass
                
                workspace_server_config = {
                    "name": "mcp_servers_and_tools/workspace_server",
                    "client_session_timeout_seconds": 500,
                    "params": {
                        "command": node_cmd,
                        "args": [self._mcp_workspace_path],
                        "env": {
                            "CODE_STORAGE_DIR": self._temp_dir,
                            "SAVED_FILES_DIR": self._saved_dir,
                            "ENV_TYPE": "venv-uv",
                            "UV_VENV_PATH": self._venv_path,
                            "PROJECT_ROOT": workspace_root,
                            "FORBIDDEN_PATH": os.path.join(workspace_root, "benchmark_tasks_and_results"),
                            "MCP_QUIET": "1",
                            "NODE_ENV": "production",
                            "MP_API_KEY": os.getenv("MP_API_KEY", ""),
                        },
                    }
                }
                
                workspace_server = await get_or_create_mcp_server("mcp_servers_and_tools/workspace_server", workspace_server_config, working_dir=workspace_root)
                servers.append(workspace_server)
                
            except Exception as e:
                print(f"âŒ Workspace Server failed: {e}")
                raise

            # Tavily Search Server
            try:
                tavily_server_config = {
                    "name": "tavily-search",
                    "client_session_timeout_seconds": 500,
                    "params": {
                        "command": "npx",
                        "args": ["-y", "tavily-mcp@0.2.1"],
                        "env": {
                            "TAVILY_API_KEY": os.getenv("TAVILY_API_KEY", ""),
                            "MCP_QUIET": "1",
                            "NODE_ENV": "production"
                        },
                    }
                }
                
                tavily_server = await get_or_create_mcp_server("tavily-search", tavily_server_config, working_dir=workspace_root)
                servers.append(tavily_server)
            except Exception as e:
                print(f"âš ï¸  Tavily search unavailable: {e}")

            # Research Server
            try:
                # Use the workspace_root calculated above
                mcp_research_path = os.path.join(workspace_root, "mcp_servers_and_tools/research_server", "src", "research_mcp.py")
                
                research_server_config = {
                    "name": "mcp_servers_and_tools/research_server",
                    "client_session_timeout_seconds": 500,
                    "params": {
                        "command": "python3",
                        "args": [mcp_research_path],
                        "env": {
                            "SUPABASE_URL": os.getenv("SUPABASE_URL"),
                            "SUPABASE_SERVICE_KEY": os.getenv("SUPABASE_SERVICE_KEY"),
                            "NEO4J_URI": os.getenv("NEO4J_URI"),
                            "NEO4J_USER": os.getenv("NEO4J_USER"),
                            "NEO4J_PASSWORD": os.getenv("NEO4J_PASSWORD"),
                            "USE_KNOWLEDGE_GRAPH": "true",
                            "GENERATE_CODE_SUMMARY": "false",
                            "OPENAI_EMBEDDING_MODEL": "text-embedding-3-small",
                            "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", ""),
                            "MCP_QUIET": "1",
                            "NODE_ENV": "production",
                            "TRANSPORT": "stdio"
                        },
                    }
                }
                
                research_server = await get_or_create_mcp_server("mcp_servers_and_tools/research_server", research_server_config, working_dir=workspace_root)
                servers.append(research_server)
            except Exception as e:
                print(f"âš ï¸  Research server unavailable: {e}")

            # Memory Server
            try:
                mcp_memory_path = os.path.join(workspace_root, "mcp_servers_and_tools", "memory_server", "src", "memory_mcp.py")

                memory_server_config = {
                    "name": "mcp_servers_and_tools/memory_server",
                    "client_session_timeout_seconds": 500,
                    "params": {
                        "command": "python3",
                        "args": [mcp_memory_path],
                        "env": {
                            "SUPABASE_URL": os.getenv("SUPABASE_URL"),
                            "SUPABASE_SERVICE_KEY": os.getenv("SUPABASE_SERVICE_KEY"),
                            "SUPABASE_DATABASE_URL": os.getenv("SUPABASE_DATABASE_URL"),
                            "NEO4J_URI": os.getenv("NEO4J_URI"),
                            "NEO4J_USER": os.getenv("NEO4J_USER"),
                            "NEO4J_PASSWORD": os.getenv("NEO4J_PASSWORD"),
                            "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", ""),
                            "MCP_QUIET": "1",
                            "TRANSPORT": "stdio"
                        },
                    }
                }

                memory_server = await get_or_create_mcp_server("mcp_servers_and_tools/memory_server", memory_server_config, working_dir=workspace_root)
                servers.append(memory_server)
            except Exception as e:
                print(f"âš ï¸  Memory server unavailable: {e}")

            self.mcp_servers = servers
            self._servers_initialized = True

            # Always print the message when servers are first initialized
            print("\nðŸ’» Code agent connected to MCP servers:")
            for server in self.mcp_servers:
                print(f"   â€¢ {server.name}")
            self._message_printed = True
        
    async def get_mcp_tools(self, run_context):
        """Get MCP tools, ensuring servers are initialized first."""
        await self._initialize_servers()
        return await super().get_mcp_tools(run_context)
    
    async def call_tool(self, tool_name: str, arguments: dict) -> Any:
        return await super().call_tool(tool_name, arguments)

async def create_code_agent() -> CodeAgent:
    """Create a code agent focused on code execution."""
    try:
        # # Check API key availability (commented out for non-OpenAI models)
        openai_api_key = os.environ.get("OPENAI_API_KEY", "")
        if not openai_api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set")

        # NOTE: When using MCP-based CodeAgent, uncomment the block below and comment out the generic Agent below with direct tools.

        # agent = CodeAgent(
        #     name="CodeExecutionAgent",
        #     instructions=CODE_AGENT_PROMPT,
        #     model=OpenAIChatCompletionsModel(model=MODEL_NAME, openai_client=client),   
        #     output_type=ExecutionReport,
        # )

        # Direct-tools Agent (kept active for running without MCP servers)
        agent = Agent(
            name="CodeExecutionAgent",
            instructions=CODE_AGENT_PROMPT,
            model=OpenAIChatCompletionsModel(model=MODEL_NAME, openai_client=client),   
            output_type=ExecutionReport,
            tools=[
                search_memory,  # Memory search for past solutions
                check_installed_packages,
                install_dependencies,
                execute_code,
                tavily_search,
                extract_code_from_url,
                retrieve_extracted_code,
            ],
            # Only set this model_settings for OpenAI GPT-5's reasoning models (gpt-5, gpt-5-mini, or gpt-5-nano), for other models, do not set this model_settings
            # model_settings=ModelSettings(reasoning=Reasoning(effort="medium"), verbosity="low")
        )

        return agent
        
    except Exception as e:
        print(f"Error creating code agent: {e}")
        raise

# Export the necessary functions and classes
__all__ = [
    "CodeAgent",
    "create_code_agent"
]
```


================================================================================
=== FILE: conversational_system\deep_solver_with_memory\debug_agent.py ===
================================================================================

```python
from __future__ import annotations

import os
import logging
from agents import Agent, OpenAIChatCompletionsModel, ModelSettings
from openai.types.shared import Reasoning
from agents.mcp import MCPServerStdio
from openai import AsyncOpenAI
import asyncio
import tempfile
import subprocess
import time
from typing import Any, Callable, TypeVar, Optional
from utils.retry_utils import retry_mcp_server_connect
from utils.mcp_server_manager import get_or_create_mcp_server, get_mcp_server_info

# Import output types
from .output_types import DebugResult

# Import memory tools
import sys
import os
# Add core to path for memory_tools import
core_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'core')
if core_path not in sys.path:
    sys.path.insert(0, core_path)

# Import memory tools from direct_tools
from mcp_servers_and_tools.direct_tools.memory_tools import search_memory

# Direct tools (for running without MCP servers)
from mcp_servers_and_tools.direct_tools import (
    execute_code,
    create_and_execute_script,
    execute_shell_command,
    read_file,
    check_installed_packages,
    install_dependencies,
    tavily_search,
    extract_code_from_url,
    retrieve_extracted_code,
    quick_introspect,
    runtime_probe_snippet,
    parse_local_package,
    query_knowledge_graph,
    check_package_version,
)

T = TypeVar('T')

# Model configuration (configurable via environment variables)
# OpenAI: use defaults or set AGENT_MODEL_NAME=gpt-4o, o3, etc.
# Local/self-hosted models: set OPENAI_BASE_URL, AGENT_MODEL_NAME, and REQUIRE_OPENAI_API_KEY=false
MODEL_NAME = os.getenv("AGENT_MODEL_NAME", "o3")
_base_url = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
_require_api_key = os.getenv("REQUIRE_OPENAI_API_KEY", "true").lower() == "true"

client = AsyncOpenAI(
    api_key=os.getenv("OPENAI_API_KEY") if _require_api_key else "EMPTY",  # vLLM doesn't need real API key
    base_url=_base_url,
    timeout=500.0,
    max_retries=3,
)

DEBUG_AGENT_PROMPT = """
ROLE: Python Code Debugging Specialist

You are an expert Python code debugger specialized in fixing failed code execution for computational materials science and chemistry tasks

CRITICAL: YOU MUST FIX AND EXECUTE CODE, NOT JUST ANALYZE IT

**INPUT FORMAT:**
You will receive:
- User ID (used for memory search)
- Original user query (with output format and unit requirements)
- Failed code that needs to be fixed
- Error information from the failed execution

**OUTPUT FORMAT:**
```json
{
  "original_user_query": "exact text of the user's query",
  "final_code": "# The debugged and fixed code\nimport package1\nimport package2\n\n# Fixed code here...",
  "execution_output": "Raw execution output, errors, logs, or results"
}
```
**CRITICAL REQUIREMENTS:**
- Output EXACTLY ONE JSON object with fields: original_user_query, final_code, execution_output
- Do NOT output multiple JSON objects or any text before/after the JSON
- The output must be a single, valid, parseable JSON object

CRITICAL RULES:
- If the code does not print ALL specific numerical/string results that the user needs, you MUST modify the code to include proper print statements that output the complete results. This ensures that the results can be further processed by the output processor agent for proper analysis
- When debugging code that needs to extract specific information from large output files (e.g., log files, calculation results), you can use tools like `read_file` to examine the actual output content and determine the correct extraction patterns. This helps you understand the exact format and structure of the output to write proper parsing code
- When you receive failed code and error information, you MUST debug and fix it using tools. You CANNOT just provide explanations or analysis
- You MUST call the required tools to fix and execute the code
- **IMPORTANT - DEBUGGING ATTEMPT USAGE LIMIT:** You have a maximum of 30 debugging attempts

WORKFLOW STEPS:

**MEMORY SEARCH (OPTIONAL):**
- You can use 'search_memory' tool to find similar past solutions before or during the debugging process
- Use user_id as one of the arguments for search_memory calls

STEP 1: ANALYZE THE ERROR (MANDATORY)
- EXTRACT the original user query, failed code, and error information
- IDENTIFY the root cause of the error
- DETERMINE the most appropriate debugging approach 
- For many Python-related errors, Introspection/Probe Fix is often a good starting point
- But if the error is from an external (non-Python) CLI computational executable invoked via subprocess, you MUST prioritize Research Fix and Local Package Fix. Do NOT use Introspection/Probe Fix or Knowledge Graph Fix for this case

STEP 2: DEBUGGING PROCESS (MANDATORY)
- Maximum 30 debugging attempts
- For each attempt:
  1. Analyze the error message or extraction issues
  2. **For obvious errors, use Direct Fix immediately**
  3. **For Python package symbol issues (imports/classes/methods/functions) or KeyError/AttributeError, use Introspection/Probe Fix first; escalate to Knowledge Graph Fix (combining with Local Package Fix when needed) only if unresolved after multiple attempts**
  4. **For external program issues**, 
  (a) Use Research Fix (e.g., `tavily-search`, `extract_code_from_url`, `retrieve_extracted_code`) to find official documentation, usage instructions, and command syntax details to solve the user's problem and then modify the code accordingly. **Always prioritize using built-in functions and commands/flags provided by the relevant software whenever possible. Try not to use a custom implementation (e.g., deriving the property by definition) in the code unless you cannot find the official built-in command/flag through Research Fix**
  (b) Local Package Fix can also be helpful for running scripts (you can even run the software command to see the output, but it is highly recommended to save the lengthy output to a log file and then use `read_file` to examine the log file), examining output and generated files (such as logs to see calculation output, error messages, or units), and extracting necessary information to meet the user's requirements
  5. Also remember to use diagnostic approaches or other approaches as needed
  6. If related to missing dependencies, use `check_installed_packages` and `install_dependencies`
  7. Repeat until success or 30 attempts reached

**AVAILABLE DEBUGGING TOOLS:**
- Python code execution: `execute_code`
- For non-Python executables, use result = subprocess.run(command, capture_output=True, text=True, check=True) within Python code to call external programs so you can still execute the code
- Research fix: `tavily-search`, `extract_code_from_url`, `retrieve_extracted_code`
- Introspection/Probe fix: `quick_introspect`, `runtime_probe_snippet`
- Knowledge graph fix: `parse_local_package`, `query_knowledge_graph`, `check_package_version`
- Local package fix: `execute_shell_command`, `create_and_execute_script`, `read_file`
- Package management: `check_installed_packages`, `install_dependencies`
- Result Processing Fix: `read_file`

**CRITICAL FILE PATH REQUIREMENTS:**
- When using `read_file` tool, you MUST prioritize using ABSOLUTE paths, not relative paths
- If you see "Access to files in the benchmark directory is forbidden" error, it may imply you're using a relative path. And you should NOT access any files in the benchmark directory
- To get absolute paths for local package fix, you can refer to the absolute path in the code execution error message
  To get the absolute path of the execution output files (like log files), you can find them in the temporary code directory. Use `execute_shell_command` with:
  ```bash
  find $HOME -name '*temp_code*' -type d 2>/dev/null
  ```
  to locate the temp directory, then:
  ```bash
  find <temp_directory_path> -name '*<filename>*' -type f 2>/dev/null 
  ```
- More generally for getting the absolute path of any files with `execute_shell_command` tool:
  ```bash
  find $HOME -name '*<filename>*' -type f 2>/dev/null
  ```
  This will search the entire home directory for files containing the specified name. The 2>/dev/null suppresses permission errors
- Always prioritize using the absolute path when calling `read_file` tool

**VERY IMPORTANT: DEBUGGING APPROACHES (choose the most efficient method or combination of methods based on the error type):**

**Direct Fix**
- **When to use**: Obvious syntax errors, simple typos, basic import issues, straightforward fixes
- **STEPs**: Fix the obvious error and re-execute immediately
- If the problem is more complex, prefer other debugging approaches for efficiency; for many Python-related errors, try Introspection/Probe Fix first

**Introspection/Probe Fix**
- **When to use**: Python package symbol resolution errors (imports, classes, methods, functions) and runtime key/attribute access errors (KeyError, AttributeError)
- **Why it's effective**: Provides fast static+runtime insights for correct import paths and symbol locations; runtime probes reveal available keys/attributes and similarity hints
- **STEPs**:
  0. Routing by error type (MANDATORY and CRITICAL):
     - If the error is KeyError or AttributeError â†’ Follow B. runtime_probe_snippet directly (do NOT start with quick_introspect)
     - If the error is related to imports/classes/methods/functions â†’ Follow A. quick_introspect
     - If uncertain, prefer A for symbol/import resolution issues; prefer B for runtime missing key/attribute access
  A. quick_introspect (for imports/classes/methods/functions related issues)
    1. Carefully analyze the error message and provide targeted parameters specific to this failure to retrieve the most relevant information
       Examples (choose what matches your error):
       - Import errors: call `quick_introspect` with `code_content` of the failing script to get import suggestions
       - Class issues: provide `class_hint` and `repo_hint` (or `package_path`)
       - Method issues: provide `method_hint` and `repo_hint` (or `package_path`); preferably also `class_hint` to narrow the scope
       - Function issues: provide `function_hint` and `repo_hint` (or `package_path`); optionally provide `module_hint` to narrow the scope
       **PARAMETER SELECTION RULES:**
       - Use `method_hint` when the symbol is a member of a class (instance or class method). Do NOT use `function_hint` for class/instance methods
       - Use `function_hint` only for top-level (module-level) functions that are not bound to a class; when calls appear as `module.function(...)`
       - Heuristics to decide: analyze the call-site pattern â€” if it appears as `SomeClass.method(...)` or `obj.method(...)`, treat it as a method; if it appears as `package.module.function(...)`, `module.function(...)` or `function(...)`, treat it as a function
       - `repo_hint` means the top-level import name (e.g., the package you `import`), while `package_path` is an absolute filesystem path obtained from `check_package_version` tool. You should provide one of them together with the specific symbol hints above
    **IMPORTANT**: Do NOT only provide `code_content` with `repo_hint`/`package_path` and nothing else. This combination can only provide import diagnostics. For non-import errors, based on the specific error message, you MUST provide targeted hints with the fuzzy or exact name of the symbol: `class_hint`/`method_hint`/`function_hint` AND one of `repo_hint` or `package_path`
    2. repo_hint here means the top-level import name
    3. Carefully review the tool output. If it instructs you to rerun with corrected parameters, adjust your arguments and call `quick_introspect` again until you get the suggestions that can be used to fix the error; if it already returns debugging guidance, pick the most promising suggestion and try it. If you provided `method_hint`/`class_hint` or `function_hint` and no valid suggestions were returned, reconsider your hint choice (e.g., you may have used `function_hint` where `method_hint`/`class_hint` is needed, or vice versa), then rerun with the corrected hints
    4. Apply the selected suggestion to fix imports/paths/calls
    5. Re-run `execute_code` with the fixed code. If errors persist, consider trying other suggestions returned by `quick_introspect`

  B. runtime_probe_snippet (for KeyError/AttributeError)
    1. Call `runtime_probe_snippet` with `snippet="try_get_key"` for KeyError, or `snippet="try_get_attr"` for AttributeError to get the import code snippet that shows you how to import the probe function from a prepared runtime_probe.py under the mcp_servers_and_tools/research_server/introspection_and_probe directory
    2. **Add the probe import**: Paste the returned snippet into your script right after existing imports
    3. **Replace the failing line**: Modify the line causing the error:
       - Replace `mapping['k']` with `try_get_key(mapping, 'k')` for KeyError
       - Replace `obj.attr` with `try_get_attr(obj, 'attr')` for AttributeError
       - You can probe the exact same key/attribute name that failed, or try a different one if you have candidates
    4. **Execute and analyze**: Run `execute_code` with the probe-enabled code to see all available keys/attributes and similarity suggestions
    5. **Fix the original code**: Based on probe output, identify the correct key/attribute and fix the original code (remove probe functions, use normal access)
    6. **Final execution**: Run `execute_code` with the corrected code (no probe functions) to see if the fix works. If it still fails, you can go back to step 5 and try a different most promising key/attribute name suggested by the probe output

  - If unresolved after Introspection/Probe Fix, proceed to Knowledge Graph Fix

**IMPORTANT NOTE:**
If the problem mainly depends on external (non-Python) CLI computational software/executables which you use subprocess.run() in the code to execute (not Python packages), Introspection/Probe Fix is usually ineffective. In such cases, you MUST prioritize using Research Fix (e.g., `tavily-search`, `extract_code_from_url`, `retrieve_extracted_code`) to find official documentation, usage instructions, and command syntax details to solve the user's problem. Local Package Fix can also be helpful for running scripts (you can even run the software command to see the output, but it is highly recommended to save the lengthy output to a log file and then use `read_file` to examine the log file), examining output and generated files (such as logs with error messages or units), and extracting necessary information. Always prioritize using built-in functions and commands provided by the relevant software whenever possible

**Knowledge Graph Fix**
- **When to use**: python code structure issues (imports, classes, methods, function calls, attribute access), module structure problems
- **Recommendation**: Use this when Direct Fix and Introspection/Probe Fix are ineffective
- **Why it's effective**: Provides complete code structure, accurate import paths, class hierarchies, and method signatures
- **STEPs**:
    1. Use `parse_local_package` with just the package name first
    2. Check if the parsing was successful. If it fails, the error message will tell you to try the correct package name, install the package if it is not installed, or use `check_package_version` to get the exact package path, then use `parse_local_package` with package_path again
       The `parse_local_package` function returns:
       - `package_name`: The name of the package
       - `package_path`: The actual package directory (e.g., `/path/to/site-packages/package_name`) - use this for `parse_local_package`
       - `version`: The package version (if detected)
    3. After parsing the package, according to the error message, use `query_knowledge_graph` with multiple targeted queries to understand the correct structure of the code:
     - For import errors: `"class ClassName"`, `"explore PackageName"`, or `"repos"` to discover available packages
     - For method/function issues: `"method MethodName ClassName"` or search for methods in class results
     - For structure exploration: `"explore PackageName"`, `"classes PackageName"`, or `"repos"` to see available repositories
     - For attribute access issues: `"query MATCH (c:Class)-[:HAS_ATTRIBUTE]->(a:Attribute) WHERE c.name = 'ClassName' RETURN a.name"`
     - For file examination: `"query MATCH (f:File) WHERE f.path CONTAINS 'package_name' RETURN f.path"`
     - For __init__.py analysis: `"query MATCH (f:File)-[:DEFINES]->(c:Class) WHERE f.path CONTAINS '__init__.py' RETURN c.name"`
     - For complex searches: `"query MATCH (c:Class) WHERE c.name CONTAINS 'Keyword1' AND c.name CONTAINS 'Keyword2' RETURN c"`
     - **Available commands**: `repos`, `explore <repo>`, `classes [repo]`, `class <class_name>`, `method <method_name> [class_name]`, `query <cypher>`
     - **Note: These are reference examples. You may query other content or use custom strategies as needed to efficiently resolve the error**
    4. Fix the code based on the discovered structure
    5. Re-execute with fixed code
    6. **Combine with Local Package Fix** when needed to examine specific files mentioned in error messages

**IMPORTANT NOTE:**
If the problem mainly depends on external computational software which you use subprocess.run() in the code to execute (not Python packages), Knowledge Graph Fix is usually ineffective. In such cases, you MUST prioritize using Research Fix (e.g., `tavily-search`, `extract_code_from_url`, `retrieve_extracted_code`) to find official documentation, usage instructions, and command syntax details to solve the user's problem. Local Package Fix can also be helpful for running scripts (you can even run the software command to see the output, but it is highly recommended to save the lengthy output to a log file and then use `read_file` to examine the log file), examining output and generated files (such as logs with error messages or units), and extracting necessary information. Always prioritize using built-in functions and commands provided by the relevant software whenever possible

**Local Package Fix**
- **When to use**: When you need to examine specific files mentioned in error messages, or to complement Knowledge Graph analysis
- **Why it's effective**: Direct access to actual source files, can quickly examine specific lines mentioned in error messages
- **STEPs**:
  1. Use `create_and_execute_script` and `execute_shell_command` to locate relevant files (e.g., "find /path/to/package -name '*.py' -exec grep -l 'error_keyword' {} ")
  2. Use `read_file` to examine specific files, README, or documentation
  3. Understand the issue and fix the code accordingly
  4. Re-execute with fixed code

**Research Fix**
- **When to use**: When the error is related to external programs or software, or when the error needs to be solved by finding the correct documentation or usage instructions through web search
- **Why it's effective**: Provides access to the latest official documentation, real-world usage examples, and community solutions. Combines comprehensive web search with targeted code extraction and RAG retrieval to find verified, working solutions from authoritative sources
- **STEPs**:
  1. **SEARCH FOR RELEVANT INFORMATION**: 
     - Create 5-10 diverse and specific search queries based on the error (keep each query conciseâ€”under 400 characters), covering different aspects such as:
       * Priority: Official documentation and API references
       * Priority: GitHub repositories (README, examples, implementation code)  
       * Implementation tutorials with actual code
       * Specific libraries, functions, or methods mentioned
     - Execute multiple `tavily-search` calls with these queries using optimized parameters. **MANDATORY**: You MUST use these exact parameters:
       * `search_depth: "advanced"` - for higher relevance in search results
       * `include_raw_content: true` - to get full extracted content for comprehensive analysis
       * `max_results: 3` - to limit results for focused analysis
       
       **EXAMPLE FORMAT**:
       ```json
       {
         "query": "your search query",
         "search_depth": "advanced",
         "include_raw_content": true,
         "max_results": 3
       }
       ```
       
       **CRITICAL**: Never use basic search - always use advanced search with raw content for better code examples and documentation.
  2. **EXTRACT AND SEARCH CODE EXAMPLES**:
     - After gathering search results, analyze them to identify the most relevant URLs that contain actual code implementations (prioritize official documentation or reputable repositories)
     - **CRITICAL**: You MUST use URLs identified from `tavily-search` results. DO NOT invent or write URLs directly. Only use `extract_code_from_url` with URLs that were actually found through search
     - For those most relevant URLs that contain code examples (such as jupyter_code_cell, markdown_code_block, command_example, codebox, etc.), use `extract_code_from_url` to extract and store code blocks in the database
     - **DECIDE WHETHER TO USE RAG SEARCH**: After extracting code blocks, evaluate the extracted content:
       - **If the extracted code is manageable and highly relevant** (not too many code blocks, clear relevance to error): Use the extracted code directly without additional RAG search
       - **If the extracted code is overwhelming or contains many irrelevant examples that may lead to hallucinations** (too many code blocks, mixed relevance): Use `retrieve_extracted_code` to find the most relevant code examples with specific queries
     - If you need more information after the above process, you have two options:
       - **Option 1**: Use `tavily-search` to directly gather or confirm information (for official documentation, github repositories, API references, etc.)
       - **Option 2**: Use `tavily-search` to find more relevant URLs, then repeat the extraction process (`extract_code_from_url` and `retrieve_extracted_code` if needed)
     - Repeat the search and extraction process until you have sufficient information to fix the error with the least hallucinations

**Other Approaches**
- Diagnostic: Write diagnostic code, execute it, use results to fix original code
  * e.g. Check file existence, program installation, environment variables, command availability, etc.
- Result Processing Fix: Modify code to produce processable results
  *If raw results cannot be processed to match the user's required output format (if any), it may imply that the code is not correct so you need to fix the code first*
  *If the raw output is too lengthy (e.g., output generated by an external, non-Python program) to process easily, consider modifying the code to parse the output and obtain the necessary information. For example, you can use `read_file` to read the subprocess output saved to a log file (and also other files generated by the external program if needed), and then parse the log file to get the required information*
  *IMPORTANT: When using `read_file` to read files, always prioritize using absolute paths. If you get "Access to files in the benchmark directory is forbidden" error, use `execute_shell_command` with `find $HOME -name '*<filename>*' -type f 2>/dev/null` to get the absolute path of the file*
  *CRITICAL OUTPUT REQUIREMENT: You MUST ensure the code prints ALL specific numerical/string results that the user needs. For example, if the user asks for energy and forces, print the actual energy value and all force components, not just their shapes or summaries. This ensures that the results are clearly presented and can be properly understood by the user*
- Missing dependencies: Use `check_installed_packages` and `install_dependencies` first
- Unit Conversion: If the software outputs in different units (e.g., Hartree, Angstroms) than what the user needs (e.g., eV, nanometers), perform the necessary unit conversions
- Missing Materials Project API key: just use os.getenv("MP_API_KEY") to get the API key

Note: You may also combine and flexibly use the above approaches, and you may use any other debugging techniques not listed here, as long as they efficiently and quickly fix the code

AFTER 30 DEBUGGING ATTEMPTS:
- If still unsuccessful, provide the last attempted code version and raw execution output

Your response must include actual tool calls and execution results. Start immediately with STEP 1: ANALYZE THE ERROR

**CRITICAL DEBUGGING LIMITATION: You cannot debug infinitely. You are limited to a maximum of 30 debugging attempts. Once you have successfully fixed and executed the code OR reached the 30-debugging-attempt limit, you must stop debugging and output your structured response. Do not continue debugging or making additional modifications when you already have a working solution**
"""


class DebugAgent(Agent):
    """Debug Agent with MCP servers for code debugging and execution."""
    
    def __init__(self, agent_id: int = 1, *args, **kwargs):
        self.agent_id = agent_id
        super().__init__(*args, **kwargs)
        self._servers_initialized = False
        self._server_init_lock = asyncio.Lock()
        self._message_printed = False
        
        # Get project root directory
        current_dir = os.path.dirname(os.path.abspath(__file__))  # deep_solver_with_memory/
        conversational_system_dir = os.path.dirname(current_dir)  # conversational_system/
        workspace_root = os.path.dirname(conversational_system_dir)  # project root

        # Set up directories relative to workspace root
        self._temp_dir = os.path.join(workspace_root, "conversational_system", "temp_code")
        self._saved_dir = os.path.join(workspace_root, "conversational_system", "saved_code")
        self._mcp_workspace_path = os.path.join(workspace_root, "mcp_servers_and_tools", "workspace_server", "build", "index.js")
        self._mcp_memory_path = os.path.join(workspace_root, "mcp_servers_and_tools", "memory_server", "src", "memory_mcp.py")
        self._venv_path = os.path.join(workspace_root, ".venv")
        
        # Create directories if they don't exist
        os.makedirs(self._temp_dir, exist_ok=True)
        os.makedirs(self._saved_dir, exist_ok=True)
    
    def reset_message_flags(self):
        """Reset message flags to allow showing connection info again."""
        self._message_printed = False
    
    async def _initialize_servers(self):
        """Initialize MCP servers with proper locking."""
        
        # Use lock to prevent multiple simultaneous initializations
        async with self._server_init_lock:
            if self._servers_initialized:
                # Always show connection info if message flag is reset
                if not self._message_printed:
                    print(f"\nðŸ› Debug Agent {self.agent_id} connected to MCP servers:")
                    for server in self.mcp_servers:
                        print(f"   â€¢ {server.name}")
                    self._message_printed = True
                return

            servers = []
            
            # Calculate the path to the project root (needed for multiple servers)
            current_dir = os.path.dirname(os.path.abspath(__file__))  # deep_solver_with_memory/
            conversational_system_dir = os.path.dirname(current_dir)  # conversational_system/
            workspace_root = os.path.dirname(conversational_system_dir)  # project root
            
            # Workspace Server
            try:
                # Prefer user-space Node 18+ via nvm if available
                nvm_dir = os.path.join(os.path.expanduser("~"), ".nvm", "versions", "node")
                node_cmd = "node"
                try:
                    if os.path.isdir(nvm_dir):
                        candidates = [d for d in os.listdir(nvm_dir) if d.startswith("v")]
                        if candidates:
                            latest = sorted(candidates, key=lambda s: [int(p) for p in s.lstrip('v').split('.')])[-1]
                            maybe = os.path.join(nvm_dir, latest, "bin", "node")
                            if os.path.exists(maybe):
                                node_cmd = maybe
                except Exception:
                    pass
                workspace_server_config = {
                    "name": "mcp_servers_and_tools/workspace_server",
                    "client_session_timeout_seconds": 500,
                    "params": {
                        "command": node_cmd,
                        "args": [self._mcp_workspace_path],
                        "env": {
                            "CODE_STORAGE_DIR": self._temp_dir,
                            "SAVED_FILES_DIR": self._saved_dir,
                            "ENV_TYPE": "venv-uv",
                            "UV_VENV_PATH": self._venv_path,
                            "PROJECT_ROOT": workspace_root,
                            "FORBIDDEN_PATH": os.path.join(workspace_root, "benchmark_tasks_and_results"),
                            "MCP_QUIET": "1",
                            "NODE_ENV": "production",
                            "MP_API_KEY": os.getenv("MP_API_KEY", ""),
                        },
                    }
                }
                
                workspace_server = await get_or_create_mcp_server("mcp_servers_and_tools/workspace_server", workspace_server_config, working_dir=workspace_root)
                servers.append(workspace_server)
                
            except Exception as e:
                print(f"âŒ Workspace Server failed: {e}")
                raise
            
            # Tavily Search Server
            try: 
                tavily_server_config = {
                    "name": "tavily-search",
                    "client_session_timeout_seconds": 500,
                    "params": {
                        "command": "npx",
                        "args": ["-y", "tavily-mcp@0.2.1"],
                        "env": {
                            "TAVILY_API_KEY": os.getenv("TAVILY_API_KEY", ""),
                            "MCP_QUIET": "1",
                            "NODE_ENV": "production"
                        },
                    }
                }
                
                tavily_server = await get_or_create_mcp_server("tavily-search", tavily_server_config, working_dir=workspace_root)
                servers.append(tavily_server)
            except Exception as e:
                print(f"âš ï¸  Tavily search unavailable: {e}")

            # Research Server
            try:
                # Use the workspace_root calculated above
                mcp_research_path = os.path.join(workspace_root, "mcp_servers_and_tools/research_server", "src", "research_mcp.py")
                
                research_server_config = {
                    "name": "mcp_servers_and_tools/research_server",
                    "client_session_timeout_seconds": 500,
                    "params": {
                        "command": "python3",
                        "args": [mcp_research_path],
                        "env": {
                            "SUPABASE_URL": os.getenv("SUPABASE_URL"),
                            "SUPABASE_SERVICE_KEY": os.getenv("SUPABASE_SERVICE_KEY"),
                            "NEO4J_URI": os.getenv("NEO4J_URI"),
                            "NEO4J_USER": os.getenv("NEO4J_USER"),
                            "NEO4J_PASSWORD": os.getenv("NEO4J_PASSWORD"),
                            "USE_KNOWLEDGE_GRAPH": "true",
                            "GENERATE_CODE_SUMMARY": "false",
                            "OPENAI_EMBEDDING_MODEL": "text-embedding-3-small",
                            "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", ""),
                            "PROJECT_ROOT": os.getenv("PROJECT_ROOT", workspace_root),
                            "FORBIDDEN_PATH": os.getenv("FORBIDDEN_PATH", os.path.join(workspace_root, "benchmark_tasks_and_results")),
                            "MCP_QUIET": "1",
                            "NODE_ENV": "production",
                            "TRANSPORT": "stdio"
                        },
                    }
                }
                
                research_server = await get_or_create_mcp_server("mcp_servers_and_tools/research_server", research_server_config, working_dir=workspace_root)
                servers.append(research_server)
            except Exception as e:
                print(f"âš ï¸  Research server unavailable: {e}")

            # Memory Server
            try:
                mcp_memory_path = os.path.join(workspace_root, "mcp_servers_and_tools", "memory_server", "src", "memory_mcp.py")

                memory_server_config = {
                    "name": "mcp_servers_and_tools/memory_server",
                    "client_session_timeout_seconds": 500,
                    "params": {
                        "command": "python3",
                        "args": [mcp_memory_path],
                        "env": {
                            "SUPABASE_URL": os.getenv("SUPABASE_URL"),
                            "SUPABASE_SERVICE_KEY": os.getenv("SUPABASE_SERVICE_KEY"),
                            "SUPABASE_DATABASE_URL": os.getenv("SUPABASE_DATABASE_URL"),
                            "NEO4J_URI": os.getenv("NEO4J_URI"),
                            "NEO4J_USER": os.getenv("NEO4J_USER"),
                            "NEO4J_PASSWORD": os.getenv("NEO4J_PASSWORD"),
                            "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", ""),
                            "MCP_QUIET": "1",
                            "TRANSPORT": "stdio"
                        },
                    }
                }

                memory_server = await get_or_create_mcp_server("mcp_servers_and_tools/memory_server", memory_server_config, working_dir=workspace_root)
                servers.append(memory_server)
            except Exception as e:
                print(f"âš ï¸  Memory server unavailable: {e}")

            self.mcp_servers = servers
            self._servers_initialized = True

            # Always print the message when servers are first initialized
            print(f"\nðŸ› Debug Agent {self.agent_id} connected to MCP servers:")
            for server in self.mcp_servers:
                print(f"   â€¢ {server.name}")
            self._message_printed = True
        
    async def get_mcp_tools(self, run_context):
        """Get MCP tools, ensuring servers are initialized first."""
        await self._initialize_servers()
        return await super().get_mcp_tools(run_context)
    
    async def call_tool(self, tool_name: str, arguments: dict) -> Any:
        return await super().call_tool(tool_name, arguments)


async def create_debug_agent(agent_id: int = 1) -> DebugAgent:
    """Create a debug agent with specified ID."""
    try:
        # Check API key availability (commented out for non-OpenAI models)
        openai_api_key = os.environ.get("OPENAI_API_KEY", "")
        if not openai_api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set")

        # NOTE: When using MCP-based DebugAgent, uncomment the block below and comment out the generic Agent below with direct tools.

        # agent = DebugAgent(
        #     agent_id=agent_id,
        #     name=f"DebugAgent{agent_id}",
        #     instructions=DEBUG_AGENT_PROMPT,
        #     model=OpenAIChatCompletionsModel(model=MODEL_NAME, openai_client=client),
        #     output_type=DebugResult,
        # )

        # Direct-tools Agent (kept active for running without MCP servers)
        agent = Agent(
            name=f"DebugAgent{agent_id}",
            instructions=DEBUG_AGENT_PROMPT,
            model=OpenAIChatCompletionsModel(model=MODEL_NAME, openai_client=client),
            output_type=DebugResult,
            tools=[
                search_memory,  # Memory search for past solutions
                check_installed_packages,
                install_dependencies,
                check_package_version,
                execute_code,
                create_and_execute_script,
                execute_shell_command,
                read_file,
                tavily_search,
                extract_code_from_url,
                retrieve_extracted_code,
                quick_introspect,
                runtime_probe_snippet,
                parse_local_package,
                query_knowledge_graph,
            ],
            # Only set this model_settings for OpenAI GPT-5's reasoning models (gpt-5, gpt-5-mini, or gpt-5-nano), for other models, do not set this model_settings
            # model_settings=ModelSettings(reasoning=Reasoning(effort="medium"), verbosity="low")
        )    
        
        return agent
        
    except Exception as e:
        print(f"Error creating debug agent: {e}")
        raise
     
# Create three debug agent instances
async def create_debug_agents():
    """Create three debug agent instances for parallel debugging"""
    agents = []
    for i in range(1, 4):
        agent = await create_debug_agent(i)
        agents.append(agent)
    return agents

# Export the necessary functions and classes
__all__ = [
    "DebugAgent",
    "create_debug_agent",
    "create_debug_agents"
] 
```


================================================================================
=== FILE: conversational_system\deep_solver_with_memory\output_processor_agent.py ===
================================================================================

```python
from __future__ import annotations

import os
import logging
from agents import Agent, OpenAIChatCompletionsModel, ModelSettings
from openai.types.shared import Reasoning
from openai import AsyncOpenAI  
import asyncio
from typing import Any, Callable, TypeVar, Optional

# Import output types
from .output_types import FinalResult

T = TypeVar('T')

# Model configuration (configurable via environment variables)
# OpenAI: use defaults or set AGENT_MODEL_NAME=gpt-4o, o3, etc.
# Local/self-hosted models: set OPENAI_BASE_URL, AGENT_MODEL_NAME, and REQUIRE_OPENAI_API_KEY=false
MODEL_NAME = os.getenv("AGENT_MODEL_NAME", "o3")
_base_url = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
_require_api_key = os.getenv("REQUIRE_OPENAI_API_KEY", "true").lower() == "true"

client = AsyncOpenAI(
    api_key=os.getenv("OPENAI_API_KEY") if _require_api_key else "EMPTY",  # vLLM doesn't need real API key
    base_url=_base_url,
    timeout=500.0,
    max_retries=3,
)

OUTPUT_PROCESSOR_PROMPT = """
ROLE: Output Processing and Validation Specialist

You are an expert at processing and validating execution results for computational materials science and chemistry tasks

CRITICAL: THE PROCESSED_OUTPUT IS THE MOST IMPORTANT FIELD FOR USER UNDERSTANDING AND EVALUATION

INPUT FORMAT: You will receive:
- Original user query
- Either THREE debug results from parallel debugging attempts (each containing final code and execution output)
- Or SUCCESSFUL EXECUTION with Final Code and Execution Results (if no debugging was needed)

**OUTPUT FORMAT:**
```json
{
  "original_user_query": "exact text of the user's query",
  "success": True or False (boolean),
  "final_code": "# The final working code\nimport package1\nimport package2\n\n# Code here...",
  "execution_results": "Raw execution output and results",
  "processed_output": "The key field for evaluation - carefully explained answer and analysis to the user's query"
}
```
**CRITICAL REQUIREMENTS:**
- Output EXACTLY ONE JSON object with fields: original_user_query, success, final_code, execution_results, processed_output
- Do NOT output multiple JSON objects or any text before/after the JSON
- The output must be a single, valid, parseable JSON object
- success is a boolean: use True if code executes without errors AND meets requirements, use False otherwise

CRITICAL RULES:
- You MUST carefully analyze the original user query to understand the user requirements and what they need
- You MUST select the best debug result based on the selection criteria in STEP 1 and STEP 2
- You MUST ensure the processed_output contains the requested data and analysis to address the user's query

CRITICAL SELECTION AND OUTPUT REQUIREMENTS:

FOR SUCCESSFUL EXECUTION (if received):
- You MUST provide the complete original user query exactly as received
- You MUST provide the final working code
- You MUST provide the raw execution results
- You MUST determine if the execution truly meets user requirements (code works AND produces correct output)
- Based on this evaluation:
  * If the execution meets user requirements: 
    â†’ Set success = True
    â†’ Write the processed_output to address the user's query
  * If the execution fails to meet user requirements: 
    â†’ Set success = False
    â†’ Set processed_output = "Failed" (exactly this word, nothing else)

FOR DEBUGGING RESULTS (if received):
STEP 1: EVALUATE ALL THREE DEBUG RESULTS (MANDATORY)
- ANALYZE each debug result to determine:
  * Does the code execute without errors?
  * Does the execution output contain the required data?
- RANK the three results from best to worst based on:
  * Successful execution (no errors)
  * Presence of required data in output
  * Quality and completeness of results

STEP 2: SELECT THE BEST RESULT (MANDATORY)
- CHOOSE the debug result that best meets the user requirements
- If multiple results are successful, pick the one with most complete/accurate data
- If all results have errors or missing data, pick the one closest to success
- IMPORTANT: If multiple debug results produce the same or very similar output, this identical result is more likely to be correct and should be preferred most of the time

STEP 3: PROCESS THE SELECTED BEST RESULT (MANDATORY) 
Based on the best result selected in STEP 2:
- You MUST provide the complete original user query exactly as received
- You MUST provide the final working code from the SELECTED BEST result
- You MUST provide the raw execution results from the SELECTED BEST result
- You MUST determine if the SELECTED BEST result truly successful (code works AND produces correct output)
- Based on this evaluation of the SELECTED BEST result:
  * If the SELECTED BEST result meets user requirements: 
    â†’ Set success = True
    â†’ Write the processed_output to address the user's query
  * If the SELECTED BEST result still fails to meet user requirements: 
    â†’ Set success = False 
    â†’ Set processed_output = "Failed" (exactly this word, nothing else)
- MOST IMPORTANT: Based on the solution provided by the SELECTED BEST result, you MUST write the processed_output to properly address the original user query OR "Failed" if the SELECTED BEST result is insufficient

Your response must focus on selecting the best result and processing it comprehensively to meet the user requirements. Start immediately with CRITICAL SELECTION AND OUTPUT REQUIREMENTS
"""

class OutputProcessorAgent(Agent):
    """Output Processor Agent for selecting best debug result and final processing."""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

async def create_output_processor_agent() -> OutputProcessorAgent:
    """Create an output processor agent focused on selecting best debug result and processing."""
    try:
        # # Check API key availability (commented out for non-OpenAI models)
        openai_api_key = os.environ.get("OPENAI_API_KEY", "")
        if not openai_api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set")

        agent = OutputProcessorAgent(
                name="OutputProcessorAgent",
                instructions=OUTPUT_PROCESSOR_PROMPT,
                model=OpenAIChatCompletionsModel(model=MODEL_NAME, openai_client=client),
                output_type=FinalResult,
                # Only set this model_settings for OpenAI GPT-5's reasoning models (gpt-5, gpt-5-mini, or gpt-5-nano), for other models, do not set this model_settings
                # model_settings=ModelSettings(reasoning=Reasoning(effort="medium"), verbosity="low")
                )
      
        return agent
        
    except Exception as e:
        print(f"Error creating output processor agent: {e}")
        raise

# Export the necessary functions and classes
__all__ = [
    "OutputProcessorAgent",
    "create_output_processor_agent"
] 
```


================================================================================
=== FILE: conversational_system\deep_solver_with_memory\output_types.py ===
================================================================================

```python
from pydantic import BaseModel, Field
from typing import List
import re


def _sanitize_output(output: str) -> str:
    """Sanitize execution output by removing problematic characters that cause JSON parsing errors."""
    if not output:
        return output
    
    # Remove null characters (\u0000) that cause JSON truncation
    sanitized = output.replace('\u0000', '')
    
    # Remove other potentially problematic control characters
    # Keep common whitespace characters but remove others
    sanitized = re.sub(r'[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]', '', sanitized)
    
    return sanitized


class SolutionResponse(BaseModel):
    """Output type for Solution Researcher Agent"""
    user_id: str = Field(description="User identifier for memory search")
    original_user_query: str = Field(description="The complete original user query including requirements and expected output")
    required_packages: List[str] = Field(description="List of packages needed for the solution")
    code_solution: str = Field(description="Complete Python code solution")


class ExecutionReport(BaseModel):
    """Output type for Code Agent"""
    user_id: str = Field(description="User identifier for memory search")
    original_user_query: str = Field(description="The complete original user query")
    executed_code: str = Field(description="The actual code that was executed")
    execution_output: str = Field(description="Raw execution output, errors, logs, or results")
    needs_debugging: bool = Field(description="Whether the code needs debugging due to errors or incorrect results")
    
    def __init__(self, **data):
        # Sanitize execution_output before creating the object
        if 'execution_output' in data:
            data['execution_output'] = _sanitize_output(data['execution_output'])
        super().__init__(**data)


class DebugResult(BaseModel):
    """Output type for Debug Agents"""
    original_user_query: str = Field(description="The complete original user query")
    final_code: str = Field(description="The debugged and fixed code")
    execution_output: str = Field(description="Raw execution output, errors, logs, or results")
    
    def __init__(self, **data):
        # Sanitize execution_output before creating the object
        if 'execution_output' in data:
            data['execution_output'] = _sanitize_output(data['execution_output'])
        super().__init__(**data)


class FinalResult(BaseModel):
    """Output type for Output Processor Agent"""
    original_user_query: str = Field(description="The complete original user query")
    success: bool = Field(description="True only if code executes without errors AND meets user requirements")
    final_code: str = Field(description="The final working code")
    execution_results: str = Field(description="Raw execution output and results")
    processed_output: str = Field(description="The key field for evaluation - carefully explained answer and analysis to the user's query")
    
    def __init__(self, **data):
        # Sanitize execution_results before creating the object
        if 'execution_results' in data:
            data['execution_results'] = _sanitize_output(data['execution_results'])
        super().__init__(**data)
    
```


================================================================================
=== FILE: conversational_system\deep_solver_with_memory\solution_researcher.py ===
================================================================================

```python
from pydantic import BaseModel, Field
from typing import Optional, List, Any, Callable, TypeVar, Dict
import json
import re
import os
import sys
import asyncio
import subprocess
import time
from pathlib import Path

from agents import Agent, OpenAIChatCompletionsModel, ModelSettings
from openai.types.shared import Reasoning
from openai import AsyncOpenAI
from agents.mcp import MCPServerStdio
from mcp.server.fastmcp import Context
from dotenv import load_dotenv
from utils.retry_utils import retry_mcp_server_connect
from utils.mcp_server_manager import get_or_create_mcp_server, get_mcp_server_info

# Import output types
from .output_types import SolutionResponse

# Import memory tools
import sys
import os
# Add core to path for memory_tools import
core_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'core')
if core_path not in sys.path:
    sys.path.insert(0, core_path)

# Import memory tools from direct_tools
from mcp_servers_and_tools.direct_tools.memory_tools import search_memory

# direct tools (use these when running without MCP servers)
from mcp_servers_and_tools.direct_tools import (
    tavily_search,
    extract_code_from_url,
    retrieve_extracted_code,
    quick_introspect,
)


T = TypeVar('T')

# Model configuration (configurable via environment variables)
# OpenAI: use defaults or set AGENT_MODEL_NAME=gpt-4o, o3, etc.
# Local/self-hosted models: set OPENAI_BASE_URL, AGENT_MODEL_NAME, and REQUIRE_OPENAI_API_KEY=false
MODEL_NAME = os.getenv("AGENT_MODEL_NAME", "o3")
_base_url = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
_require_api_key = os.getenv("REQUIRE_OPENAI_API_KEY", "true").lower() == "true"

client = AsyncOpenAI(
    api_key=os.getenv("OPENAI_API_KEY") if _require_api_key else "EMPTY",  # vLLM doesn't need real API key
    base_url=_base_url,
    timeout=500.0,
    max_retries=3,
)

# Prompt for the solution researcher agent

SOLUTION_RESEARCHER_PROMPT = """
ROLE: Materials Science Solution Researcher

You are a materials science and chemistry researcher who specializes in finding code solutions through systematic, step-by-step research

NOTE: All the requests here require code implementation. Your primary goal is to provide working code solutions with accurate package dependencies

**TASK PLANNING AND TOOL LEARNING GUIDANCE:**
For complex problems that don't provide explicit step-by-step instructions, you need to:
1. **TASK ANALYSIS:** Understand the user's goal and keep the requirements in mind (you MUST follow the requirements)
2. **TASK DECOMPOSITION:** Use your prior knowledge, along with 'tavily-search' tools to systematically break down complex tasks into clear, executable steps
3. **TOOL SELECTION:** For each step, always prefer using direct, built-in functionalities or dedicated commands of established tools and software to obtain the required property or result. Do not attempt to reconstruct or re-derive results via indirect or manual methods (such as using formulas or combining outputs) unless explicitly required or built-in functionalities are not available
- Note: some tools may not be listed in the user query, sometimes you need to find them yourself
4. **TOOL LEARNING:** For each step, learn the tools and libraries that are needed to complete the step with 'tavily-search', 'extract_code_from_url', and 'retrieve_extracted_code' tools and your prior knowledge
5. **GENERAL POLICY:** strongly prefer all numbers to be codeâ€‘derived via library/API calls; avoid remembered, hardâ€‘coded, or example numbers

**OUTPUT FORMAT:**
```json
{
  "user_id": "User identifier for memory search",
  "original_user_query": "exact text of the user's query",
  "required_packages": ["package1", "package2", "package3"],
  "code_solution": "# Complete Python code\nimport package1\nimport package2\n\n# Your solution code here..."
}
```
**CRITICAL REQUIREMENTS:**
- Output EXACTLY ONE JSON object with fields: user_id, original_user_query, required_packages, code_solution
- Do NOT output multiple JSON objects or any text before/after the JSON
- The output must be a single, valid, parseable JSON object

CRITICAL RULES:
1. You MUST follow the workflow steps in order - NO SKIPPING
2. You MUST complete each step before moving to the next. **CRITICAL**: You should ALWAYS use `tavily-search` (to identify the most relevant URLs) before using `extract_code_from_url` and `retrieve_extracted_code` tools
3. You MUST NOT provide direct answers without following the workflow
4. **IMPORTANT - TOOL USAGE LIMIT:** You have a maximum of 20 tool calls total. Do NOT exceed 20 tool calls

WORKFLOW STEPS:

STEP 0. UNDERSTAND THE REQUEST (MANDATORY)
   - Clearly identify the user's technical goal and requirements

STEP 1. SEARCH MEMORY FOR PAST SOLUTIONS (RECOMMENDED)
   - Before starting web research, use 'search_memory' to check if similar problems have been solved before
   - Call search_memory with relevant queries and user_id about the user's request
   - If you find relevant past solutions or code examples in memory, use them as reference to adapt the solution to the current requirements
   - This can save time and provide proven solutions, but always verify they match the current requirements
   - You can use this tool multiple times if needed, with different queries and the same user_id

STEP 2. SEARCH FOR RELEVANT INFORMATION (MANDATORY)
   - Create 5-10 diverse and specific search queries based on the user's request (keep each query conciseâ€”under 400 characters), covering different aspects such as:
     * Priority: Official documentation and API references
     * Priority: GitHub repositories (README, examples, implementation code)
     * Implementation tutorials with actual code
     * Specific libraries, functions, or methods mentioned
   - Execute multiple `tavily-search` calls with these queries. **MANDATORY**: You MUST use these exact parameters for EVERY tavily-search call:
     * `search_depth: "advanced"` - for higher relevance in search results
     * `max_results: 3` - to limit results for focused analysis
     
     **EXAMPLE FORMAT**:
     ```json
     {
       "query": "your search query",
       "search_depth": "advanced",
       "max_results": 3
     }
     ```
     
     **CRITICAL**: Never use basic search - always use advanced search for better code examples and documentation
   - You may optionally call `quick_introspect` during information gathering to confirm exact import paths and class/method/function names if the relevant packages are installed. Provide targeted hints (`class_hint`/`method_hint`/`function_hint`) together with `repo_hint` (top-level import name, preferred over `package_path`) or `package_path`. For import-diagnostics mode you need to provide `code_content` you write
   - Function vs Method when calling `quick_introspect`:
      * Use method_hint when the target is a class member (instance/class method). Do not use function_hint for class/instance methods. **Most of the time, you should use method_hint**
      * Use function_hint only for top-level (module-level) functions; optionally add module_hint to narrow


STEP 3. EXTRACT AND SEARCH CODE EXAMPLES (MANDATORY) - TO REDUCE CODE HALLUCINATIONS
   - After gathering all search results, analyze them to identify **the most relevant URLs (at most 3)** that contain actual code implementations or other high-quality resources
     **URL SELECTION PRIORITY**:
      **First Priority**: Official documentation and API references and GitHub repositories (README files, example code, implementation files)
      **Second Priority**: Other web pages (only if they contain particularly relevant or unique code examples not found in official sources)
   - Prioritize complete, working code examples over theoretical discussions
     * The identified URLs are preferred to be single, highly relevant web pages that already contain the code examples we need to refer to (such as jupyter_code_cell, markdown_code_block, command_example, codebox, etc.) *
   - **CRITICAL**: You MUST use `tavily-search` first and then use URLs identified from `tavily-search` results. DO NOT invent or write URLs directly. Only use `extract_code_from_url` with URLs that were actually found through search
   - **MANDATORY**: Once you identify relevant URLs from search results in STEP 2, you MUST proceed to extract code from them. Do NOT just search without extracting - every identified relevant and important URL that contains code examples should be processed through `extract_code_from_url`
   - For those most relevant URLs that contain code examples (at most 3 most relevant URLs), use `extract_code_from_url` to extract and store code blocks in the database
   - **DECIDE WHETHER TO USE RAG SEARCH**: After extracting code blocks, evaluate the extracted content:
      * **If the extracted code is manageable and highly relevant** (not too many code blocks, clear relevance to user query): Use the extracted code directly without additional RAG search
      * **If the extracted code is overwhelming or contains many irrelevant examples that may lead to hallucinations** (too many code blocks, mixed relevance): Use `retrieve_extracted_code` to find the most relevant code examples with specific queries
   - If you need more information after the above process, you have two options:
      * **Option 1**: Use `tavily-search` to directly gather or confirm information (for official documentation, github repositories, API references, etc.)
      * **Option 2**: Use `tavily-search` to find more relevant URLs, then repeat the extraction process (`extract_code_from_url` and `retrieve_extracted_code` if needed)

STEP 4. REVIEW INFORMATION AND UNDERSTAND ADDITIONAL REQUIREMENTS (MANDATORY)
   - Review ALL extracted information and documentation (from 'tavily-search') and code examples (from 'extract_code_from_url' results, and optionally from 'retrieve_extracted_code' if used) to understand the actual working implementations
   - Understand the following requirements to prepare for solution synthesis:
      * You MUST provide Python code solutions (with seamless integration of external programs if needed)
      * For non-Python executables, design solutions using subprocess.run() within Python code
        **SUBPROCESS EXECUTION REQUIREMENTS (MANDATORY FOR NON-PYTHON EXECUTABLES):**
        - ALWAYS use the code with the exact parameter settings: result = subprocess.run(command, capture_output=True, text=True, check=True)
        - ALWAYS save subprocess output to timestamped log files for further debugging and parsing
        - ALWAYS print log file location for reference
        - Parse specific information from result.stdout to extract required data if needed
        - Example pattern:
          ```python
          result = subprocess.run(command, capture_output=True, text=True, check=True)
          timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
          log_file = f"subprocess_{timestamp}.log"
          with open(log_file, 'w') as f:
              f.write(f"# Command: {' '.join(command)}\n")
              f.write(f"# Return code: {result.returncode}\n")
              f.write(f"# STDOUT:\n{result.stdout}\n")
              if result.stderr:
                  f.write(f"# STDERR:\n{result.stderr}\n")
          import os
          absolute_log_path = os.path.abspath(log_file)
          print(f"Subprocess output saved to: {absolute_log_path}")
          ```
        **OTHER CRITICAL REQUIREMENTS:**
        - NEVER hardcode molecular/materials structures manually (e.g., it is totally FORBIDDEN to directly write XYZ coordinates in code). ALWAYS generate structures with established tools/libraries. Ensure coordinates are in correct units (xyz uses Angstrom by default). You can use `tavily-search`, `extract_code_from_url`, and `retrieve_extracted_code` to find the commonly used tool/library for generating structures with official code examples
        - Optimize the structure before performing calculations unless the task explicitly requires using the original/fixed geometry
        - ALWAYS use established computational software/packages for calculations
        - ALWAYS follow documented protocols and best practices. When the target software provides a built-in command/flag that directly computes the requested property, you MUST prefer it over ad-hoc manual implementations
        - If the current code uses a custom implementation (e.g., deriving the property by definition), you MUST use `tavily-search`, `extract_code_from_url`, and `retrieve_extracted_code` to see if you can find the official built-in command/flag to compute the requested property (very likely you can find it). If so, modify the code to replace the custom implementation before code execution
        - ALWAYS use default parameters and settings unless specified otherwise
        - ALWAYS ensure the solution is reproducible and based on verified methods
        - ALWAYS check and convert units to match user requirements - if the code/software outputs in different units (e.g., Hartree, Angstroms) than what the user needs (e.g., eV, nanometers), perform the necessary unit conversions
        - **CRITICAL**: strongly prefer all numbers to be codeâ€‘derived via library/API calls; avoid remembered, hardâ€‘coded, or example numbers
        - **CRITICAL OUTPUT REQUIREMENT**: You MUST print ALL specific numerical/string results that the user needs. For example, if the user asks for energy and forces, print the actual energy value and all force components, not just their shapes or summaries. This ensures that the results are clearly presented and can be properly understood by the user
   - Check if you have sufficient information to meet ALL requirements above
   - **DECISION POINT**: If you need more information, return to STEP 2 and STEP 3 to gather additional information. But do NOT overdo it, as it may lead to code bloat and slow down the solution process
   - Only proceed to STEP 5 when you have complete understanding of requirements and enough information to synthesize the solution

STEP 5. SYNTHESIZE SOLUTION (MANDATORY)
   - Base your solution STRICTLY on the verified code patterns, imports, and structures found in the extracted and searched content (prioritize exact code examples), with the least modifications to the code examples while ensuring the solution exactly matches the user's request
   - Cross-reference multiple sources to ensure consistency in API usage, method calls, and data structures
   - Try your best to reduce hallucinations and provide accurate information. Do NOT invent or assume any code elements - every import, class, method, and attribute must be confirmed from extracted and searched sources (from 'tavily-search' and extracted code examples)
   - If any code structure is unclear, perform additional 'tavily-search' calls or 'extract_code_from_url' processes (and 'retrieve_extracted_code' if needed) to clarify before proceeding
   - Synthesize a complete solution that combines the best practices from multiple verified sources. **CRITICAL**: Always prioritize built-in command-line options, flags, and functionalities of the target software over custom implementations or workarounds. For example, if a software has built-in flags for specific calculations or properties, use those instead of manually implementing the calculations. Built-in options are more reliable, tested, and efficient than custom solutions

CRITICAL REQUIREMENTS FOR YOUR RESPONSE:
    - You MUST provide the complete original user query exactly as received
    - You MUST identify and list ALL required packages needed for the solution
    - You MUST provide complete, executable, correct and relevant Python code based on verified sources
    - Your code MUST include environment variable setup where needed (e.g., os.getenv("MP_API_KEY") for Materials Project queries. Note: this is not provided in the user query but are stored in the environment variables exactly called 'MP_API_KEY')
    - When using MPRester, you MUST use the code 'from mp_api.client import MPRester' with os.getenv('MP_API_KEY') instead of 'from pymatgen.ext.matproj import MPRester'
    - Ensure the code follows patterns verified from your research. No hallucinations are allowed
    - **FOLLOW STEP 4 REQUIREMENTS**: Your solution must adhere to all requirements understood in STEP 4

Your ultimate goal is to provide a complete, researched code solution that addresses the user's needs. Start with STEP 0: UNDERSTAND THE REQUEST and then 'search_memory' and 'tavily-search'

**CRITICAL TOOL USAGE LIMITATION: You cannot use tools infinitely. You are limited to a maximum of 20 tool calls (including tavily-search, extract_code_from_url, retrieve_extracted_code, quick_introspect). Once you have gathered sufficient information to provide a complete solution OR reached the 20-tool limit, you must stop using tools and output your structured response. Do not over-research or continue searching when you already have enough information to synthesize the solution**
"""

class SolutionResearcherAgent(Agent):
    """Solution Researcher Agent with MCP servers for web search, code extraction and retrieval."""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._servers_initialized = False
        self._server_init_lock = asyncio.Lock()
        self._message_printed = False
        
        # Get project root directory
        current_dir = os.path.dirname(os.path.abspath(__file__))  # deep_solver_with_memory/
        conversational_system_dir = os.path.dirname(current_dir)  # conversational_system/
        self._workspace_root = os.path.dirname(conversational_system_dir)  # project root

        # Set up directories relative to workspace root
        self._mcp_research_path = os.path.join(self._workspace_root, "mcp_servers_and_tools", "research_server", "src", "research_mcp.py")
        self._mcp_memory_path = os.path.join(self._workspace_root, "mcp_servers_and_tools", "memory_server", "src", "memory_mcp.py")
    
    def reset_message_flags(self):
        """Reset message flags to allow showing connection info again."""
        self._message_printed = False
    
    async def _initialize_servers(self):
        """Initialize MCP servers with proper locking."""

        # Use lock to prevent multiple simultaneous initializations
        async with self._server_init_lock:
            if self._servers_initialized:
                # Always show connection info if message flag is reset
                if not self._message_printed:
                    print("\nâœ… Solution researcher connected to MCP servers:")
                    for server in self.mcp_servers:
                        print(f"   â€¢ {server.name}")
                    self._message_printed = True
                return

            servers = []
            
            # Research Server
            try:
                research_server_config = {
                    "name": "mcp_servers_and_tools/research_server",
                    "client_session_timeout_seconds": 500,
                    "params": {
                        "command": "python3",
                        "args": [self._mcp_research_path],
                        "env": {
                            "SUPABASE_URL": os.getenv("SUPABASE_URL"),
                            "SUPABASE_SERVICE_KEY": os.getenv("SUPABASE_SERVICE_KEY"),
                            "NEO4J_URI": os.getenv("NEO4J_URI"),
                            "NEO4J_USER": os.getenv("NEO4J_USER"),
                            "NEO4J_PASSWORD": os.getenv("NEO4J_PASSWORD"),
                            "USE_KNOWLEDGE_GRAPH": "true",
                            "GENERATE_CODE_SUMMARY": "false",
                            "OPENAI_EMBEDDING_MODEL": "text-embedding-3-small",
                            "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", ""),
                            "MCP_QUIET": "1",
                            "NODE_ENV": "production",
                            "TRANSPORT": "stdio"
                        },
                    }
                }
                
                research_server = await get_or_create_mcp_server("mcp_servers_and_tools/research_server", research_server_config, working_dir=self._workspace_root)
                servers.append(research_server)
                
            except Exception as e:
                print(f"âš ï¸  Research server failed: {e}")
                raise
            
            # Tavily Search Server
            try:
                tavily_server_config = {
                    "name": "tavily-search",
                    "client_session_timeout_seconds": 500,
                    "params": {
                        "command": "npx",
                        "args": ["-y", "tavily-mcp@0.2.1"],
                        "env": {
                            "TAVILY_API_KEY": os.getenv("TAVILY_API_KEY"),
                            "MCP_QUIET": "1",
                            "NODE_ENV": "production"
                        },
                    }
                }
                
                tavily_server = await get_or_create_mcp_server("tavily-search", tavily_server_config, working_dir=self._workspace_root)
                servers.append(tavily_server)

            except Exception as e:
                print(f"âš ï¸  Tavily search unavailable: {e}")

            # Memory Server
            try:
                memory_server_config = {
                    "name": "mcp_servers_and_tools/memory_server",
                    "client_session_timeout_seconds": 500,
                    "params": {
                        "command": "python3",
                        "args": [self._mcp_memory_path],
                        "env": {
                            "SUPABASE_URL": os.getenv("SUPABASE_URL"),
                            "SUPABASE_SERVICE_KEY": os.getenv("SUPABASE_SERVICE_KEY"),
                            "SUPABASE_DATABASE_URL": os.getenv("SUPABASE_DATABASE_URL"),
                            "NEO4J_URI": os.getenv("NEO4J_URI"),
                            "NEO4J_USER": os.getenv("NEO4J_USER"),
                            "NEO4J_PASSWORD": os.getenv("NEO4J_PASSWORD"),
                            "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY", ""),
                            "MCP_QUIET": "1",
                            "TRANSPORT": "stdio"
                        },
                    }
                }

                memory_server = await get_or_create_mcp_server("mcp_servers_and_tools/memory_server", memory_server_config, working_dir=self._workspace_root)
                servers.append(memory_server)

            except Exception as e:
                print(f"âš ï¸  Memory server unavailable: {e}")

            self.mcp_servers = servers
            self._servers_initialized = True

            # Always print the message when servers are first initialized
            print("\nâœ… Solution researcher connected to MCP servers:")
            for server in self.mcp_servers:
                print(f"   â€¢ {server.name}")
            self._message_printed = True
    
    async def get_mcp_tools(self, run_context):
        """Get MCP tools, ensuring servers are initialized first."""
        await self._initialize_servers()
        return await super().get_mcp_tools(run_context)
    
    async def call_tool(self, tool_name: str, arguments: dict) -> Any:
        return await super().call_tool(tool_name, arguments)

async def create_solution_researcher_agent() -> SolutionResearcherAgent:
    """Create and return a solution researcher agent with web search capabilities."""
    try:
        # Check API key availability (commented out for non-OpenAI models)
        openai_api_key = os.environ.get("OPENAI_API_KEY", "")
        if not openai_api_key:
            raise ValueError("OPENAI_API_KEY environment variable is not set")

        # NOTE: When using MCP-based SolutionResearcherAgent, uncomment the block below and comment out the generic Agent below with direct tools.

        # agent = SolutionResearcherAgent(
        #     name="SolutionResearcherAgent",
        #     instructions=SOLUTION_RESEARCHER_PROMPT,
        #     model=OpenAIChatCompletionsModel(model=MODEL_NAME, openai_client=client),
        #     output_type=SolutionResponse,
        # )

        # Direct-tools Agent (kept active for running without MCP servers)
        
        agent = Agent(
            name="SolutionResearcherAgent",
            instructions=SOLUTION_RESEARCHER_PROMPT,
            model=OpenAIChatCompletionsModel(model=MODEL_NAME, openai_client=client),
            output_type=SolutionResponse,
            tools=[
                search_memory,  # Memory search for past solutions
                tavily_search,
                extract_code_from_url,
                retrieve_extracted_code,
                quick_introspect,
            ],
            # Only set this model_settings for OpenAI GPT-5's reasoning models (gpt-5, gpt-5-mini, or gpt-5-nano), for other models, do not set this model_settings
            # model_settings=ModelSettings(reasoning=Reasoning(effort="medium"), verbosity="low")
        )
        
        return agent
        
    except Exception as e:
        print(f"Error creating solution researcher agent: {e}")
        raise

# Export the necessary functions and classes
__all__ = [
    "SolutionResearcherAgent",
    "create_solution_researcher_agent"
]
```


================================================================================
=== FILE: conversational_system\deep_solver_with_memory\__init__.py ===
================================================================================

```python
"""
Free-form agent overrides for deep problem-solving workflow.

These agents use free-form output instead of structured Pydantic models,
allowing for more detailed explanations and flexible responses.

Set USE_ADAPTIVE_AGENTS=true to use adaptive agents for local/self-hosted models.
"""

import os

# Check if adaptive agents should be used (for local/self-hosted models)
USE_ADAPTIVE_AGENTS = os.getenv('USE_ADAPTIVE_AGENTS', 'false').lower() == 'true'

if USE_ADAPTIVE_AGENTS:
    # Use adaptive agents for local/self-hosted models
    from .adaptive_solution_researcher import create_adaptive_solution_researcher as create_solution_researcher_agent
    from .adaptive_code_agent import create_adaptive_code_agent as create_code_agent
    from .adaptive_debug_agent import create_adaptive_debug_agents as create_debug_agents
    print("Using adaptive agents for local/self-hosted models")
else:
    # Use standard agents for OpenAI models
    from .solution_researcher import create_solution_researcher_agent
    from .code_agent import create_code_agent
    from .debug_agent import create_debug_agents

from .output_processor_agent import create_output_processor_agent

__all__ = [
    'create_solution_researcher_agent',
    'create_code_agent',
    'create_debug_agents',
    'create_output_processor_agent'
]

```


================================================================================
=== FILE: conversational_system\frontend\session_manager.py ===
================================================================================

```python
"""
Session Manager for Multi-Turn Conversations

Manages conversation state using OpenAI Agents SDK's Session functionality.
Handles:
- Multi-turn dialogue history
- Current solution tracking (for improvement scenarios)
- User feedback status
- Timeout management (5 minutes without response)
"""

from __future__ import annotations

import os
import time
import asyncio
from typing import Optional, Dict, Any, Literal
from datetime import datetime
from agents import Runner, SQLiteSession

# Import MLflow for tracing
try:
    import mlflow
    MLFLOW_AVAILABLE = True
except ImportError:
    MLFLOW_AVAILABLE = False


# Global cache to track which databases have been checked/migrated
_MIGRATED_DATABASES = set()


def _ensure_saved_conversations_schema(db_path: str) -> bool:
    """
    Ensure the database has the saved conversations schema.

    This function checks if the required columns (is_saved, custom_title, notes)
    exist in the agent_sessions table. If not, it adds them automatically.

    This is called automatically when creating a ConversationSessionManager,
    so users don't need to run a separate migration script.

    Args:
        db_path: Path to the SQLite database

    Returns:
        True if schema is correct (or was successfully migrated), False on error
    """
    # Check if we've already migrated this database in this session
    if db_path in _MIGRATED_DATABASES:
        return True

    import sqlite3

    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # Check if table exists first
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='agent_sessions'")
        if not cursor.fetchone():
            # Table doesn't exist yet - it will be created when user starts first conversation
            conn.close()
            return True

        # Check existing columns
        cursor.execute("PRAGMA table_info(agent_sessions)")
        existing_columns = [col[1] for col in cursor.fetchall()]

        # Define required columns for saved conversations feature
        required_columns = {
            'is_saved': 'INTEGER DEFAULT 0',
            'custom_title': 'TEXT',
            'notes': 'TEXT'
        }

        # Add missing columns
        migrations_performed = []
        for column_name, column_type in required_columns.items():
            if column_name not in existing_columns:
                alter_query = f"ALTER TABLE agent_sessions ADD COLUMN {column_name} {column_type}"
                cursor.execute(alter_query)
                migrations_performed.append(column_name)

        conn.commit()
        conn.close()

        # Log migration if any columns were added
        if migrations_performed:
            print(f"âœ“ Database schema updated: added {', '.join(migrations_performed)} columns")

        # Mark this database as migrated
        _MIGRATED_DATABASES.add(db_path)
        return True

    except Exception as e:
        print(f"âš ï¸  Warning: Could not ensure saved conversations schema: {e}")
        return False


class ConversationSessionManager:
    """
    Manages state for a multi-turn conversation with a user.

    Uses SQLiteSession from OpenAI Agents SDK to automatically maintain
    conversation history across multiple agent runs.
    """

    def __init__(
        self,
        session_id: str,
        user_id: str,
        db_path: str = "conversations.db",
        load_existing: bool = False
    ):
        """
        Initialize conversation session.

        Args:
            session_id: Unique identifier for this conversation session
            user_id: User identifier (from authentication)
            db_path: Path to SQLite database for storing sessions
            load_existing: If True, load an existing session; if False, create new session with timestamp
        """
        self.session_id = session_id
        self.user_id = user_id
        self.db_path = db_path

        # Ensure database has saved conversations schema (auto-migration)
        _ensure_saved_conversations_schema(db_path)

        # Create SQLiteSession - automatically manages conversation history
        # If load_existing=True, uses session_id as-is (for resuming sessions)
        # If load_existing=False, session_id should already include timestamp
        self.session = SQLiteSession(
            session_id=session_id,
            db_path=db_path
        )

        # Conversation state
        self.last_activity = time.time()
        self.current_solution = None  # Stores current solution for improvement
        self.waiting_for_feedback = False
        self.feedback_received = None
        self.original_query = None  # Track the original user query before any improvements

        print(f"ðŸ“ Session {session_id} created for user {user_id}")

    def update_activity(self):
        """Update last activity timestamp."""
        self.last_activity = time.time()

    def get_idle_time(self) -> float:
        """
        Get time elapsed since last activity in seconds.

        Returns:
            Seconds since last activity
        """
        return time.time() - self.last_activity

    def is_timeout(self, timeout_seconds: int = 300) -> bool:
        """
        Check if session has timed out.

        Args:
            timeout_seconds: Timeout duration (default 300 = 5 minutes)

        Returns:
            True if session has timed out
        """
        return self.get_idle_time() > timeout_seconds

    async def run_conversation_turn(
        self,
        orchestrator,
        user_message: str
    ) -> Any:
        """
        Execute one turn of conversation with the Orchestrator.

        The session automatically includes full conversation history,
        so the agent has context from previous turns.

        IMPORTANT: This method injects the user_id at the beginning of each message
        so that the Orchestrator can pass it to tools that require user_id
        (like search_memory, solve_with_deep_solver, save_to_memory).

        Args:
            orchestrator: The Orchestrator agent
            user_message: User's message/question

        Returns:
            Agent run result
        """
        # Update activity timestamp
        self.update_activity()

        # Inject user_id and critical reminders at the beginning of the message
        # This allows the Orchestrator to extract user_id and pass it to tools
        # The reminders reinforce critical workflow steps without being saved to original query
        full_message = (
            f"[SYSTEM: user_id={self.user_id}]\n"
            f"[CRITICAL REMINDER: 1) FIRST call search_memory(query, user_id). "
            f"2) If the problem needs online search/deepresearch, documentation lookup, or investigation â†’ use solve_with_deep_solver(query, user_id) which has dedicated research agents with online search capabilities. "
            f"3) Only skip solve_with_deep_solver if: problem is very simple or you have working memory solution, AND you are 100% confident that you can solve it without any research.]\n"
            f"{user_message}"
        )

        print(f"\nðŸ’¬ User: {user_message[:100]}...")

        turn_start = time.time()

        # Use MLflow run - let autolog automatically create spans
        # DO NOT create manual spans - they interfere with autolog's trace hierarchy
        if MLFLOW_AVAILABLE:
            run_name = f"conversation_turn_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

            with mlflow.start_run(run_name=run_name):
                # Log parameters
                mlflow.log_param("session_id", self.session_id)
                mlflow.log_param("user_id", self.user_id)
                mlflow.log_param("message_length", len(user_message))
                mlflow.log_param("user_message_preview", user_message[:500] + "..." if len(user_message) > 500 else user_message)

                try:
                    # Run orchestrator - autolog will automatically create "AgentRunner.run" span
                    # and capture all agent/tool/LLM spans in proper hierarchy
                    runner = Runner()
                    result = await runner.run(
                        orchestrator,
                        full_message,
                        session=self.session,
                        max_turns=5000
                    )

                    # Update activity after response
                    self.update_activity()

                    turn_time = time.time() - turn_start
                    print(f"ðŸ¤– Orchestrator responded")

                    # Extract response text
                    response_text = str(result.final_output) if hasattr(result, 'final_output') else str(result)

                    # Log metrics to the run
                    mlflow.log_metric("execution_time_seconds", turn_time)
                    mlflow.log_metric("response_length", len(response_text))
                    mlflow.log_param("success", True)

                    return result

                except Exception as e:
                    turn_time = time.time() - turn_start
                    print(f"âŒ Orchestrator error: {str(e)}")

                    # Log error metrics
                    mlflow.log_metric("execution_time_seconds", turn_time)
                    mlflow.log_param("success", False)
                    mlflow.log_param("error_type", type(e).__name__)
                    mlflow.log_param("error_message", str(e))

                    raise
        else:
            # No MLflow - just run normally
            runner = Runner()
            result = await runner.run(
                orchestrator,
                full_message,
                session=self.session,
                max_turns=5000
            )
            self.update_activity()
            return result

    def store_current_solution(self, solution_data: Dict[str, Any]):
        """
        Store the current solution for potential improvement.

        Called after Orchestrator returns a solution.
        This allows user to request improvements based on current code.

        Args:
            solution_data: Dictionary containing:
                - final_code: The working code
                - explanation: Explanation of solution
                - execution_results: Raw execution output
        """
        self.current_solution = {
            "code": solution_data.get("final_code", ""),
            "explanation": solution_data.get("explanation", ""),
            "results": solution_data.get("execution_results", ""),
            "timestamp": datetime.now().isoformat()
        }
        self.waiting_for_feedback = True

        print("ðŸ’¾ Current solution stored for potential improvement")

    def get_current_solution_code(self) -> Optional[str]:
        """
        Get current solution code for improvement scenarios.

        Returns:
            Current solution code, or None if no solution stored
        """
        if self.current_solution:
            return self.current_solution.get("code")
        return None

    def set_original_query(self, query: str):
        """
        Set the original user query at the start of a problem-solving session.

        This should be called when user first asks a question.
        It will NOT be updated during improvement iterations.

        Args:
            query: The original user query
        """
        if self.original_query is None:
            self.original_query = query
            print(f"ðŸ“Œ Original query set: {query[:50]}...")

    def get_original_query(self) -> Optional[str]:
        """
        Get the original user query.

        Returns:
            Original query, or None if not set
        """
        return self.original_query

    def append_to_original_query(self, additional_context: str):
        """
        Append additional context to the original query.

        Used when user clicks "Continue" to add supplementary details
        to the original problem.

        Args:
            additional_context: Additional details to append
        """
        if self.original_query is not None:
            # Append with a clear separator
            self.original_query = f"{self.original_query}; {additional_context}"
            print(f"ðŸ“ Updated original query: {self.original_query[:100]}...")
        else:
            # If no original query exists, set it as the query
            self.original_query = additional_context
            print(f"ðŸ“Œ Original query set: {additional_context[:50]}...")

    def set_feedback(
        self,
        feedback_type: Literal["satisfied", "improve", "exit"]
    ):
        """
        Record user feedback on current solution.

        Args:
            feedback_type: Type of feedback
                - "satisfied": User is happy, save to memory
                - "improve": User wants improvements
                - "exit": User wants to stop
        """
        self.feedback_received = feedback_type
        self.waiting_for_feedback = False
        self.update_activity()

        print(f"ðŸ“Š User feedback: {feedback_type}")

    def clear_feedback(self):
        """Clear feedback status after processing."""
        self.feedback_received = None
        self.waiting_for_feedback = False

    def reset_for_new_query(self):
        """
        Reset state for a new query.

        Called after feedback is processed and moving to next question.
        """
        self.current_solution = None
        self.original_query = None  # Clear original query for next problem
        self.clear_feedback()
        self.update_activity()

    async def wait_for_feedback(
        self,
        timeout_seconds: int = 300,
        check_interval: float = 1.0
    ) -> Literal["satisfied", "improve", "exit", "timeout"]:
        """
        Wait for user feedback with timeout.

        Args:
            timeout_seconds: Timeout duration (default 300 = 5 minutes)
            check_interval: How often to check for feedback in seconds

        Returns:
            Feedback type or "timeout"
        """
        start_time = time.time()

        while (time.time() - start_time) < timeout_seconds:
            # Check if feedback received
            if self.feedback_received:
                return self.feedback_received

            # Wait before next check
            await asyncio.sleep(check_interval)

        # Timeout reached
        print("â±ï¸ Feedback timeout - 5 minutes elapsed")
        return "timeout"

    def clear_session(self):
        """
        Clear the conversation history.

        Useful for starting fresh or after logout.
        """
        self.session.clear_session()
        self.reset_for_new_query()
        print("ðŸ—‘ï¸ Session history cleared")

    def get_session_info(self) -> Dict[str, Any]:
        """
        Get information about current session.

        Returns:
            Dictionary with session metadata
        """
        return {
            "session_id": self.session_id,
            "user_id": self.user_id,
            "idle_time": self.get_idle_time(),
            "is_timeout": self.is_timeout(),
            "waiting_for_feedback": self.waiting_for_feedback,
            "has_current_solution": self.current_solution is not None
        }

    async def export_conversation_history(self) -> str:
        """
        Export the conversation history as Markdown.

        Returns:
            Markdown-formatted conversation history as string
        """
        try:
            # Get all conversation items from session (async method)
            items = await self.session.get_items()
            return self._export_as_markdown(items)

        except Exception as e:
            print(f"Error exporting conversation: {e}")
            return f"Error exporting conversation: {str(e)}"

    def _export_as_markdown(self, items: list) -> str:
        """
        Format conversation items as Markdown.

        Args:
            items: List of conversation items from SQLiteSession

        Returns:
            Markdown-formatted string
        """
        lines = []

        # Header
        lines.append("# ðŸ’¬ Conversation History")
        lines.append("")
        lines.append(f"> **Session:** `{self.session_id}`")
        lines.append(f"> **User:** `{self.user_id}`")
        lines.append(f"> **Exported:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append("")
        lines.append("---")
        lines.append("")

        if not items:
            lines.append("*No conversation history available.*")
            return "\n".join(lines)

        # Process conversation items
        for i, item in enumerate(items, 1):
            role = item.get("role", "unknown")
            content = item.get("content", "")

            # Format based on role
            if role == "user":
                lines.append("")
                lines.append(f"### ðŸ‘¤ User")
                lines.append("")
                lines.append(f"> {content}")
                lines.append("")

            elif role == "assistant":
                lines.append("")
                lines.append(f"### ðŸ¤– Assistant")
                lines.append("")

                # Parse content to extract code and results if present
                content_str = str(content)

                # Check if content has multiple sections (code, results)
                if "```" in content_str:
                    # Content has code blocks, preserve them
                    lines.append(content_str)
                else:
                    # Regular text content
                    lines.append(content_str)

                lines.append("")
                lines.append("---")
                lines.append("")

        # Footer
        lines.append("---")
        lines.append("")
        lines.append("<div align='center'>")
        lines.append("<i>Generated by CASCADE</i>")
        lines.append("</div>")

        return "\n".join(lines)

    @staticmethod
    def list_user_sessions(user_id: str, db_path: str = "conversations.db", limit: int = 20):
        """
        List all sessions for a given user.

        Args:
            user_id: User identifier
            db_path: Path to SQLite database
            limit: Maximum number of sessions to return

        Returns:
            List of tuples: [(session_id, created_at, updated_at), ...]
            Ordered by updated_at DESC (most recent first)
        """
        import sqlite3

        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()

            # Query sessions that belong to this user (session_id contains user_id)
            query = """
                SELECT session_id, created_at, updated_at
                FROM agent_sessions
                WHERE session_id LIKE ?
                ORDER BY updated_at DESC
                LIMIT ?
            """

            cursor.execute(query, (f"session_{user_id}%", limit))
            sessions = cursor.fetchall()

            conn.close()
            return sessions

        except Exception as e:
            print(f"Error listing sessions: {e}")
            return []

    @staticmethod
    def get_session_preview(session_id: str, db_path: str = "conversations.db"):
        """
        Get preview information for a session.

        Args:
            session_id: Session identifier
            db_path: Path to SQLite database

        Returns:
            Tuple: (preview_text, message_count)
            preview_text: First user message (max 50 chars)
            message_count: Total number of messages in session
        """
        import sqlite3
        import json

        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()

            # Get all messages for this session
            query = """
                SELECT message_data
                FROM agent_messages
                WHERE session_id = ?
                ORDER BY created_at ASC
            """

            cursor.execute(query, (session_id,))
            messages = cursor.fetchall()

            conn.close()

            if not messages:
                return ("Empty session", 0)

            # Find first user message for preview
            preview_text = "Session"
            for msg_data in messages:
                try:
                    msg = json.loads(msg_data[0])
                    if msg.get("role") == "user":
                        content = msg.get("content", "")
                        # Remove system tags like [SYSTEM: user_id=...] and [CRITICAL REMINDER:...]
                        if "[SYSTEM:" in content:
                            content = content.split("\n", 2)[-1]  # Take text after system lines
                        if "[CRITICAL REMINDER:" in content:
                            content = content.split("\n", 2)[-1]

                        preview_text = content.strip()[:50]
                        break
                except:
                    continue

            return (preview_text, len(messages))

        except Exception as e:
            print(f"Error getting session preview: {e}")
            return ("Error loading preview", 0)

    @staticmethod
    def load_session_messages(session_id: str, db_path: str = "conversations.db"):
        """
        Load all messages from a session in Streamlit format.

        Args:
            session_id: Session identifier
            db_path: Path to SQLite database

        Returns:
            List of message dicts: [{"role": "user/assistant", "content": "...", ...}, ...]
        """
        import sqlite3
        import json

        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()

            # Get all messages for this session
            query = """
                SELECT message_data
                FROM agent_messages
                WHERE session_id = ?
                ORDER BY created_at ASC
            """

            cursor.execute(query, (session_id,))
            messages = cursor.fetchall()

            conn.close()

            # Parse and format messages for Streamlit
            formatted_messages = []
            for msg_data in messages:
                try:
                    msg = json.loads(msg_data[0])
                    role = msg.get("role")
                    content = msg.get("content", "")

                    # Skip messages with invalid or missing role
                    if not role or role not in ["user", "assistant"]:
                        continue

                    # Handle content that might be a list (from agent responses)
                    if isinstance(content, list):
                        # Extract text from list of dicts (agent response format)
                        text_parts = []
                        for item in content:
                            if isinstance(item, dict):
                                if "text" in item:
                                    text_parts.append(str(item["text"]))
                            else:
                                text_parts.append(str(item))
                        content = "\n".join(text_parts) if text_parts else ""

                    # Convert to string if not already
                    content = str(content) if content else ""

                    # Skip empty messages
                    if not content.strip():
                        continue

                    # Clean up system tags from user messages
                    if role == "user":
                        # Remove [SYSTEM: user_id=...] and [CRITICAL REMINDER:...] lines
                        lines = content.split("\n")
                        cleaned_lines = []
                        for line in lines:
                            if not line.startswith("[SYSTEM:") and not line.startswith("[CRITICAL REMINDER:") and not line.startswith("[ORIGINAL_QUERY:"):
                                cleaned_lines.append(line)
                        content = "\n".join(cleaned_lines).strip()

                    # Skip if content is now empty after cleaning
                    if not content:
                        continue

                    formatted_messages.append({
                        "role": role,
                        "content": content
                    })

                except Exception as e:
                    print(f"Error parsing message: {e}")
                    continue

            return formatted_messages

        except Exception as e:
            print(f"Error loading session messages: {e}")
            return []

    @staticmethod
    def toggle_saved_status(session_id: str, is_saved: bool, db_path: str = "conversations.db"):
        """
        Toggle the saved status of a session.

        Args:
            session_id: Session identifier
            is_saved: True to mark as saved, False to unsave
            db_path: Path to SQLite database

        Returns:
            True if successful, False otherwise
        """
        import sqlite3

        # Ensure schema has required columns
        _ensure_saved_conversations_schema(db_path)

        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()

            # Update the is_saved flag
            query = """
                UPDATE agent_sessions
                SET is_saved = ?
                WHERE session_id = ?
            """

            cursor.execute(query, (1 if is_saved else 0, session_id))
            conn.commit()
            conn.close()

            print(f"{'ðŸ’¾' if is_saved else 'ðŸ“¤'} Session {session_id[:8]}... {'saved' if is_saved else 'unsaved'}")
            return True

        except Exception as e:
            print(f"Error toggling saved status: {e}")
            return False

    @staticmethod
    def set_session_metadata(
        session_id: str,
        custom_title: Optional[str] = None,
        notes: Optional[str] = None,
        db_path: str = "conversations.db"
    ):
        """
        Set custom title and notes for a session.

        Args:
            session_id: Session identifier
            custom_title: Custom title for the session (can be empty string to clear)
            notes: Notes/description for the session (can be empty string to clear)
            db_path: Path to SQLite database

        Returns:
            True if successful, False otherwise
        """
        import sqlite3

        # Ensure schema has required columns
        _ensure_saved_conversations_schema(db_path)

        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()

            # Build update query dynamically based on what's provided
            # Now we check for None (meaning not provided) vs empty string (meaning clear the field)
            updates = []
            params = []

            # Use a sentinel to detect if argument was actually passed
            # If custom_title is not None, it was passed (even if empty string)
            if custom_title is not None:
                updates.append("custom_title = ?")
                # Convert empty string to None in database (NULL)
                params.append(custom_title if custom_title else None)

            if notes is not None:
                updates.append("notes = ?")
                # Convert empty string to None in database (NULL)
                params.append(notes if notes else None)

            if not updates:
                # Nothing to update
                conn.close()
                return True

            params.append(session_id)
            query = f"""
                UPDATE agent_sessions
                SET {', '.join(updates)}
                WHERE session_id = ?
            """

            cursor.execute(query, params)
            conn.commit()
            conn.close()

            print(f"ðŸ“ Session metadata updated for {session_id[:8]}...")
            return True

        except Exception as e:
            print(f"Error setting session metadata: {e}")
            return False

    @staticmethod
    def list_saved_sessions(user_id: str, db_path: str = "conversations.db"):
        """
        List all saved sessions for a given user.

        Args:
            user_id: User identifier
            db_path: Path to SQLite database

        Returns:
            List of dicts: [{"session_id": "...", "custom_title": "...", "notes": "...", ...}, ...]
            Ordered by updated_at DESC (most recent first)
        """
        import sqlite3

        # Ensure schema has required columns
        _ensure_saved_conversations_schema(db_path)

        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()

            # Query saved sessions that belong to this user
            query = """
                SELECT session_id, created_at, updated_at, custom_title, notes
                FROM agent_sessions
                WHERE session_id LIKE ? AND is_saved = 1
                ORDER BY updated_at DESC
            """

            cursor.execute(query, (f"session_{user_id}%",))
            rows = cursor.fetchall()

            conn.close()

            # Format as list of dicts
            sessions = []
            for row in rows:
                sessions.append({
                    "session_id": row[0],
                    "created_at": row[1],
                    "updated_at": row[2],
                    "custom_title": row[3],
                    "notes": row[4]
                })

            return sessions

        except Exception as e:
            print(f"Error listing saved sessions: {e}")
            return []

    @staticmethod
    def get_session_metadata(session_id: str, db_path: str = "conversations.db"):
        """
        Get metadata for a specific session.

        Args:
            session_id: Session identifier
            db_path: Path to SQLite database

        Returns:
            Dict with metadata: {"is_saved": bool, "custom_title": str, "notes": str}
            Returns None if session not found
        """
        import sqlite3

        # Ensure schema has required columns
        _ensure_saved_conversations_schema(db_path)

        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()

            query = """
                SELECT is_saved, custom_title, notes
                FROM agent_sessions
                WHERE session_id = ?
            """

            cursor.execute(query, (session_id,))
            row = cursor.fetchone()

            conn.close()

            if row:
                return {
                    "is_saved": bool(row[0]),
                    "custom_title": row[1],
                    "notes": row[2]
                }
            else:
                return None

        except Exception as e:
            print(f"Error getting session metadata: {e}")
            return None


class SessionRegistry:
    """
    Registry to manage multiple active sessions.

    Useful for tracking sessions across multiple users.
    """

    def __init__(self):
        """Initialize session registry."""
        self.sessions: Dict[str, ConversationSessionManager] = {}

    def create_session(
        self,
        session_id: str,
        user_id: str,
        db_path: str = "conversations.db"
    ) -> ConversationSessionManager:
        """
        Create and register a new session.

        Args:
            session_id: Unique session identifier
            user_id: User identifier
            db_path: Database path

        Returns:
            New ConversationSessionManager instance
        """
        session = ConversationSessionManager(session_id, user_id, db_path)
        self.sessions[session_id] = session
        return session

    def get_session(self, session_id: str) -> Optional[ConversationSessionManager]:
        """
        Get existing session by ID.

        Args:
            session_id: Session identifier

        Returns:
            Session manager or None if not found
        """
        return self.sessions.get(session_id)

    def remove_session(self, session_id: str):
        """
        Remove session from registry.

        Args:
            session_id: Session identifier
        """
        if session_id in self.sessions:
            del self.sessions[session_id]
            print(f"ðŸ—‘ï¸ Session {session_id} removed from registry")

    def cleanup_timeout_sessions(self, timeout_seconds: int = 300):
        """
        Remove timed-out sessions from registry.

        Args:
            timeout_seconds: Timeout threshold
        """
        timeout_sessions = [
            sid for sid, session in self.sessions.items()
            if session.is_timeout(timeout_seconds)
        ]

        for sid in timeout_sessions:
            self.remove_session(sid)

        if timeout_sessions:
            print(f"ðŸ§¹ Cleaned up {len(timeout_sessions)} timed-out sessions")


# Global session registry
_session_registry = SessionRegistry()


def get_session_registry() -> SessionRegistry:
    """Get the global session registry."""
    return _session_registry


# Export
__all__ = [
    'ConversationSessionManager',
    'SessionRegistry',
    'get_session_registry'
]

```


================================================================================
=== FILE: conversational_system\frontend\streamlit_app.py ===
================================================================================

```python
"""
Streamlit Frontend for CASCADE (Conversational Materials Science and Chemistry Assistant)

Features:
- User authentication with Supabase
- Multi-turn conversation with memory
- Code display and execution results
- User feedback buttons (Satisfied / Improve / Exit)
- Timeout handling (5 minutes)
- User preferences management (API keys)
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# Load environment variables FIRST
# Priority: .env file > shell environment (.bashrc) > code defaults
current_dir = Path(__file__).resolve().parent
conversational_system_dir = current_dir.parent
project_root = conversational_system_dir.parent
sys.path.insert(0, str(project_root))
env_path = project_root / '.env'
load_dotenv(env_path, override=True)

# Database path for conversation history (can be overridden via environment variable)
CONVERSATIONS_DB_PATH = os.getenv(
    "CONVERSATIONS_DB_PATH",
    str(conversational_system_dir / "conversations.db")
)

# Clean up temp_code directory ONLY on first startup (not on every Streamlit re-run)
# Use environment variable to track if cleanup has been done this session
if not os.getenv("_TEMP_CODE_CLEANED"):
    temp_code_dir = conversational_system_dir / "temp_code"
    if temp_code_dir.exists():
        import shutil
        for item in temp_code_dir.iterdir():
            if item.name != ".gitkeep":
                if item.is_file():
                    item.unlink()
                else:
                    shutil.rmtree(item)
        print(f"ðŸ§¹ Cleaned up temp_code directory: {temp_code_dir}")
    os.environ["_TEMP_CODE_CLEANED"] = "1"

# Now import other modules
import asyncio
import streamlit as st
from datetime import datetime
import supabase
from supabase import create_client, Client

# Initialize MLflow tracing for the conversational system
try:
    import mlflow
    mlflow_uri = os.getenv("MLFLOW_TRACKING_URI", "http://localhost:5001")
    mlflow.set_tracking_uri(mlflow_uri)
    # NOTE: log_traces=True is required for both individual API calls AND agent traces
    # Individual traces (embedding/completion) can be filtered out in the UI or via queries
    mlflow.openai.autolog(disable=False, log_traces=True)
    mlflow.tracing.enable()
    mlflow.set_experiment("conversational_system")
    print(f"âœ… MLflow tracing enabled at {mlflow_uri}")
except Exception as e:
    print(f"âš ï¸  MLflow tracing not available: {e}")

# Import our components
from conversational_system.core.orchestrator import create_orchestrator
from conversational_system.core.deep_solver import solve_with_deep_solver
from conversational_system.frontend.session_manager import (
    ConversationSessionManager,
    get_session_registry
)

# Initialize Supabase client
supabase_url = os.environ.get("SUPABASE_URL", "")
# Try SUPABASE_SERVICE_KEY first (standard naming), fallback to SUPABASE_KEY
supabase_key = os.environ.get("SUPABASE_SERVICE_KEY") or os.environ.get("SUPABASE_KEY", "")

if not supabase_url or not supabase_key:
    st.error("âš ï¸ Supabase credentials not configured. Please set SUPABASE_URL and SUPABASE_SERVICE_KEY in .env file")
    st.info("ðŸ’¡ To set up Supabase, please refer to the QUICKSTART.md guide")
    st.stop()

supabase_client: Client = create_client(supabase_url, supabase_key)

# Page configuration
st.set_page_config(
    page_title="CASCADE",
    page_icon="ðŸ§ª",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better UI
st.markdown("""
<style>
.stButton>button {
    width: 100%;
}
.feedback-button {
    margin: 5px;
}
</style>
""", unsafe_allow_html=True)


# Authentication functions

def sign_up(email: str, password: str, full_name: str):
    """Register new user."""
    try:
        response = supabase_client.auth.sign_up({
            "email": email,
            "password": password,
            "options": {
                "data": {
                    "full_name": full_name
                }
            }
        })
        if response and response.user:
            st.session_state.authenticated = True
            st.session_state.user = response.user
            st.rerun()
        return response
    except Exception as e:
        st.error(f"âŒ Error signing up: {str(e)}")
        return None


def sign_in(email: str, password: str):
    """Sign in existing user."""
    try:
        response = supabase_client.auth.sign_in_with_password({
            "email": email,
            "password": password
        })
        if response and response.user:
            st.session_state.authenticated = True
            st.session_state.user = response.user
            st.rerun()
        return response
    except Exception as e:
        st.error(f"âŒ Error signing in: {str(e)}")
        return None


def sign_out():
    """Sign out current user."""
    try:
        supabase_client.auth.sign_out()
        st.session_state.authenticated = False
        st.session_state.user = None
        st.session_state.logout_requested = True
    except Exception as e:
        st.error(f"âŒ Error signing out: {str(e)}")


# Initialize session state

if "messages" not in st.session_state:
    st.session_state.messages = []

if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

if "user" not in st.session_state:
    st.session_state.user = None

if "conversation_session" not in st.session_state:
    st.session_state.conversation_session = None

if "orchestrator" not in st.session_state:
    st.session_state.orchestrator = None

if "waiting_for_feedback" not in st.session_state:
    st.session_state.waiting_for_feedback = False

if "current_solution" not in st.session_state:
    st.session_state.current_solution = None

if "improvement_mode" not in st.session_state:
    st.session_state.improvement_mode = False

if "continue_mode" not in st.session_state:
    st.session_state.continue_mode = False

# Check for logout flag
if st.session_state.get("logout_requested", False):
    st.session_state.logout_requested = False
    st.rerun()


# Async function to initialize orchestrator

async def initialize_orchestrator():
    """Initialize Orchestrator with DeepSolver tool."""
    if st.session_state.orchestrator is None:
        orchestrator = await create_orchestrator(deep_solver_tool=solve_with_deep_solver)
        st.session_state.orchestrator = orchestrator
        return orchestrator
    return st.session_state.orchestrator


# Main app

def main():
    """Main application entry point."""

    # Sidebar for authentication and settings
    with st.sidebar:
        st.title("ðŸ§ª CASCADE")

        if not st.session_state.authenticated:
            # Login/Signup tabs
            tab1, tab2 = st.tabs(["Login", "Sign Up"])

            with tab1:
                st.subheader("Login")
                login_email = st.text_input("Email", key="login_email")
                login_password = st.text_input("Password", type="password", key="login_password")
                login_button = st.button("Login", key="login_btn")

                if login_button:
                    if login_email and login_password:
                        sign_in(login_email, login_password)
                    else:
                        st.warning("âš ï¸ Please enter both email and password")

            with tab2:
                st.subheader("Sign Up")
                signup_email = st.text_input("Email", key="signup_email")
                signup_password = st.text_input("Password", type="password", key="signup_password")
                signup_name = st.text_input("Full Name", key="signup_name")
                signup_button = st.button("Sign Up", key="signup_btn")

                if signup_button:
                    if signup_email and signup_password and signup_name:
                        response = sign_up(signup_email, signup_password, signup_name)
                        if response and response.user:
                            st.success("âœ… Sign up successful! Please check your email to confirm.")
                    else:
                        st.warning("âš ï¸ Please fill in all fields")

        else:
            # User is authenticated
            user = st.session_state.user
            st.success(f"ðŸ‘¤ Logged in as: {user.email}")
            st.button("Logout", on_click=sign_out, key="logout_btn")

            st.divider()

            # =====================================================
            # Saved Conversations Section
            # =====================================================
            st.subheader("â­ Saved Conversations")

            # Get database path
            if st.session_state.conversation_session:
                db_path = st.session_state.conversation_session.db_path
            else:
                db_path = CONVERSATIONS_DB_PATH

            # List saved sessions
            try:
                saved_sessions = ConversationSessionManager.list_saved_sessions(user.id, db_path=db_path)

                if saved_sessions:
                    # Use expander for saved conversations
                    with st.expander(f"ðŸ“Œ {len(saved_sessions)} Saved", expanded=True):
                        for session_data in saved_sessions:
                            session_id = session_data["session_id"]
                            custom_title = session_data["custom_title"]
                            notes = session_data["notes"]

                            # Get preview if no custom title
                            if custom_title:
                                display_title = custom_title
                            else:
                                try:
                                    preview_result = ConversationSessionManager.get_session_preview(session_id, db_path=db_path)
                                    if preview_result and len(preview_result) == 2:
                                        display_title, msg_count = preview_result
                                    else:
                                        display_title = "Saved Session"
                                except:
                                    display_title = "Saved Session"

                            # Current session indicator
                            is_current = bool(
                                st.session_state.conversation_session and
                                st.session_state.conversation_session.session_id == session_id
                            )

                            # Create columns for session button and unsave button
                            col1, col2 = st.columns([4, 1])

                            with col1:
                                button_label = f"{'ðŸŸ¢' if is_current else 'â­'} {display_title[:40]}"
                                if st.button(
                                    button_label,
                                    key=f"load_saved_{session_id}",
                                    disabled=is_current,
                                    use_container_width=True
                                ):
                                    # Load session
                                    st.session_state.conversation_session = ConversationSessionManager(
                                        session_id=session_id,
                                        user_id=user.id,
                                        db_path=db_path,
                                        load_existing=True
                                    )

                                    # Load messages
                                    loaded_messages = ConversationSessionManager.load_session_messages(session_id, db_path=db_path)
                                    st.session_state.messages = loaded_messages

                                    # Reset feedback states
                                    st.session_state.waiting_for_feedback = False
                                    st.session_state.current_solution = None
                                    st.session_state.improvement_mode = False
                                    st.session_state.continue_mode = False

                                    st.success("âœ… Loaded saved conversation")
                                    st.rerun()

                            with col2:
                                # Unsave button
                                if st.button("â˜†", key=f"unsave_{session_id}", help="Remove from saved"):
                                    ConversationSessionManager.toggle_saved_status(session_id, False, db_path)
                                    st.success("Removed from saved")
                                    st.rerun()

                            # Show notes if present
                            if notes:
                                st.caption(f"ðŸ“ {notes[:50]}{'...' if len(notes) > 50 else ''}")

                else:
                    st.info("No saved conversations yet")

            except Exception as e:
                st.error(f"âŒ Error loading saved sessions: {e}")

            st.divider()

            # Conversation History section
            st.subheader("ðŸ“š Conversation History")

            # New Conversation button
            if st.button("âž• New Conversation", key="new_conv_btn", use_container_width=True):
                # Create new session with timestamp
                new_session_id = f"session_{user.id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                db_path = CONVERSATIONS_DB_PATH
                st.session_state.conversation_session = ConversationSessionManager(
                    session_id=new_session_id,
                    user_id=user.id,
                    db_path=db_path,
                    load_existing=False
                )
                # Clear messages for new conversation
                st.session_state.messages = []
                st.session_state.waiting_for_feedback = False
                st.session_state.current_solution = None
                st.session_state.improvement_mode = False
                st.session_state.continue_mode = False
                st.success("âœ… New conversation started!")
                st.rerun()

            # List previous sessions
            try:
                # Get database path from existing session or use default
                if st.session_state.conversation_session:
                    db_path = st.session_state.conversation_session.db_path
                else:
                    # Use parent directory for database (since we're in frontend/)
                    db_path = CONVERSATIONS_DB_PATH

                sessions = ConversationSessionManager.list_user_sessions(user.id, db_path=db_path, limit=20)

                if sessions:
                    # Group sessions by date
                    from datetime import datetime as dt, timedelta
                    now = dt.now()
                    today = now.date()
                    yesterday = today - timedelta(days=1)
                    week_ago = today - timedelta(days=7)

                    grouped_sessions = {
                        "Today": [],
                        "Yesterday": [],
                        "This Week": [],
                        "Older": []
                    }

                    for session_id, created_at, updated_at in sessions:
                        # Parse updated_at timestamp (format: YYYY-MM-DD HH:MM:SS)
                        try:
                            # Handle both string and None types
                            if updated_at:
                                session_date = dt.strptime(updated_at.split('.')[0], '%Y-%m-%d %H:%M:%S').date()
                            else:
                                session_date = today
                        except Exception as parse_error:
                            session_date = today  # Fallback to today if parse fails

                        if session_date == today:
                            grouped_sessions["Today"].append((session_id, created_at, updated_at))
                        elif session_date == yesterday:
                            grouped_sessions["Yesterday"].append((session_id, created_at, updated_at))
                        elif session_date > week_ago:
                            grouped_sessions["This Week"].append((session_id, created_at, updated_at))
                        else:
                            grouped_sessions["Older"].append((session_id, created_at, updated_at))

                    # Display grouped sessions
                    for group_name, group_sessions in grouped_sessions.items():
                        if group_sessions:
                            st.write(f"**{group_name}**")

                            for session_id, created_at, updated_at in group_sessions:
                                # Get preview
                                try:
                                    preview_result = ConversationSessionManager.get_session_preview(session_id, db_path=db_path)
                                    if preview_result and len(preview_result) == 2:
                                        preview_text, msg_count = preview_result
                                    else:
                                        preview_text = "Unknown session"
                                        msg_count = 0
                                except Exception as preview_error:
                                    preview_text = "Error loading preview"
                                    msg_count = 0

                                # Get saved status
                                try:
                                    metadata = ConversationSessionManager.get_session_metadata(session_id, db_path=db_path)
                                    is_saved = metadata["is_saved"] if metadata else False
                                except:
                                    is_saved = False

                                # Current session indicator
                                is_current = bool(
                                    st.session_state.conversation_session and
                                    st.session_state.conversation_session.session_id == session_id
                                )

                                # Create columns for session button and star button
                                col1, col2 = st.columns([4, 1])

                                with col1:
                                    # Create button label
                                    if is_current:
                                        button_label = f"ðŸŸ¢ {preview_text} ({msg_count} msgs)"
                                    else:
                                        button_label = f"ðŸ’¬ {preview_text} ({msg_count} msgs)"

                                    # Session button
                                    if st.button(
                                        button_label,
                                        key=f"load_{session_id}",
                                        disabled=is_current,
                                        use_container_width=True
                                    ):
                                        # Load session
                                        st.session_state.conversation_session = ConversationSessionManager(
                                            session_id=session_id,
                                            user_id=user.id,
                                            db_path=db_path,
                                            load_existing=True
                                        )

                                        # Load messages
                                        loaded_messages = ConversationSessionManager.load_session_messages(session_id, db_path=db_path)
                                        st.session_state.messages = loaded_messages

                                        # Reset feedback states
                                        st.session_state.waiting_for_feedback = False
                                        st.session_state.current_solution = None
                                        st.session_state.improvement_mode = False
                                        st.session_state.continue_mode = False

                                        st.success(f"âœ… Loaded session with {msg_count} messages")
                                        st.rerun()

                                with col2:
                                    # Star/unstar button
                                    star_icon = "â­" if is_saved else "â˜†"
                                    star_help = "Remove from saved" if is_saved else "Save conversation"
                                    if st.button(star_icon, key=f"star_{session_id}", help=star_help):
                                        ConversationSessionManager.toggle_saved_status(session_id, not is_saved, db_path)
                                        st.success(f"{'Saved' if not is_saved else 'Unsaved'} conversation")
                                        st.rerun()
                else:
                    st.info("No previous conversations yet")

            except Exception as e:
                st.error(f"âŒ Error loading sessions: {e}")

            st.divider()

            # Session info
            st.subheader("ðŸ“Š Session Info")
            if st.session_state.conversation_session:
                session_info = st.session_state.conversation_session.get_session_info()
                st.caption(f"Session ID: {session_info['session_id'][:8]}...")
                st.caption(f"Idle time: {int(session_info['idle_time'])}s")

                # Export conversation history
                st.write("**ðŸ“¥ Export Chat History**")

                if st.button("Export Conversation", key="export_md"):
                    try:
                        # Call async method using asyncio.run()
                        markdown_content = asyncio.run(
                            st.session_state.conversation_session.export_conversation_history()
                        )

                        # Create download button
                        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                        filename = f"conversation_{timestamp}.md"

                        st.download_button(
                            label="ðŸ“¥ Download Markdown",
                            data=markdown_content,
                            file_name=filename,
                            mime="text/markdown",
                            key="download_md"
                        )
                        st.success("âœ… Ready to download!")
                    except Exception as e:
                        st.error(f"âŒ Export error: {e}")

                st.divider()

                # Save Conversation section
                st.write("**ðŸ’¾ Save This Conversation**")

                # Get current session metadata
                current_session_id = st.session_state.conversation_session.session_id
                try:
                    metadata = ConversationSessionManager.get_session_metadata(current_session_id, db_path)
                    is_saved = metadata["is_saved"] if metadata else False
                    current_title = metadata["custom_title"] if metadata and metadata["custom_title"] else ""
                    current_notes = metadata["notes"] if metadata and metadata["notes"] else ""
                except:
                    is_saved = False
                    current_title = ""
                    current_notes = ""

                if is_saved:
                    st.info("â­ This conversation is saved")
                    # Show edit form
                    with st.expander("âœï¸ Edit Title & Notes", expanded=False):
                        edit_title = st.text_input("Title", value=current_title, key="edit_title", placeholder="Enter a title (optional)")
                        edit_notes = st.text_area("Notes", value=current_notes, key="edit_notes", placeholder="Add notes about this conversation")

                        col1, col2 = st.columns(2)
                        with col1:
                            if st.button("Update", key="update_metadata", use_container_width=True):
                                # Always pass the values (including empty strings) to allow clearing
                                ConversationSessionManager.set_session_metadata(
                                    current_session_id,
                                    custom_title=edit_title.strip() if edit_title else "",
                                    notes=edit_notes.strip() if edit_notes else "",
                                    db_path=db_path
                                )
                                st.success("âœ… Updated!")
                                st.rerun()

                        with col2:
                            if st.button("Remove from Saved", key="remove_saved", use_container_width=True):
                                ConversationSessionManager.toggle_saved_status(current_session_id, False, db_path)
                                st.success("Removed from saved")
                                st.rerun()
                else:
                    # Show save form
                    with st.expander("ðŸ’¾ Save with Title & Notes", expanded=False):
                        save_title = st.text_input("Title", key="save_title", placeholder="Enter a title (optional)")
                        save_notes = st.text_area("Notes", key="save_notes", placeholder="Add notes about this conversation")

                        if st.button("Save Conversation", key="save_conv", use_container_width=True):
                            # Set metadata first (if provided)
                            if save_title or save_notes:
                                ConversationSessionManager.set_session_metadata(
                                    current_session_id,
                                    custom_title=save_title if save_title else None,
                                    notes=save_notes if save_notes else None,
                                    db_path=db_path
                                )
                            # Mark as saved
                            ConversationSessionManager.toggle_saved_status(current_session_id, True, db_path)
                            st.success("âœ… Conversation saved!")
                            st.rerun()

                st.divider()

                if st.button("Clear Conversation History"):
                    st.session_state.conversation_session.clear_session()
                    st.session_state.messages = []
                    st.success("ðŸ—‘ï¸ History cleared")
                    st.rerun()

    # Main conversation area
    if st.session_state.authenticated and st.session_state.user:
        user_id = st.session_state.user.id

        # Initialize conversation session if needed
        if st.session_state.conversation_session is None:
            session_id = f"session_{user_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            db_path = CONVERSATIONS_DB_PATH
            st.session_state.conversation_session = ConversationSessionManager(
                session_id=session_id,
                user_id=user_id,
                db_path=db_path
            )

        # Optional: Check for very long inactivity (30 minutes) to prevent memory leaks
        # This only clears the feedback state, NOT the conversation history
        if st.session_state.conversation_session.is_timeout(timeout_seconds=1800):  # 30 minutes
            if st.session_state.waiting_for_feedback:
                st.info("â„¹ï¸ Feedback timeout - you can continue the conversation or start a new question.")
                # Only clear feedback state, keep conversation history
                st.session_state.waiting_for_feedback = False
                st.session_state.current_solution = None
                st.session_state.improvement_mode = False

        st.title("ðŸ’¬ Conversational Assistant")
        st.write("Ask me anything about computational materials science and chemistry!")

        # Display conversation history
        for msg in st.session_state.messages:
            with st.chat_message(msg["role"]):
                st.write(msg["content"])

                # Show code if present
                if "code" in msg and msg["code"]:
                    with st.expander("ðŸ“ View Code"):
                        st.code(msg["code"], language="python")

                # Show execution results if present
                if "results" in msg and msg["results"]:
                    with st.expander("ðŸ“Š Execution Results"):
                        st.text(msg["results"])

        # Feedback buttons (if waiting for feedback)
        if st.session_state.waiting_for_feedback:
            st.divider()
            st.write("**How would you like to proceed?**")

            col1, col2, col3, col4 = st.columns(4)

            with col1:
                if st.button("âœ… Save Solution", type="primary", key="save_btn"):
                    # Send satisfaction message to Orchestrator to trigger save_to_memory
                    # Include original query for context
                    original_q = st.session_state.conversation_session.get_original_query()
                    if original_q:
                        satisfaction_message = f"[ORIGINAL_QUERY: {original_q}]\nI'm satisfied with the solution. Please save it to memory for future reference."
                        display_message = "I'm satisfied with the solution. Please save it to memory for future reference."
                    else:
                        satisfaction_message = "I'm satisfied with the solution. Please save it to memory for future reference."
                        display_message = satisfaction_message

                    # Add to message history (display version without ORIGINAL_QUERY tag)
                    st.session_state.messages.append({
                        "role": "user",
                        "content": display_message
                    })

                    # Display user message
                    with st.chat_message("user"):
                        st.write(display_message)

                    # Clear feedback state
                    st.session_state.waiting_for_feedback = False
                    st.session_state.current_solution = None

                    # Run orchestrator to save the solution
                    with st.spinner("ðŸ’¾ Saving solution to memory..."):
                        try:
                            # Initialize orchestrator if needed
                            orchestrator = asyncio.run(initialize_orchestrator())

                            # Run conversation turn (use satisfaction_message with ORIGINAL_QUERY tag)
                            result = asyncio.run(
                                st.session_state.conversation_session.run_conversation_turn(
                                    orchestrator,
                                    satisfaction_message  # This includes [ORIGINAL_QUERY: xxx] if available
                                )
                            )

                            # Get response
                            response_text = str(result.final_output) if hasattr(result, 'final_output') else str(result)

                            # Add assistant response to history
                            st.session_state.messages.append({
                                "role": "assistant",
                                "content": response_text
                            })

                            # Display success message
                            with st.chat_message("assistant"):
                                st.write(response_text)

                            # Reset for new query
                            st.session_state.conversation_session.reset_for_new_query()

                        except Exception as e:
                            st.error(f"âŒ Error: {str(e)}")

                    st.rerun()

            with col2:
                if st.button("ðŸ”§ Request Improvements", key="improve_btn"):
                    st.session_state.improvement_mode = True
                    # Keep waiting_for_feedback=True to preserve original_query tracking
                    # (we're still in the same problem-solving session)
                    st.rerun()

            with col3:
                if st.button("âž• Continue", key="continue_btn"):
                    st.session_state.continue_mode = True
                    # Keep waiting_for_feedback=True
                    st.rerun()

            with col4:
                if st.button("âŒ Exit", key="exit_btn"):
                    # Send exit message to Orchestrator
                    exit_message = "I don't want to save this solution. Let's move on."

                    # Add to message history
                    st.session_state.messages.append({
                        "role": "user",
                        "content": exit_message
                    })

                    # Display user message
                    with st.chat_message("user"):
                        st.write(exit_message)

                    # Clear feedback state but keep conversation history
                    st.session_state.waiting_for_feedback = False
                    st.session_state.current_solution = None

                    # Run orchestrator to acknowledge exit
                    with st.spinner("Processing..."):
                        try:
                            # Initialize orchestrator if needed
                            orchestrator = asyncio.run(initialize_orchestrator())

                            # Run conversation turn
                            result = asyncio.run(
                                st.session_state.conversation_session.run_conversation_turn(
                                    orchestrator,
                                    exit_message
                                )
                            )

                            # Get response
                            response_text = str(result.final_output) if hasattr(result, 'final_output') else str(result)

                            # Add assistant response to history
                            st.session_state.messages.append({
                                "role": "assistant",
                                "content": response_text
                            })

                            # Display response
                            with st.chat_message("assistant"):
                                st.write(response_text)

                            # Reset for new query (clears original_query for next problem)
                            st.session_state.conversation_session.reset_for_new_query()

                        except Exception as e:
                            st.error(f"âŒ Error: {str(e)}")

                    st.rerun()

        # Improvement request input
        if st.session_state.improvement_mode:
            st.divider()
            st.subheader("ðŸ”§ Request Improvements")

            improvement_text = st.text_area(
                "What would you like to improve?",
                placeholder="Describe what you'd like to change or improve...",
                key="improvement_text"
            )

            if st.button("Submit Improvement Request", key="submit_improvement"):
                if improvement_text:
                    # Format as improvement request, include original query for context
                    original_q = st.session_state.conversation_session.get_original_query()
                    if original_q:
                        user_input = f"[ORIGINAL_QUERY: {original_q}]\nðŸ”§ Improvement request: {improvement_text}"
                    else:
                        user_input = f"ðŸ”§ Improvement request: {improvement_text}"

                    # Add user message to history (without the ORIGINAL_QUERY tag for display)
                    display_message = f"ðŸ”§ Improvement request: {improvement_text}"
                    st.session_state.messages.append({
                        "role": "user",
                        "content": display_message
                    })

                    # Display user message
                    with st.chat_message("user"):
                        st.write(display_message)

                    st.session_state.improvement_mode = False

                    # Run orchestrator to process the improvement request
                    with st.spinner("ðŸ¤” Processing improvement request..."):
                        try:
                            # Initialize orchestrator if needed
                            orchestrator = asyncio.run(initialize_orchestrator())

                            # Run conversation turn
                            result = asyncio.run(
                                st.session_state.conversation_session.run_conversation_turn(
                                    orchestrator,
                                    user_input
                                )
                            )

                            # Process result
                            assistant_response = result.final_output

                            # Handle None or empty response first
                            if assistant_response is None:
                                response_text = "I apologize, but I couldn't generate a proper response. Could you please rephrase your question?"
                                code = ""
                                results = ""
                                st.session_state.waiting_for_feedback = False

                            # Extract components if structured
                            elif isinstance(assistant_response, dict):
                                response_text = assistant_response.get("explanation", "")
                                code = assistant_response.get("final_code", "")
                                results = assistant_response.get("execution_results", "")
                                success = assistant_response.get("success", False)

                                # Check if response is empty
                                if not response_text or not response_text.strip():
                                    response_text = "I apologize, but I couldn't generate a proper response. Could you please rephrase your question?"
                                    st.session_state.waiting_for_feedback = False
                                # Store current solution if successful
                                elif success and code:
                                    st.session_state.current_solution = {
                                        "query": user_input,
                                        "code": code,
                                        "explanation": response_text,
                                        "results": results,
                                        "metadata": {}
                                    }
                                    st.session_state.waiting_for_feedback = True

                            else:
                                # Free-form text response (from Orchestrator)
                                response_text = str(assistant_response) if assistant_response else ""
                                code = ""
                                results = ""

                                # Check if response is empty first
                                if not response_text or not response_text.strip() or response_text == "None":
                                    response_text = "I apologize, but I couldn't generate a proper response. Could you please rephrase your question?"
                                    st.session_state.waiting_for_feedback = False
                                else:
                                    # Only show feedback buttons if this looks like a final answer
                                    # Don't show if orchestrator is asking a question or confirming
                                    is_question = response_text.strip().endswith('?')
                                    is_short_response = len(response_text.strip()) < 100
                                    looks_like_final_answer = not is_question and not is_short_response

                                    if looks_like_final_answer:
                                        st.session_state.current_solution = {
                                            "query": user_input,
                                            "code": "",
                                            "explanation": response_text,
                                            "results": "",
                                            "metadata": {}
                                        }
                                        st.session_state.waiting_for_feedback = True
                                    # else: Don't set waiting_for_feedback - let user respond to question

                            # Add assistant message to history
                            msg_data = {
                                "role": "assistant",
                                "content": response_text
                            }
                            if code:
                                msg_data["code"] = code
                            if results:
                                msg_data["results"] = results

                            st.session_state.messages.append(msg_data)

                            # Display assistant response
                            with st.chat_message("assistant"):
                                st.write(response_text)

                                if code:
                                    with st.expander("ðŸ“ View Code"):
                                        st.code(code, language="python")

                                if results:
                                    with st.expander("ðŸ“Š Execution Results"):
                                        st.text(results)

                            st.rerun()

                        except Exception as e:
                            error_msg = f"I apologize, but I encountered an error: {str(e)}"
                            st.error(f"âŒ Error: {str(e)}")

                            # Add and display error message
                            st.session_state.messages.append({
                                "role": "assistant",
                                "content": error_msg
                            })

                            with st.chat_message("assistant"):
                                st.write(error_msg)

                            # Don't show feedback buttons after error
                            st.session_state.waiting_for_feedback = False
                            st.rerun()
                else:
                    st.warning("âš ï¸ Please describe what you'd like to improve")

        # Continue conversation
        if st.session_state.continue_mode:
            st.divider()
            st.subheader("âž• Continue Conversation")
            st.info("ðŸ’¡ Add more details, ask follow-up questions, or provide additional context to continue the conversation.")

            continue_text = st.text_area(
                "What would you like to add or ask?",
                placeholder="Add more details, ask follow-up questions, provide constraints, or clarify your needs...",
                key="continue_text"
            )

            if st.button("Submit", key="submit_continue"):
                if continue_text:
                    # Append to original query
                    st.session_state.conversation_session.append_to_original_query(continue_text)

                    # Format message with ORIGINAL_QUERY tag
                    original_q = st.session_state.conversation_session.get_original_query()
                    if original_q:
                        user_input = f"[ORIGINAL_QUERY: {original_q}]\nâž• Continue: {continue_text}"
                    else:
                        user_input = f"âž• Continue: {continue_text}"

                    # Add user message to history (without the ORIGINAL_QUERY tag for display)
                    display_message = f"âž• Continue: {continue_text}"
                    st.session_state.messages.append({
                        "role": "user",
                        "content": display_message
                    })

                    # Display user message
                    with st.chat_message("user"):
                        st.write(display_message)

                    st.session_state.continue_mode = False

                    # Run orchestrator to process with updated query
                    with st.spinner("ðŸ¤” Processing with additional context..."):
                        try:
                            # Initialize orchestrator if needed
                            orchestrator = asyncio.run(initialize_orchestrator())

                            # Run conversation turn
                            result = asyncio.run(
                                st.session_state.conversation_session.run_conversation_turn(
                                    orchestrator,
                                    user_input
                                )
                            )

                            # Process result
                            assistant_response = result.final_output

                            # Handle None or empty response first
                            if assistant_response is None:
                                response_text = "I apologize, but I couldn't generate a proper response. Could you please rephrase your question?"
                                code = ""
                                results = ""
                                st.session_state.waiting_for_feedback = False

                            # Extract components if structured
                            elif isinstance(assistant_response, dict):
                                response_text = assistant_response.get("explanation", "")
                                code = assistant_response.get("final_code", "")
                                results = assistant_response.get("execution_results", "")
                                success = assistant_response.get("success", False)

                                # Check if response is empty
                                if not response_text or not response_text.strip():
                                    response_text = "I apologize, but I couldn't generate a proper response. Could you please rephrase your question?"
                                    st.session_state.waiting_for_feedback = False
                                # Store current solution if successful
                                elif success and code:
                                    st.session_state.current_solution = {
                                        "query": continue_text,
                                        "code": code,
                                        "explanation": response_text,
                                        "results": results,
                                        "metadata": {}
                                    }
                                    st.session_state.waiting_for_feedback = True

                            else:
                                # Free-form text response (from Orchestrator)
                                response_text = str(assistant_response) if assistant_response else ""
                                code = ""
                                results = ""

                                # Check if response is empty first
                                if not response_text or not response_text.strip() or response_text == "None":
                                    response_text = "I apologize, but I couldn't generate a proper response. Could you please rephrase your question?"
                                    st.session_state.waiting_for_feedback = False
                                else:
                                    # Only show feedback buttons if this looks like a final answer
                                    # Don't show if orchestrator is asking a question or confirming
                                    is_question = response_text.strip().endswith('?')
                                    is_short_response = len(response_text.strip()) < 100
                                    looks_like_final_answer = not is_question and not is_short_response

                                    if looks_like_final_answer:
                                        st.session_state.current_solution = {
                                            "query": continue_text,
                                            "code": "",
                                            "explanation": response_text,
                                            "results": "",
                                            "metadata": {}
                                        }
                                        st.session_state.waiting_for_feedback = True
                                    # else: Don't set waiting_for_feedback - let user respond to question

                            # Add assistant message to history
                            msg_data = {
                                "role": "assistant",
                                "content": response_text
                            }
                            if code:
                                msg_data["code"] = code
                            if results:
                                msg_data["results"] = results

                            st.session_state.messages.append(msg_data)

                            # Display assistant message
                            with st.chat_message("assistant"):
                                st.write(response_text)

                                if code:
                                    with st.expander("ðŸ“ View Code"):
                                        st.code(code, language="python")

                                if results:
                                    with st.expander("ðŸ“Š Execution Results"):
                                        st.text(results)

                            st.rerun()

                        except Exception as e:
                            error_msg = f"I apologize, but I encountered an error: {str(e)}"
                            st.error(f"âŒ Error: {str(e)}")

                            # Add and display error message
                            st.session_state.messages.append({
                                "role": "assistant",
                                "content": error_msg
                            })

                            with st.chat_message("assistant"):
                                st.write(error_msg)

                            # Don't show feedback buttons after error
                            st.session_state.waiting_for_feedback = False
                            st.rerun()
                else:
                    st.warning("âš ï¸ Please enter your message")

        # User input (disabled when waiting for feedback to encourage using the 4 buttons)
        user_input = st.chat_input(
            "Type your question here..." if not st.session_state.waiting_for_feedback
            else "Please use the buttons above to proceed...",
            disabled=st.session_state.waiting_for_feedback
        )

        if user_input:
            # Set original query if this is the first message in current problem-solving session
            if not st.session_state.waiting_for_feedback:
                st.session_state.conversation_session.set_original_query(user_input)

            # Add user message to history
            st.session_state.messages.append({
                "role": "user",
                "content": user_input
            })

            # Display user message
            with st.chat_message("user"):
                st.write(user_input)

            # Run orchestrator (async)
            with st.spinner("ðŸ¤” Thinking..."):
                try:
                    # Initialize orchestrator if needed
                    orchestrator = asyncio.run(initialize_orchestrator())

                    # Run conversation turn
                    result = asyncio.run(
                        st.session_state.conversation_session.run_conversation_turn(
                            orchestrator,
                            user_input
                        )
                    )

                    # Process result
                    assistant_response = result.final_output

                    # Handle None or empty response first
                    if assistant_response is None:
                        response_text = "I apologize, but I couldn't generate a proper response. Could you please rephrase your question?"
                        code = ""
                        results = ""
                        st.session_state.waiting_for_feedback = False

                    # Extract components if structured
                    elif isinstance(assistant_response, dict):
                        response_text = assistant_response.get("explanation", "")
                        code = assistant_response.get("final_code", "")
                        results = assistant_response.get("execution_results", "")
                        success = assistant_response.get("success", False)

                        # Check if response is empty
                        if not response_text or not response_text.strip():
                            response_text = "I apologize, but I couldn't generate a proper response. Could you please rephrase your question?"
                            st.session_state.waiting_for_feedback = False
                        # Store current solution if successful
                        elif success and code:
                            st.session_state.current_solution = {
                                "query": user_input,
                                "code": code,
                                "explanation": response_text,
                                "results": results,
                                "metadata": {}
                            }
                            st.session_state.waiting_for_feedback = True

                    else:
                        # Free-form text response (from Orchestrator)
                        response_text = str(assistant_response) if assistant_response else ""
                        code = ""
                        results = ""

                        # Check if response is empty first
                        if not response_text or not response_text.strip() or response_text == "None":
                            response_text = "I apologize, but I couldn't generate a proper response. Could you please rephrase your question?"
                            st.session_state.waiting_for_feedback = False
                        else:
                            # Only show feedback buttons if this looks like a final answer
                            # Don't show if orchestrator is asking a question or confirming
                            is_question = response_text.strip().endswith('?')
                            is_short_response = len(response_text.strip()) < 100
                            looks_like_final_answer = not is_question and not is_short_response

                            if looks_like_final_answer:
                                st.session_state.current_solution = {
                                    "query": user_input,
                                    "code": "",
                                    "explanation": response_text,
                                    "results": "",
                                    "metadata": {}
                                }
                                st.session_state.waiting_for_feedback = True
                            # else: Don't set waiting_for_feedback - let user respond to question

                    # Add assistant message to history
                    msg_data = {
                        "role": "assistant",
                        "content": response_text
                    }
                    if code:
                        msg_data["code"] = code
                    if results:
                        msg_data["results"] = results

                    st.session_state.messages.append(msg_data)

                    # Display assistant response
                    with st.chat_message("assistant"):
                        st.write(response_text)

                        if code:
                            with st.expander("ðŸ“ View Code"):
                                st.code(code, language="python")

                        if results:
                            with st.expander("ðŸ“Š Execution Results"):
                                st.text(results)

                    st.rerun()

                except Exception as e:
                    error_msg = f"I apologize, but I encountered an error: {str(e)}"
                    st.error(f"âŒ Error: {str(e)}")

                    # Add and display error message
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": error_msg
                    })

                    with st.chat_message("assistant"):
                        st.write(error_msg)

                    # Don't show feedback buttons after error
                    st.session_state.waiting_for_feedback = False
                    st.rerun()

    else:
        # Welcome screen for unauthenticated users
        st.title("ðŸ§ª Welcome to CASCADE")
        st.write("Please login or sign up to start chatting with the AI assistant for materials science and chemistry.")

        st.divider()

        st.subheader("âœ¨ Features")

        col1, col2, col3 = st.columns(3)

        with col1:
            st.markdown("### ðŸ§  Memory-Powered")
            st.write("Remembers your preferences and past successful solutions")

        with col2:
            st.markdown("### ðŸ”¬ Deep Research")
            st.write("Comprehensive research and debugging for complex problems")

        with col3:
            st.markdown("### ðŸ’¬ Multi-Turn")
            st.write("Natural conversation with feedback and improvements")


if __name__ == "__main__":
    main()

```


================================================================================
=== FILE: conversational_system\frontend\__init__.py ===
================================================================================

```python
"""
Frontend components for the conversational system.

Includes:
- Streamlit web interface
- Session management for multi-turn conversations
- User authentication
"""

```


================================================================================
=== FILE: docker\Dockerfile ===
================================================================================

```
# Dockerfile for CASCADE Conversational System
# This provides an isolated environment for running the materials science assistant

FROM python:3.12-slim

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    git \
    curl \
    build-essential \
    ca-certificates \
    gnupg \
    procps \
    # For Selenium/Playwright browser
    chromium \
    chromium-driver \
    libxrender1 \
    libxext6 \
    libsm6 \
    libice6 \
    libnss3 \
    libnspr4 \
    libatk1.0-0 \
    libatk-bridge2.0-0 \
    libcups2 \
    libdrm2 \
    libxkbcommon0 \
    libxcomposite1 \
    libxdamage1 \
    libxfixes3 \
    libxrandr2 \
    libgbm1 \
    libasound2 \
    libpango-1.0-0 \
    libcairo2 \
    # Install Node.js for workspace server
    && mkdir -p /etc/apt/keyrings \
    && curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg \
    && echo "deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_20.x nodistro main" | tee /etc/apt/sources.list.d/nodesource.list \
    && apt-get update \
    && apt-get install -y nodejs \
    && rm -rf /var/lib/apt/lists/*

# Install UV package manager (faster than pip)
RUN pip install uv

# Copy requirements first for better caching
COPY docker/requirements-docker.txt /tmp/requirements.txt

# Create virtual environment at /app/.venv (tools expect this path)
RUN uv venv /app/.venv \
    && . /app/.venv/bin/activate \
    && uv pip install -r /tmp/requirements.txt

# Copy the application code
COPY . /app

# Build workspace server (Node.js MCP server)
WORKDIR /app/mcp_servers_and_tools/workspace_server
RUN npm install && npm run build

# Back to app directory
WORKDIR /app

# Create directories for temp files, saved code, and data persistence
RUN mkdir -p /app/temp_code /app/saved_code /app/data /app/.home

# Copy startup script and set permissions (before switching to non-root user)
COPY docker/entrypoint.sh /usr/local/bin/entrypoint.sh
RUN chmod 755 /usr/local/bin/entrypoint.sh

# Create non-root user for security
RUN useradd -m -u 1000 cascade_user \
    && chown -R cascade_user:cascade_user /app \
    && chmod -R a+rX /app

# Make specific directories world-writable for any UID (AFTER chown/chmod)
# These directories need write access regardless of which user runs the container
RUN chmod 777 /app/temp_code /app/saved_code /app/data /app/.home

# Switch to non-root user (can be overridden by docker-compose user:)
USER cascade_user

# Install Playwright browsers as cascade_user
RUN /app/.venv/bin/playwright install chromium

# Switch back to root to fix permissions, then back to cascade_user
USER root
# Make cascade_user's home and playwright cache accessible to any UID
# (needed when container runs with user: "${UID}:${GID}" override)
RUN chmod a+rx /home/cascade_user && chmod -R a+rX /home/cascade_user/.cache
USER cascade_user

# Environment variables (can be overridden)
ENV PROJECT_ROOT=/app
ENV CODE_STORAGE_DIR=/app/temp_code
ENV SAVED_FILES_DIR=/app/saved_code
ENV PYTHONUNBUFFERED=1
# Ensure virtual environment is used
ENV VIRTUAL_ENV=/app/.venv
ENV PATH="/app/.venv/bin:$PATH"

# Expose Streamlit port
EXPOSE 8501

# Default command
ENTRYPOINT ["/usr/local/bin/entrypoint.sh"]
CMD ["streamlit", "run", "conversational_system/frontend/streamlit_app.py", "--server.address=0.0.0.0", "--server.port=8501"]

```


================================================================================
=== FILE: docker\entrypoint.sh ===
================================================================================

```bash
#!/bin/bash
# Entrypoint script for CASCADE Docker container
# Cleans up temp files and starts the application

set -e

# Activate virtual environment
source /app/.venv/bin/activate

echo "ðŸš€ Starting CASCADE Conversational System..."

# Clean up temp_code directory to avoid accumulation
if [ -d "/app/temp_code" ]; then
    echo "ðŸ§¹ Cleaning up temp_code directory..."
    rm -rf /app/temp_code/*
fi

# Check required environment variables
check_env_var() {
    if [ -z "${!1}" ]; then
        echo "âš ï¸  Warning: $1 not set"
    else
        echo "âœ… $1 is configured"
    fi
}

echo ""
echo "ðŸ“‹ Checking environment variables..."
check_env_var "OPENAI_API_KEY"
check_env_var "SUPABASE_URL"
check_env_var "SUPABASE_SERVICE_KEY"
check_env_var "NEO4J_URI"
check_env_var "NEO4J_USER"
check_env_var "NEO4J_PASSWORD"
check_env_var "TAVILY_API_KEY"
check_env_var "MP_API_KEY"
echo ""

# Set default Neo4j URI if not provided (for docker-compose network)
if [ -z "$NEO4J_URI" ]; then
    export NEO4J_URI="bolt://neo4j:7687"
    echo "ðŸ“Œ Using default NEO4J_URI: $NEO4J_URI"
fi

echo "ðŸŒ Starting application..."
echo "   Access the UI at: http://localhost:8501"
echo ""

# Execute the main command
exec "$@"

```


================================================================================
=== FILE: docker\requirements-docker.txt ===
================================================================================

```
# CASCADE Docker Requirements
# Streamlined for conversational system
# Python 3.12+ required

# ============== CORE DEPENDENCIES ==============

# --- AI/ML Core Frameworks ---
openai>=1.50.0
openai-agents==0.2.8
tiktoken>=0.7.0

# --- Materials Science/Chemistry Core ---
pymatgen>=2024.0.0
ase>=3.22.0
mp-api>=0.40.0
mpcontribs-client>=5.0.0
rdkit>=2024.0.0
smact>=2.5.0
emmet-core>=0.80.0
spglib>=2.0.0
sympy>=1.12.0

# --- Database Clients ---
sqlalchemy>=2.0.0
neo4j>=5.0.0
pymongo>=4.0.0
supabase>=2.0.0

# --- Multi-turn Chat & Memory ---
streamlit>=1.30.0
mem0ai[graph]>=1.0.0
vecs>=0.4.0

# --- HTTP & Async ---
httpx>=0.27.0
httpx-sse>=0.4.0
aiohttp>=3.9.0
aiofiles>=24.0.0
requests>=2.31.0
anyio>=4.0.0

# --- MCP Protocol ---
mcp>=1.0.0

# --- Search & Web Scraping ---
tavily-python>=0.5.0
crawl4ai>=0.5.0
beautifulsoup4>=4.12.0
playwright>=1.40.0
lxml>=5.0.0
selenium>=4.15.0

# --- Observability ---
mlflow>=3.3.0

# --- Configuration & Utilities ---
python-dotenv>=1.0.0
pyyaml>=6.0.0
click>=8.1.0
rich>=13.0.0
tqdm>=4.66.0

# --- Scientific Computing ---
numpy>=1.26.0,<2.0.0
scipy>=1.11.0
pandas>=2.0.0
matplotlib>=3.8.0

# --- Data Serialization ---
jsonschema>=4.20.0
orjson>=3.9.0

# --- Security ---
cryptography>=42.0.0
bcrypt>=4.1.0
pyjwt>=2.8.0

# --- Data Validation ---
pydantic>=2.0.0
pydantic-settings>=2.0.0

# --- Additional Core Dependencies ---
monty>=2024.0.0
packaging>=24.0
pillow>=10.0.0
jinja2>=3.1.0
typing-extensions>=4.9.0
attrs>=23.0.0
filelock>=3.13.0
fsspec>=2024.9.0,<2025.0.0
cachetools>=5.3.0

```


================================================================================
=== FILE: mcp_servers_and_tools\__init__.py ===
================================================================================

```python
#!/usr/bin/env python3
"""
MCP Servers and Tools Package
Contains MCP servers (memory, research, workspace) and direct tool implementations.
"""

# Direct tools are available via:
# from mcp_servers_and_tools.direct_tools import tavily_search, execute_code, ...

```


================================================================================
=== FILE: mcp_servers_and_tools\direct_tools\custom_memory_prompts.py ===
================================================================================

```python
"""
Custom Memory Extraction Prompts for Materials Science Assistant

Overrides mem0's default "Personal Information Organizer" prompt to also
store technical solutions, code examples, and scientific knowledge.
"""

from datetime import datetime

MATERIALS_SCIENCE_EXTRACTION_PROMPT = f"""You are a Materials Science and Chemistry Research Assistant specialized in accurately storing user preferences, technical solutions, API usage patterns, code implementation details, and scientific knowledge. Your role is to extract relevant information from conversations and organize them into distinct, manageable facts for easy retrieval when solving similar problems in the future.

EXTRACTION PURPOSE: The extracted facts will be used to help solve similar problems in the future. Therefore, focus on actionable implementation details that enable problem-solving, not just descriptions of what was asked.

IMPORTANT:
- For user preferences and configuration: Extract preferences about tools, databases, workflows
- For technical solutions: Extract HOW to implement solutions (methods, functions, parameters, patterns), NOT descriptions of what the user asked
- Every fact should help answer "How do I solve a similar problem?" not just "What did the user ask?"

Types of Information to Remember:

1. Personal Preferences and Configuration:
   - Preferred databases (Materials Project, AFLOW, OQMD, etc.)
   - API key locations and access methods (e.g., os.getenv('MP_API_KEY'))
   - Favorite tools and libraries for a specific task (pymatgen, ASE, VASP, etc.)
   - Research focus areas and materials of interest

2. Technical Implementation Details:
   - Specific API methods and functions 
   - Function calls and their parameters
   - Data retrieval patterns and workflows
   - Field names and object attributes accessed
   - Computational methods and their parameters
   - Code patterns that successfully solved problems

3. Scientific Knowledge:
   - Material properties and characteristics
   - Crystal structures and space groups
   - Computational methods and techniques
   - Analysis procedures and best practices

4. API and Library Usage:
   - Specific methods and their purposes
   - Required parameters and field names
   - Object attributes and how to access them
   - Common usage patterns
   - Library versions and compatibility notes

5. Project Context:
   - Research goals and objectives
   - Current projects and their requirements
   - Collaboration details and data sources
   - Important dates and milestones

6. Other Information:
   - Common errors and troubleshooting solutions
   - Performance tips and optimizations
   - Any other technical details that help solve similar problems

Few-Shot Examples:

Input: Hi.
Output: {{"facts": []}}

Input: I prefer using the Materials Project API for crystal structure data.
Output: {{"facts": ["Prefers Materials Project API for crystal structure data"]}}

Input: My MP API key is stored in the MP_API_KEY environment variable.
Output: {{"facts": ["MP API key is accessed via os.getenv('MP_API_KEY')"]}}

Input: User asked: How to get formation energy? Solution: Use MPRester and call mpr.materials.summary.search() with fields=['formation_energy_per_atom'], then access via docs[0].formation_energy_per_atom.
Output: {{"facts": ["Use mpr.materials.summary.search() to retrieve formation energy", "Pass fields=['formation_energy_per_atom'] to materials.summary.search()", "Access formation energy via docs[0].formation_energy_per_atom"]}}

Input: I'm working with Silicon for my semiconductor research and prefer pymatgen for structure analysis.
Output: {{"facts": ["Works with Silicon for research", "Research focus is semiconductors", "Prefers pymatgen for structure analysis"]}}

Return the facts in JSON format as shown above.

Guidelines:
- Today's date is {datetime.now().strftime("%Y-%m-%d")}.
- Purpose: Extract information that helps solve similar problems in the future.
- For user preferences: Extract preferences about tools, databases, materials, workflows.
- For technical solutions: Extract implementation details - specific methods, functions, parameters, code patterns.
- When code/solutions are provided, extract HOW it works: API methods, function calls, parameters, field access patterns.
- Focus on actionable technical information that enables solving similar problems.
- If no relevant information is found, return an empty list for the "facts" key.
- Detect the language of user input and record facts in the same language. Normally, the user input is in English.
- The response must be valid JSON with a "facts" key containing a list of strings.

Following is a conversation between the user and the assistant. Extract relevant facts about user preferences, API keys, technical implementation details, API methods, code patterns, and scientific knowledge from the conversation and return them in JSON format as shown above.
"""

# Graph Memory Custom Prompt (for entity and relationship extraction)
MATERIALS_SCIENCE_GRAPH_PROMPT = """Focus on extracting technical implementation details and relationships in materials science code and research context. The purpose is to help solve similar problems in the future by capturing HOW solutions are implemented.

IMPORTANT:
- Extract specific API methods, function calls, and technical components from code
- Focus on implementation details that show how to solve problems, not just what was asked
- Capture relationships between methods, parameters, and data fields

Example Entity Types (adapt based on actual content):
   - Materials: Chemical elements, compounds, alloys, material IDs
   - Properties: Material properties
   - Tools/Libraries: Software and libraries
   - API Methods: Specific API methods and functions
   - Functions: Function calls and methods 
   - Parameters: Function parameters and field names
   - Data Fields: Object attributes and accessed fields
   - Databases: Data sources (Materials Project, AFLOW, OQMD, etc.)
   - API Keys: Configuration methods
   - Units: Measurement units

Relationship Guidelines:
   - Extract relationships that help solve similar problems: both user preferences AND technical implementation
   - For user preferences: Capture tool preferences, database choices, workflow patterns (e.g., "user â†’ prefers â†’ Materials_Project")
   - For technical implementation: Show HOW code works through method calls, parameter passing, field access
   - Prefer specific technical relationships (e.g., "calls_method" over generic "uses")
   - Include relationships between methods and the data they retrieve
   - Include relationships between parameters and functions that accept them
   - Focus on actionable relationships that enable future problem-solving
"""

# Export
__all__ = [
    'MATERIALS_SCIENCE_EXTRACTION_PROMPT',
    'MATERIALS_SCIENCE_GRAPH_PROMPT'
]

```


================================================================================
=== FILE: mcp_servers_and_tools\direct_tools\memory_tools.py ===
================================================================================

```python
"""
Memory Tools for Conversational System
All agents (Orchestrator and internal agents in DeepSolver) can use these tools

Graph Memory:
- Enabled by default (extracts entities and relationships from conversations)
- Set ENABLE_GRAPH_MEMORY=false to disable and use only vector-based memory
- Requires Neo4j database with NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD for graph memory
"""

import os
from mem0 import Memory
from agents import function_tool
from .custom_memory_prompts import (
    MATERIALS_SCIENCE_EXTRACTION_PROMPT,
    MATERIALS_SCIENCE_GRAPH_PROMPT
)


# ============================================================================
# MEMORY CONFIGURATION
# ============================================================================
# MEM0_USE_LLM: Whether to use LLM for intelligent memory extraction (default: true)
#   - true: LLM extracts key facts + saves original content (two saves)
#   - false: Only saves original content directly (one save, no LLM needed)
#
# MEM0_LLM_MODEL: Which model to use for memory extraction (default: gpt-4o-mini)
#
# ENABLE_GRAPH_MEMORY: Whether to use Neo4j graph for entity relationships (default: true)
# ============================================================================

MEM0_USE_LLM = os.getenv('MEM0_USE_LLM', 'true').lower() == 'true'
MEM0_LLM_MODEL = os.getenv('MEM0_LLM_MODEL', 'gpt-4o-mini')
ENABLE_GRAPH_MEMORY = os.getenv('ENABLE_GRAPH_MEMORY', 'true').lower() == 'true'

# Base configuration dictionary for from_config method
config_dict = {
    "vector_store": {
        "provider": "supabase",
        "config": {
            "connection_string": os.getenv('SUPABASE_DATABASE_URL'),
            "collection_name": "conversational_memories"
        }
    },
}

# Add LLM config only if enabled
if MEM0_USE_LLM:
    config_dict["llm"] = {
        "provider": "openai",
        "config": {
            "model": MEM0_LLM_MODEL
        }
    }
    config_dict["custom_fact_extraction_prompt"] = MATERIALS_SCIENCE_EXTRACTION_PROMPT
    print(f"âœ… Memory LLM ENABLED - Using {MEM0_LLM_MODEL} for intelligent extraction")
else:
    print("â„¹ï¸  Memory LLM DISABLED - Direct storage only (no extraction)")

# Add graph store if enabled
if ENABLE_GRAPH_MEMORY:
    config_dict["graph_store"] = {
        "provider": "neo4j",
        "config": {
            "url": os.getenv('NEO4J_URI', 'bolt://localhost:7687'),
            "username": os.getenv('NEO4J_USER', 'neo4j'),
            "password": os.getenv('NEO4J_PASSWORD', 'password')
        },
        "custom_prompt": MATERIALS_SCIENCE_GRAPH_PROMPT
    }
    print("âœ… Graph memory ENABLED - Entity relationships will be tracked")
else:
    print("â„¹ï¸  Graph memory DISABLED - Using vector-based memory only")

print("âœ… Custom materials science prompts ENABLED (vector + graph)")

# Create global memory instance with custom prompt
mem0 = Memory.from_config(config_dict)


@function_tool
async def search_memory(query: str, user_id: str) -> str:
    """
    Search through stored memories

    Returns relevant memories that may help solve the current problem
    Use this to retrieve user preferences, API keys, similar problems, solutions, or relevant experience from past memories

    Args:
        query: What to search for
        user_id: User identifier extracted from message

    Returns:
        Formatted string with relevant memories (from vector store) and entity relationships (from graph store)
    """
    try:
        search_result = mem0.search(query, user_id=user_id, limit=5)

        output_parts = []

        # Add vector store results
        if search_result and search_result.get('results'):
            vector_results = search_result['results']
            output_parts.append("Relevant memories:")
            for mem in vector_results:
                output_parts.append(f"- {mem['memory']}")

        # Add graph store results (entity relationships)
        if ENABLE_GRAPH_MEMORY and search_result and search_result.get('relations'):
            relations = search_result['relations'][:5]  # Limit to top 5 relations
            if relations:
                output_parts.append("\nRelated entity relationships:")
                for rel in relations:
                    source = rel.get('source', 'unknown')
                    relationship = rel.get('relationship', 'relates_to')
                    destination = rel.get('destination', 'unknown')
                    output_parts.append(f"- {source} â†’ {relationship} â†’ {destination}")

        if output_parts:
            return "\n".join(output_parts)

        return "No relevant memories found."
    except Exception as e:
        print(f"Error searching memory: {e}")
        return "Error searching memories."


@function_tool
async def save_to_memory(content: str, user_id: str) -> str:
    """
    Save important information to memory

    IMPORTANT:
    - For solutions: Only call this AFTER the user confirms they are satisfied
    - For preferences/configuration: Can be called anytime
    - Do NOT save failed solutions or solutions the user is not happy with

    Behavior depends on MEM0_USE_LLM environment variable:
    - If MEM0_USE_LLM=true: Saves both LLM-extracted facts AND complete original content
    - If MEM0_USE_LLM=false: Only saves complete original content (no LLM processing)

    This builds a knowledge base for future reference

    Args:
        content: Content to save (e.g., user's query, solution code, explanation,
                 user preferences, API keys, how to access API keys, etc.)
        user_id: User identifier extracted from message

    Returns:
        Status message
    """
    try:
        messages = [{"role": "user", "content": content}]

        if MEM0_USE_LLM:
            # Save 1: LLM extracts key facts (vector + graph memory with custom prompts)
            mem0.add(messages, user_id=user_id, infer=True)

            # Save 2: Store complete original content as-is (vector only, no LLM processing)
            # Temporarily disable graph to avoid duplicate graph updates for same content
            original_enable_graph = mem0.enable_graph
            mem0.enable_graph = False
            mem0.add(messages, user_id=user_id, infer=False)
            mem0.enable_graph = original_enable_graph

            return "Information saved to memory successfully (facts + complete content)."
        else:
            # No LLM: Only save complete original content directly
            mem0.add(messages, user_id=user_id, infer=False)
            return "Information saved to memory successfully (direct storage)."
    except Exception as e:
        print(f"Error saving to memory: {e}")
        return f"Error saving to memory: {str(e)}"


# Export public interfaces
__all__ = [
    'mem0',
    'search_memory',
    'save_to_memory',
    'ENABLE_GRAPH_MEMORY'  # For checking if graph memory is enabled
]

```


================================================================================
=== FILE: mcp_servers_and_tools\direct_tools\research_tools.py ===
================================================================================

```python
"""
Direct Function Tools for Research Operations

This module provides direct implementations of research tools, bypassing MCP server overhead
"""

import asyncio
import atexit
import json
import os
import sys
import time
import traceback
import warnings
from pathlib import Path
from typing import Any, Dict, List, Optional
from urllib.parse import urlparse

# Suppress harmless "Event loop is closed" warnings from asyncio subprocess cleanup
# These occur when browser processes are garbage collected after the event loop closes
warnings.filterwarnings("ignore", message="Event loop is closed", category=RuntimeWarning)

# Patch asyncio to suppress "Event loop is closed" errors during subprocess cleanup
# This is a known Python issue where subprocess transports are GC'd after event loop closes
_original_del = None
try:
    import asyncio.base_subprocess
    _original_del = asyncio.base_subprocess.BaseSubprocessTransport.__del__

    def _patched_del(self):
        try:
            _original_del(self)
        except RuntimeError as e:
            if "Event loop is closed" not in str(e):
                raise
    asyncio.base_subprocess.BaseSubprocessTransport.__del__ = _patched_del
except Exception:
    pass  # If patching fails, continue without it

from agents import function_tool
from crawl4ai import AsyncWebCrawler, BrowserConfig, CacheMode, CrawlerRunConfig
from dotenv import load_dotenv
from supabase import Client
import re
# Compute a human-readable site-packages hint for docstrings
try:
    import sysconfig, site as _site
    _purelib = sysconfig.get_paths().get("purelib") or ""
    _sitepkgs = []
    try:
        _sitepkgs = _site.getsitepackages() or []
    except Exception:
        _sitepkgs = []
    SITE_PACKAGES_HINT = _purelib or (_sitepkgs[0] if _sitepkgs else "<site-packages>")
except Exception:
    SITE_PACKAGES_HINT = "<site-packages>"

# Add paths for importing local modules
project_root = Path(__file__).resolve().parent.parent.parent
mcp_servers = project_root / 'mcp_servers_and_tools'
knowledge_graphs_path = mcp_servers / 'research_server' / 'knowledge_graphs'
probe_path = mcp_servers / 'research_server' / 'introspection_and_probe'
research_server_path = mcp_servers / 'research_server' / 'src'
sys.path.append(str(knowledge_graphs_path))
sys.path.append(str(probe_path))
sys.path.append(str(research_server_path))

# Load environment variables
# Priority: .env file > shell environment (.bashrc) > code defaults
project_root = Path(__file__).resolve().parent.parent.parent
dotenv_path = project_root / '.env'
load_dotenv(dotenv_path, override=True)

# Import utilities from research server
import importlib.util
_research_utils_spec = importlib.util.spec_from_file_location("research_server_utils", research_server_path / "research_server_utils.py")
_research_utils = importlib.util.module_from_spec(_research_utils_spec)
_research_utils_spec.loader.exec_module(_research_utils)

get_supabase_client = _research_utils.get_supabase_client
extract_code_blocks = _research_utils.extract_code_blocks
generate_code_example_summary = _research_utils.generate_code_example_summary
check_extracted_code_exists = _research_utils.check_extracted_code_exists
save_extracted_code_to_supabase = _research_utils.save_extracted_code_to_supabase
get_extracted_code_from_supabase = _research_utils.get_extracted_code_from_supabase
search_code_blocks = _research_utils.search_code_blocks
detect_content_type_and_source = _research_utils.detect_content_type_and_source
extract_readthedocs_code_blocks = _research_utils.extract_readthedocs_code_blocks
extract_markdown_code_blocks = _research_utils.extract_markdown_code_blocks
extract_command_examples = _research_utils.extract_command_examples
extract_smart_context_before = _research_utils.extract_smart_context_before
extract_smart_context_after = _research_utils.extract_smart_context_after
extract_generic_html_code_blocks = _research_utils.extract_generic_html_code_blocks
extract_github_html_code_blocks = _research_utils.extract_github_html_code_blocks
extract_mkdocs_code_blocks = _research_utils.extract_mkdocs_code_blocks
extract_jupyter_notebook_cells = _research_utils.extract_jupyter_notebook_cells

# Import knowledge graph and introspection tools
from parse_repo_into_neo4j import DirectNeo4jExtractor
from quick_introspect_core import run_quick_introspect
from runtime_probe import (
    show_all_keys_or_attrs as rp_show_all,
    try_get_key as rp_try_get_key,
    try_get_attr as rp_try_get_attr
)

# Initialize shared resources
_crawler = None
_supabase_client = None
_repo_extractor = None

async def get_crawler():
    """Get or create the shared crawler instance with error handling and retry"""
    global _crawler
    if _crawler is None:
        max_retries = 3
        for attempt in range(max_retries):
            try:
                browser_config = BrowserConfig(
                    headless=True,
                    verbose=False
                )
                _crawler = AsyncWebCrawler(config=browser_config)
                await _crawler.start()
                break  # Success, exit retry loop
            except Exception as e:
                _crawler = None  # Reset on failure
                if attempt == max_retries - 1:
                    # Final attempt failed
                    raise Exception(f"Failed to start web crawler after {max_retries} attempts. This may be due to browser resource conflicts. Error: {str(e)}")
                else:
                    # Wait before retry
                    import asyncio
                    await asyncio.sleep(1)
    return _crawler

async def reset_crawler():
    """Force reset the crawler instance (useful for troubleshooting)"""
    global _crawler
    if _crawler is not None:
        try:
            await _crawler.close()
        except:
            pass  # Ignore errors during cleanup
        _crawler = None

async def cleanup_crawler():
    """Public function to cleanup the crawler before process exit.
    This should be called to properly close the browser and prevent hanging."""
    await reset_crawler()

def _cleanup_crawler_sync():
    """Synchronous cleanup for atexit - closes browser before event loop shuts down"""
    global _crawler
    if _crawler is not None:
        try:
            # Try to get or create an event loop for cleanup
            try:
                loop = asyncio.get_event_loop()
                if loop.is_closed():
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)

            loop.run_until_complete(_crawler.close())
        except Exception:
            pass  # Ignore all errors during cleanup
        finally:
            _crawler = None

# Register cleanup handler to close browser before Python exits
atexit.register(_cleanup_crawler_sync)

def get_supabase():
    """Get or create the shared Supabase client"""
    global _supabase_client
    if _supabase_client is None:
        _supabase_client = get_supabase_client()
    return _supabase_client

async def get_repo_extractor():
    """Get or create the shared repository extractor instance"""
    global _repo_extractor
    if _repo_extractor is None:
        # Check if knowledge graph functionality is enabled
        knowledge_graph_enabled = os.getenv("USE_KNOWLEDGE_GRAPH", "false") == "true"
        if knowledge_graph_enabled:
            neo4j_uri = os.getenv("NEO4J_URI")
            neo4j_user = os.getenv("NEO4J_USER")
            neo4j_password = os.getenv("NEO4J_PASSWORD")
            if neo4j_uri and neo4j_user and neo4j_password:
                try:
                    _repo_extractor = DirectNeo4jExtractor(neo4j_uri, neo4j_user, neo4j_password)
                    await _repo_extractor.initialize()
                    # print("âœ“ Neo4j knowledge graph extractor initialized")
                except Exception as e:
                    print(f"âœ— Failed to initialize Neo4j extractor: {e}")
                    _repo_extractor = None
            else:
                print("Neo4j credentials not configured - knowledge graph tools will be unavailable")
        else:
            print("Knowledge graph functionality disabled - set USE_KNOWLEDGE_GRAPH=true to enable")
    return _repo_extractor

# Import helper functions directly from research_mcp.py
from research_mcp import (
    crawl_batch,
    crawl_recursive_internal_links,
    crawl_markdown_file,
    _generate_code_summary,
    is_sitemap,
    is_txt,
    parse_sitemap,
    smart_chunk_markdown,
    github_blob_to_raw,
    validate_and_normalize_url,
    _extract_code_single_page,
    _extract_code_smart_crawl,
    _handle_repos_command,
    _handle_explore_command,
    _handle_classes_command,
    _handle_class_command,
    _handle_method_command,
    _handle_query_command
)

# ============================================================================
# MAIN TOOL FUNCTIONS
# ============================================================================

@function_tool
async def extract_code_from_url(
    url: str = None,
    urls: List[str] = None
) -> str:
    """
    Extract code examples and commands from one or more URLs for immediate use by agents.
    This tool uses a caching strategy: first checks if code has already been extracted from the URL(s),
    and if not, performs extraction and stores the results in Supabase for future use.
    
    Extraction strategy:
    1. Check cache first
    2. Try single page extraction with optimized strategy:
       - HTML extraction only for ReadTheDocs/Sphinx
       - Markdown extraction for other content types
       - Special handling for Jupyter notebooks (JSON extraction)
    3. If no code blocks found, fallback to smart crawl
    
    If a list of URLs is provided, all will be processed and results merged.
    Each code block in the result will include a 'source_url' key indicating its origin.
    
    Args:
        url: Single URL to extract code from 
        urls: List of URLs to extract code from
        
    Returns:
        JSON string with extracted code examples, commands, and content summary.
        Key fields to focus on:
        - 'code': The extracted code content
        - 'context_before': Text content before the code block
        - 'context_after': Text content after the code block
        - 'source_url': URL where the code was extracted from
        - 'summary': Code summary (often an empty string unless language model summary is enabled)
    """
    # Set timeout for the entire extraction process (100 seconds)
    EXTRACTION_TIMEOUT = 100
    
    try:
        start_time = time.time()
        
        # Wrap the entire function in a timeout
        async def extract_with_timeout():
            return await _extract_code_from_url_internal(url, urls)
        
        try:
            result = await asyncio.wait_for(extract_with_timeout(), timeout=EXTRACTION_TIMEOUT)
            elapsed_time = time.time() - start_time
            # print(f"âœ… Extraction completed in {elapsed_time:.2f} seconds")
            return result
        except asyncio.TimeoutError:
            elapsed_time = time.time() - start_time
            print(f"â° Extraction timed out after {elapsed_time:.2f} seconds")
            
            # Return a friendly error message instead of crashing
            timeout_message = {
                "success": False,
                "error": f"Extraction timed out after {EXTRACTION_TIMEOUT} seconds",
                "error_type": "timeout",
                "suggestion": "This webpage may not exist, or extraction is taking too long. Do NOT try to extract code from this URL again. Please try:",
                "alternatives": [
                    "Use a different related webpage for code extraction",
                    "Use tavily search to find information about this webpage if the URL exists"
                ],
                "url": url if url else (urls[0] if urls else "unknown"),
                "extracted_code": [],
                "code_blocks_found": 0
            }
            
            if urls:
                # For multiple URLs, return timeout info for all
                timeout_message["urls"] = urls
                timeout_message["per_url_results"] = [
                    {
                        "success": False,
                        "url": u,
                        "error": f"Extraction timed out after {EXTRACTION_TIMEOUT} seconds",
                        "error_type": "timeout",
                        "suggestion": "Try alternative methods or different URLs"
                    } for u in urls
                ]
                timeout_message["summary"] = {
                    "total_unique_urls": len(urls),
                    "successful_extractions": 0,
                    "timeout_urls": len(urls),
                    "total_code_blocks_found": 0
                }
            
            return json.dumps(timeout_message, indent=2)
            
    except Exception as e:
        # print(f"Error in extract_code_from_url: {traceback.format_exc()}")
        return json.dumps({
            "success": False,
            "error": str(e),
            "error_type": "exception"
        }, indent=2)

async def _extract_code_from_url_internal(url: str = None, urls: list = None) -> str:
    """
    Internal implementation of extract_code_from_url without timeout wrapper.
    This is the original function logic.
    """
    try:
        supabase_client = get_supabase()
        crawler = await get_crawler()
        
        async def process_one_url(url):
            """Process a single URL with caching and fallback strategy"""
            url, is_valid = validate_and_normalize_url(url)
            if not is_valid:
                return {
                    "success": False,
                    "url": url,
                    "error": "Invalid URL provided"
                }
            # 1. Check cache first
            if await check_extracted_code_exists(supabase_client, url):
                # print(f"ðŸ“‹ Found cached extracted code for {url}")
                cached_code_blocks = await get_extracted_code_from_supabase(supabase_client, url)
                if cached_code_blocks:
                    for block in cached_code_blocks:
                        block["source_url"] = url
                    return {
                        "success": True,
                        "url": url,
                        "code_blocks_found": len(cached_code_blocks),
                        "extracted_code": cached_code_blocks,
                        "extraction_method": "cached",
                        "cached": True
                    }
            # 2. Try single page extraction
            # print(f"ðŸ”„ Extracting code from {url}...")
            single_page_result = await _extract_code_single_page(crawler, url)
            try:
                result_data = json.loads(single_page_result)
                # If single page crawl succeeded
                if result_data.get("success", False):
                    extracted_code = result_data.get("extracted_code", [])
                    # Add source_url to each code block
                    for block in extracted_code:
                        block["source_url"] = url
                    # Check if we found any code blocks
                    if result_data.get("code_blocks_found", 0) > 0:
                        # Found code blocks - save and return
                        await save_extracted_code_to_supabase(
                            supabase_client, 
                            url, 
                            extracted_code, 
                            result_data.get("code_blocks_found", 0),
                            "single_page"
                        )
                        result_data["cached"] = False
                        return result_data
                    else:
                        # No code blocks found - try smart crawl as fallback
                        # print(f"ðŸ“ No code blocks found with single page, trying smart crawl for {url}...")
                        pass
                else:
                    # Single page crawl failed - try smart crawl
                    # print(f"âŒ Single page crawl failed for {url}, trying smart crawl...")
                    pass
            except (json.JSONDecodeError, KeyError) as e:
                print(f"Error parsing single page result: {e}")
            # 3. Fallback to smart crawl (either no code found or crawl failed)
            smart_result = await _extract_code_smart_crawl(crawler, url)
            try:
                smart_data = json.loads(smart_result)
                extracted_code = smart_data.get("extracted_code", [])
                # Add source_url to each code block
                for block in extracted_code:
                    block["source_url"] = url
                # Save to cache
                await save_extracted_code_to_supabase(
                    supabase_client, 
                    url, 
                    extracted_code, 
                    smart_data.get("code_blocks_found", 0),
                    "smart_crawl"
                )
                smart_data["cached"] = False
                return smart_data
            except Exception as e:
                return {
                    "success": False, 
                    "url": url, 
                    "error": f"Both single page and smart crawl failed: {str(e)}"
                }
        # Handle multiple URLs (only if urls is provided and not empty)
        if urls is not None and len(urls) > 0:
            # Special case: if only one URL provided, treat it like single URL for consistent output
            if len(urls) == 1:
                result = await process_one_url(urls[0])
                return json.dumps(result, indent=2)
            
            # Remove duplicates while preserving order
            unique_urls = []
            seen_urls = set()
            duplicate_urls = []
            for u in urls:
                if u in seen_urls:
                    duplicate_urls.append(u)
                else:
                    unique_urls.append(u)
                    seen_urls.add(u)
            
            # After deduplication, check if we ended up with only one URL
            if len(unique_urls) == 1:
                result = await process_one_url(unique_urls[0])
                return json.dumps(result, indent=2)
            
            # Process multiple unique URLs
            all_results = []
            all_code_blocks = []
            cached_count = 0
            extracted_count = 0
            for u in unique_urls:
                result = await process_one_url(u)
                all_results.append(result)
                if result.get("success") and result.get("extracted_code"):
                    all_code_blocks.extend(result["extracted_code"])
                    if result.get("cached", False):
                        cached_count += 1
                    else:
                        extracted_count += 1
            # Create summary
            summary_info = {
                "cached_urls": cached_count,
                "newly_extracted_urls": extracted_count,
                "total_unique_urls": len(unique_urls),
                "total_original_urls": len(urls),
                "duplicate_urls_skipped": len(duplicate_urls)
            }
            if duplicate_urls:
                summary_info["duplicate_urls"] = duplicate_urls
            return json.dumps({
                "success": True,
                "urls": urls,
                "unique_urls": unique_urls,
                "total_code_blocks_found": len(all_code_blocks),
                "all_extracted_code": all_code_blocks,
                "per_url_results": all_results,
                "summary": summary_info
            }, indent=2)
        # Handle single URL
        elif url is not None:
            result = await process_one_url(url)
            return json.dumps(result, indent=2)
        # No URL provided
        else:
            return json.dumps({
                "success": False,
                "error": "No url or urls provided"
            }, indent=2)
    except Exception as e:
        # print(f"Error in _extract_code_from_url_internal: {traceback.format_exc()}")
        return json.dumps({
            "success": False,
            "error": str(e)
        }, indent=2)


@function_tool
async def retrieve_extracted_code(
    query: str,
    match_count: int = 5
) -> str:
    """
    Retrieve extracted code blocks relevant to the query.
    This tool searches the extracted_code table for code blocks relevant to the query and returns
    the matching examples with their summaries and context.
    You can call this tool multiple times with different queries to get more relevant results.
    Args:
        query: The search query
        match_count: Maximum number of results to return (default: 5)
    Returns:
        JSON string with the search results
    """
    try:
        supabase_client = get_supabase()
        # Search for code blocks
        results = await search_code_blocks(
            client=supabase_client,
            query=query,
            match_count=match_count
        )
        # Format the results
        formatted_results = []
        for result in results:
            formatted_result = {
                "source_url": result.get("source_url"),
                "code": result.get("code"),
                "summary": result.get("summary"),
                "context_before": result.get("context_before"),
                "context_after": result.get("context_after"),
                "type": result.get("type"),
                "language": result.get("language"),
                "index": result.get("index"),
                "similarity_score": result.get("similarity_score")
            }
            formatted_results.append(formatted_result)
        return json.dumps({
            "success": True,
            "query": query,
            "search_mode": "vector",
            "results": formatted_results,
            "count": len(formatted_results)
        }, indent=2)
    except Exception as e:
        return json.dumps({
            "success": False,
            "query": query,
            "error": str(e)
        }, indent=2)

@function_tool
async def quick_introspect(
    code_content: str = None,
    class_hint: str = None,
    method_hint: str = None,
    package_path: str = None,
    function_hint: str = None,
    module_hint: str = None,
    repo_hint: str = None,
    max_suggestions: int = 10,
    no_imports: bool = False,
) -> str:
    f"""
    Fast, static-first introspection for fixing import/class/method/function related errors.
    Uses Jedi for static discovery first (no side effects), then runtime import/inspect fallback.
    Returns a human/agent-readable report string with suggested import lines (if no-imports is not set), class methods (if method_hint/class_hint and repo_hint or package_path are provided), and functions (if function_hint/module_hint and repo_hint or package_path are provided).
    Parameter relationships:
    - repo_hint vs package_path: mutually exclusive; pass at most one.
    - module_hint must be used together with function_hint.
    - If class_hint or method_hint is provided, one of repo_hint or package_path is required.
    - If function_hint is provided, one of repo_hint or package_path is required.
    Notes:
    - You can provide fuzzy hints for class_hint, method_hint, function_hint, module_hint (but repo_hint need to be exact) if you cannot provide exact hints.
    - It is recommended to use code_content to provide the code content for import diagnostics.
    - repo_hint is the top-level import module name (may differ from pip distribution name). If repo_hint cannot be imported, provide package_path instead.
    - PATH HANDLING for package_path:
      * Absolute path: used as-is.
      * Relative path: resolved against the active environment's site-packages root {SITE_PACKAGES_HINT}.
        For example, passing "pydantic" will be tried as {SITE_PACKAGES_HINT}/pydantic.
      * If unsure about the absolute path, use your check_package_version tool to obtain it if you have this tool.
      * If repo_hint fails to import, try providing an absolute or relative package_path (relative path starts from {SITE_PACKAGES_HINT}).
    - Function vs Method:
      * Use method_hint when the target is a class member (instance/class method). Do not use function_hint for class/instance methods. **Most of the time, you should use method_hint**
      * Use function_hint only for top-level (module-level) functions; optionally add module_hint to narrow
      * Heuristics: analyze the call-site pattern â€” calls like SomeClass.method(...)/obj.method(...) â†’ method_hint; calls like package.module.function(...), module.function(...), function(...) â†’ function_hint
    - method_hint can be provided without class_hint to trigger a repo-wide search (noisy but useful, but oftenit is more recommended to add fuzzy or exact class_hint to narrow down the search).
    - To silence import diagnostics if you think it is too noisy for your use case, set no_imports=true and reuse this tool to introspect the code again.
    - max_suggestions is the maximum number of suggestions to return. If not provided, it will return all suggestions. You can set it to a smaller number to reduce the noise if needed.
    - Set env QI_DEBUG_ENGINE=1 to see whether Jedi or runtime fallback is used. But normally you don't need to set this.
    """
    try:
        # Resolve relative package_path against active environment's site-packages
        if package_path:
            try:
                import sysconfig, site
                candidate_paths = []
                purelib = sysconfig.get_paths().get("purelib")
                if purelib:
                    candidate_paths.append(purelib)
                # site.getsitepackages may return multiple entries (venv + global)
                try:
                    for p in site.getsitepackages():
                        if p not in candidate_paths:
                            candidate_paths.append(p)
                except Exception:
                    pass
                if not os.path.isabs(package_path):
                    for root in candidate_paths:
                        joined = os.path.join(root, package_path)
                        if os.path.exists(joined):
                            package_path = joined
                            break
            except Exception:
                pass
        report = run_quick_introspect(
            code_content=code_content,
            class_hint=class_hint,
            method_hint=method_hint,
            package_path=package_path,
            function_hint=function_hint,
            module_hint=module_hint,
            repo_hint=repo_hint,
            max_suggestions=max_suggestions,
            no_imports=no_imports,
        )
        import json as _json
        return _json.dumps({
            "success": True,
            "report": report,
        }, indent=2)
    except Exception as e:
        import json as _json
        # Return the core error message in 'report' to keep a single consumption path
        message = str(e)
        return _json.dumps({
            "success": False,
            "report": message,
        }, indent=2)

@function_tool
async def runtime_probe_snippet(
    snippet: str
) -> str:
    """
    Return a ready-to-paste Python probe snippet for targeted debugging of runtime errors.
    snippet: one of "try_get_key", "try_get_attr".
    - try_get_key: Use at a KeyError site. Replace mapping['k'] with try_get_key(mapping, 'k').
      If the key exists, it prints an OK message (either the earlier error was at a different site and you
      should probe the right line and re-run, or you are now probing a different key than the one that caused the earlier KeyError and have successfully
      debugged it). If missing, it prints available keys and error context to guide a fix.
    - try_get_attr: Use at an AttributeError site. Replace obj.attr with try_get_attr(obj, 'attr').
      If the attribute exists, it prints an OK message (either the earlier error was at a different site and
      you should probe the right line and re-run, or you are now probing a different attribute than the one that caused the earlier AttributeError and have
      successfully debugged it). If missing, it prints public attributes, similarity suggestions and error context to guide a fix.
    The returned report contains a short usage header followed by the snippet code. You MUST Paste the snippet into
    your current script right after your imports and follow the usage note to replace the failing access.
    """
    import json as _json
    SNIPPETS: dict[str, str] = {
        "try_get_key": (
            "import sys, os\n"
            "from pathlib import Path\n"
            "# Resolve project root heuristically and add probe folder to sys.path\n"
            "_here = Path(__file__).resolve() if '__file__' in globals() else Path.cwd()\n"
            "_root = _here\n"
            "for _ in range(5):\n"
            "    if (_root / 'mcp_servers_and_tools' / 'research_server' / 'introspection_and_probe').exists():\n"
            "        sys.path.insert(0, str(_root / 'mcp_servers_and_tools' / 'research_server' / 'introspection_and_probe'))\n"
            "        break\n"
            "    _root = _root.parent\n"
            "from runtime_probe import try_get_key\n\n"
            "# Usage at the KeyError site: replace mapping['k'] with try_get_key(mapping, 'k')\n"
        ),
        "try_get_attr": (
            "import sys, os\n"
            "from pathlib import Path\n"
            "_here = Path(__file__).resolve() if '__file__' in globals() else Path.cwd()\n"
            "_root = _here\n"
            "for _ in range(5):\n"
            "    if (_root / 'mcp_servers_and_tools' / 'research_server' / 'introspection_and_probe').exists():\n"
            "        sys.path.insert(0, str(_root / 'mcp_servers_and_tools' / 'research_server' / 'introspection_and_probe'))\n"
            "        break\n"
            "    _root = _root.parent\n"
            "from runtime_probe import try_get_attr\n\n"
            "# Usage at the AttributeError site: replace obj.attr with try_get_attr(obj, 'attr')\n"
        ),
    }
    key = (snippet or "").strip()
    if key not in SNIPPETS:
        return _json.dumps({
            "success": False,
            "report": "Invalid snippet. Use one of: try_get_key, try_get_attr",
        }, indent=2)
    headers = {
        "try_get_key": (
            "Usage at the KeyError site: replace mapping['k'] with try_get_key(mapping, 'k').\n"
            "**You MUST Paste this snippet right after your imports.**\n"
        ),
        "try_get_attr": (
            "Usage at the AttributeError site: replace obj.attr with try_get_attr(obj, 'attr').\n"
            "**You MUST Paste this snippet right after your imports.**\n"
        ),
    }
    header = headers[key]
    return _json.dumps({
        "success": True,
        "report": header + "\n" + SNIPPETS[key],
    }, indent=2)

@function_tool
async def parse_local_package(
    package_name: str = None,
    package_path: str = None
) -> str:
    f"""
    Parse a locally installed Python package into the Neo4j knowledge graph.
    This tool analyzes a locally installed Python package, extracts its code structure,
    and stores it in Neo4j for use in knowledge graph exploration. The tool supports
    flexible package location strategies:
    **Package Location Strategies:**
    1. **Auto-detection**: Provide only `package_name` - tool will use `uv pip show` to locate the package
    2. **Direct path**: Provide only `package_path` - tool will parse the specified directory directly
    3. **Hybrid approach**: Provide both - tool will try `package_path` first, fallback to `package_name` auto-detection
    **Error Handling:**
    - If package_name cannot be found or is not installed, provides detailed error with suggestions
    - If package_path doesn't exist but package_name is provided, tries auto-detection as fallback
    - Returns clear error messages with actionable suggestions
    **Features:**
    - Locates the package in the Python environment or uses provided path
    - Analyzes Python files to extract code structure  
    - Stores classes, methods, functions, and imports in Neo4j
    - Provides detailed statistics about the parsing results
    - Ensures version compatibility with the local environment
    - Checks if the specific version already exists before parsing
    PATH HANDLING for package_path:
    - Absolute path: used as-is.
    - Relative path: resolved against the active environment's site-packages root {SITE_PACKAGES_HINT}.
      For example, passing "pydantic" will be tried as {SITE_PACKAGES_HINT}/pydantic.
    - If unsure about the absolute path, use your check_package_version tool (if you have this tool) to obtain it.

    Args:
        package_name: Name of the package to parse (optional if package_path is provided)
        package_path: Path to the package directory (optional if package_name is provided)
    Returns:
        JSON string with parsing results, statistics, and package information
    """
    try:
        # Check if knowledge graph functionality is enabled
        knowledge_graph_enabled = os.getenv("USE_KNOWLEDGE_GRAPH", "false") == "true"
        if not knowledge_graph_enabled:
            return json.dumps({
                "success": False,
                "error": "Knowledge graph functionality is disabled. Set USE_KNOWLEDGE_GRAPH=true in environment."
            }, indent=2)
        # Get the repository extractor
        repo_extractor = await get_repo_extractor()
        if not repo_extractor:
            return json.dumps({
                "success": False,
                "error": "Repository extractor not available. Check Neo4j configuration in environment variables."
            }, indent=2)
        # Validate input parameters
        if not package_name and not package_path:
            return json.dumps({
                "success": False,
                "error": "Either package_name or package_path must be provided"
            }, indent=2)
        valid_package_path = None
        detected_version = None
        final_package_name = package_name
        def try_auto_detect_package(pkg_name):
            """Helper function to auto-detect package path from package name"""
            try:
                import subprocess
                result = subprocess.run(
                    ["uv", "pip", "show", pkg_name], 
                    capture_output=True, 
                    text=True, 
                    check=True
                )
                # Parse pip show output to get location and version
                site_packages_path = None
                version = None
                for line in result.stdout.split('\n'):
                    if line.startswith('Location:'):
                        site_packages_path = line.split(':', 1)[1].strip()
                    elif line.startswith('Version:'):
                        version = line.split(':', 1)[1].strip()
                # Construct the actual package path by appending package name to site-packages location
                if site_packages_path:
                    auto_detected_path = os.path.join(site_packages_path, pkg_name)
                    if os.path.exists(auto_detected_path):
                        return auto_detected_path, version
            except subprocess.CalledProcessError:
                pass
            return None, None
        # Helper to resolve relative package_path against site-packages
        def _resolve_relative_package_path(path: str) -> Optional[str]:
            try:
                import sysconfig, site
                bases: list[str] = []
                purelib = sysconfig.get_paths().get("purelib")
                if purelib:
                    bases.append(purelib)
                try:
                    for p in site.getsitepackages():
                        if p not in bases:
                            bases.append(p)
                except Exception:
                    pass
                if not os.path.isabs(path):
                    for root in bases:
                        candidate = os.path.join(root, path)
                        if os.path.exists(candidate):
                            return candidate
            except Exception:
                pass
            return None

        # Case 1: Only package_path provided
        if package_path and not package_name:
            resolved_package_path = package_path
            if not os.path.isabs(resolved_package_path):
                maybe = _resolve_relative_package_path(resolved_package_path)
                if maybe:
                    resolved_package_path = maybe
            if os.path.exists(resolved_package_path):
                valid_package_path = resolved_package_path
                # Try to infer package name from path for better error messages
                final_package_name = os.path.basename(valid_package_path.rstrip('/'))
            else:
                return json.dumps({
                    "success": False,
                    "package_path": package_path,
                    "error": f"Package path does not exist: {package_path}. Please confirm the package_path is correct or use package_name instead and retry this tool.",
                    "suggestions": [
                        "1. Verify the package_path is correct and accessible",
                        "2. Use package_name instead to auto-detect the location",
                        "3. Check if the directory exists and contains Python files"
                    ]
                }, indent=2)
        # Case 2: Only package_name provided  
        elif package_name and not package_path:
            valid_package_path, detected_version = try_auto_detect_package(package_name)
            if not valid_package_path:
                return json.dumps({
                    "success": False,
                    "package_name": package_name,
                    "error": (
                        f"Package '{package_name}' may be incorrect or not installed. "
                        f"Please verify the package name is correct, ensure it's installed, "
                        f"or provide package_path instead (absolute path, or relative path starting from {SITE_PACKAGES_HINT}) and retry this tool. "
                        f"If you have the check_package_version tool, you can use it to obtain the absolute package_path."
                    ),
                    "suggestions": [
                        f"1. Verify package name is correct: {package_name}",
                        f"2. Install the package: uv pip install {package_name}",
                        f"3. Provide package_path manually (absolute path or relative path from {SITE_PACKAGES_HINT})",
                        f"4. If you want to use absolute path, use check_package_version tool (if you have this tool) to get the absolute package_path"
                    ]
                }, indent=2)
        # Case 3: Both package_name and package_path provided
        else:  # both package_name and package_path are provided
            # Try package_path first
            resolved_package_path = package_path
            if not os.path.isabs(resolved_package_path):
                maybe = _resolve_relative_package_path(resolved_package_path)
                if maybe:
                    resolved_package_path = maybe
            if os.path.exists(resolved_package_path):
                valid_package_path = resolved_package_path
                # Infer the actual package name from the path (more reliable than user input)
                inferred_package_name = os.path.basename(valid_package_path.rstrip('/'))
                final_package_name = inferred_package_name
                # Try to get version info using the inferred package name
                _, detected_version = try_auto_detect_package(inferred_package_name)
                # If that fails, try with the provided package_name as fallback
                if not detected_version:
                    _, detected_version = try_auto_detect_package(package_name)
            else:
                # Fallback to package_name auto-detection
                valid_package_path, detected_version = try_auto_detect_package(package_name)
                if not valid_package_path:
                    return json.dumps({
                        "success": False,
                        "package_name": package_name,
                        "package_path": package_path,
                        "error": (
                            f"Both provided package_path '{package_path}' and package_name '{package_name}' "
                            f"are incorrect. Please check both parameters and provide correct information to retry."
                        ),
                        "suggestions": [
                            "1. Verify the package_path exists and is accessible",
                            "2. Verify the package_name is correct and installed",
                            "3. You can provide either package_name or package_path (or both)"
                        ]
                    }, indent=2)
        # Try to get version info if we still don't have it and we have a valid path
        if not detected_version and valid_package_path and final_package_name:
            _, detected_version = try_auto_detect_package(final_package_name)
        # Parse the local package using the valid path and detected version
        # print(f"Starting local package analysis for: {final_package_name}")
        # print(f"Using package path: {valid_package_path}")
        # if detected_version:
        #     print(f"Version: {detected_version}")
        # else:
        #     print("Version: Not detected")
        result = await repo_extractor.analyze_local_package(final_package_name, valid_package_path, detected_version)
        if result.get("skipped", False):
            return json.dumps({
                "success": True,
                "package_name": final_package_name,
                "package_path": valid_package_path,
                "version": detected_version,
                "message": result["message"],
                "skipped": True
            }, indent=2)
        # print(f"Local package analysis completed for: {final_package_name}")
        # Use the result from analyze_local_package
        if result.get("success", False):
            stats = {
                "package": final_package_name,
                "files_processed": result.get("files_processed", 0),
                "classes_created": result.get("classes_created", 0),
                "methods_created": result.get("methods_created", 0), 
                "functions_created": result.get("functions_created", 0)
            }
            return json.dumps({
                "success": True,
                "package_name": final_package_name,
                "package_path": valid_package_path,
                "version": detected_version,
                "message": f"Successfully parsed local package '{final_package_name}' into knowledge graph",
                "statistics": stats,
                "ready_for_validation": True,
                "next_steps": [
                    "Package is now available for knowledge graph exploration",
                    f"Use query_knowledge_graph to explore the knowledge graph for {final_package_name}",
                    "The knowledge graph contains classes, methods, and functions from this package"
                ]
            }, indent=2)
        else:
            return json.dumps({
                "success": False,
                "package_name": final_package_name,
                "error": "Failed to parse local package"
            }, indent=2)
    except Exception as e:
        return json.dumps({
            "success": False,
            "package_name": final_package_name if 'final_package_name' in locals() else package_name,
            "error": f"Local package parsing failed: {str(e)}"
        }, indent=2)

@function_tool
async def query_knowledge_graph(
    command: str
) -> str:
    """
    Query and explore the Neo4j knowledge graph containing repository data.
    This tool provides comprehensive access to the knowledge graph for exploring repositories,
    classes, methods, functions, and their relationships. Perfect for understanding what data
    is available for knowledge graph exploration and debugging validation results.
    **IMPORTANT: Always start with the `repos` command first!**
    Before using any other commands, run `repos` to see what repositories are available
    in your knowledge graph. This will help you understand what data you can explore.
    ## Available Commands:
    **Repository Commands:**
    - `repos` - **START HERE!** List all repositories in the knowledge graph
    - `explore <repo_name>` - Get detailed overview of a specific repository
    **Class Commands:**  
    - `classes` - List all classes across all repositories (limited to 20)
    - `classes <repo_name>` - List classes in a specific repository
    - `class <class_name>` - Get detailed information about a specific class including methods and attributes
    **Method Commands:**
    - `method <method_name>` - Search for methods by name across all classes
    - `method <method_name> <class_name>` - Search for a method within a specific class
    **Custom Query:**
    - `query <cypher_query>` - Execute a custom Cypher query (results limited to 20 records)
    ## Knowledge Graph Schema:
    **Node Types:**
    - Repository: `(r:Repository {name: string})`
    - File: `(f:File {path: string, module_name: string})`
    - Class: `(c:Class {name: string, full_name: string})`
    - Method: `(m:Method {name: string, params_list: [string], params_detailed: [string], return_type: string, args: [string]})`
    - Function: `(func:Function {name: string, params_list: [string], params_detailed: [string], return_type: string, args: [string]})`
    - Attribute: `(a:Attribute {name: string, type: string})`
    **Relationships:**
    - `(r:Repository)-[:CONTAINS]->(f:File)`
    - `(f:File)-[:DEFINES]->(c:Class)`
    - `(c:Class)-[:HAS_METHOD]->(m:Method)`
    - `(c:Class)-[:HAS_ATTRIBUTE]->(a:Attribute)`
    - `(f:File)-[:DEFINES]->(func:Function)`
    ## Example Workflow:
    ```
    1. repos                                    # See what repositories are available
    2. explore pydantic-ai                      # Explore a specific repository
    3. classes pydantic-ai                      # List classes in that repository
    4. class Agent                              # Explore the Agent class
    5. method run_stream                        # Search for run_stream method
    6. method __init__ Agent                    # Find Agent constructor
    7. query "MATCH (c:Class)-[:HAS_METHOD]->(m:Method) WHERE m.name = 'run' RETURN c.name, m.name LIMIT 5"
    ```
    Args:
        command: Command string to execute (see available commands above)
    Returns:
        JSON string with query results, statistics, and metadata
    """
    try:
        # Check if knowledge graph functionality is enabled
        knowledge_graph_enabled = os.getenv("USE_KNOWLEDGE_GRAPH", "false") == "true"
        if not knowledge_graph_enabled:
            return json.dumps({
                "success": False,
                "error": "Knowledge graph functionality is disabled. Set USE_KNOWLEDGE_GRAPH=true in environment."
            }, indent=2)
            
        # Get the repository extractor
        repo_extractor = await get_repo_extractor()
        if not repo_extractor or not repo_extractor.driver:
            return json.dumps({
                "success": False,
                "error": "Neo4j connection not available. Check Neo4j configuration in environment variables."
            }, indent=2)
        
        # Parse command
        command = command.strip()
        if not command:
            return json.dumps({
                "success": False,
                "command": "",
                "error": "Command cannot be empty. Available commands: repos, explore <repo>, classes [repo], class <name>, method <name> [class], query <cypher>"
            }, indent=2)
            
        parts = command.split()
        cmd = parts[0].lower()
        args = parts[1:] if len(parts) > 1 else []
        
        async with repo_extractor.driver.session() as session:
            # Route to appropriate handler
            if cmd == "repos":
                return await _handle_repos_command(session, command)
            elif cmd == "explore":
                if not args:
                    return json.dumps({
                        "success": False,
                        "command": command,
                        "error": "Repository name required. Usage: explore <repo_name>"
                    }, indent=2)
                return await _handle_explore_command(session, command, args[0])
            elif cmd == "classes":
                repo_name = args[0] if args else None
                return await _handle_classes_command(session, command, repo_name)
            elif cmd == "class":
                if not args:
                    return json.dumps({
                        "success": False,
                        "command": command,
                        "error": "Class name required. Usage: class <class_name>"
                    }, indent=2)
                return await _handle_class_command(session, command, args[0])
            elif cmd == "method":
                if not args:
                    return json.dumps({
                        "success": False,
                        "command": command,
                        "error": "Method name required. Usage: method <method_name> [class_name]"
                    }, indent=2)
                method_name = args[0]
                class_name = args[1] if len(args) > 1 else None
                return await _handle_method_command(session, command, method_name, class_name)
            elif cmd == "query":
                if not args:
                    return json.dumps({
                        "success": False,
                        "command": command,
                        "error": "Cypher query required. Usage: query <cypher_query>"
                    }, indent=2)
                cypher_query = " ".join(args)
                return await _handle_query_command(session, command, cypher_query)
            else:
                return json.dumps({
                    "success": False,
                    "command": command,
                    "error": f"Unknown command '{cmd}'. Available commands: repos, explore <repo>, classes [repo], class <name>, method <name> [class], query <cypher>"
                }, indent=2)
        
    except Exception as e:
        return json.dumps({
            "success": False,
            "command": command,
            "error": f"Query execution failed: {str(e)}"
        }, indent=2)

```


================================================================================
=== FILE: mcp_servers_and_tools\direct_tools\search_tool.py ===
================================================================================

```python
#!/usr/bin/env python3
"""
Direct Tavily Search Tool
A direct implementation of Tavily search without MCP server overhead
"""

import os
from agents import function_tool
from tavily import TavilyClient


# Global Tavily client instance
_tavily_client = None


def get_tavily_client():
    """Get Tavily client instance"""
    global _tavily_client
    if _tavily_client is None:
        api_key = os.getenv("TAVILY_API_KEY")
        if not api_key:
            raise ValueError("TAVILY_API_KEY environment variable is not set")
        _tavily_client = TavilyClient(api_key=api_key)
    return _tavily_client


@function_tool(name_override="tavily-search")
def tavily_search(
    query: str,
    search_depth: str = "advanced",
    max_results: int = 5
) -> str:
    """
    Perform a web search using Tavily and return a summarized result.
    
    Args:
        query: The search query string
        search_depth: Search depth - "basic" or "advanced" (default: "advanced")
        max_results: Maximum number of results to return (default: 5)
    
    Returns:
        Search results as a formatted string
    """
    try:
        tavily_client = get_tavily_client()
        
        response = tavily_client.search(
            query=query,
            search_depth=search_depth,
            max_results=max_results
        )
        
        results = response.get("results", [])
        return results or "No results found."
        
    except Exception as e:
        return f"Search error: {str(e)}"

# Export the tool for easy importing
__all__ = ["tavily-search"]


```


================================================================================
=== FILE: mcp_servers_and_tools\direct_tools\workspace_tools.py ===
================================================================================

```python
#!/usr/bin/env python3
"""
Direct Workspace Tools
Direct implementations of workspace management tools without MCP server overhead.
Provides the same functionality as workspace-server but as direct Python functions.
"""

import asyncio
import json
import os
import platform
import random
import subprocess
import tempfile
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Union

from agents import function_tool, RunContextWrapper


# ============================================================================
# CONFIGURATION AND ENVIRONMENT SETUP
# ============================================================================

def get_project_root() -> Path:
    """Get the project root directory by looking for .git directory"""
    current_path = Path(__file__).resolve()
    # Navigate up to find project root (directory containing .git)
    while not (current_path / ".git").exists() and current_path.parent != current_path:
        current_path = current_path.parent
    if (current_path / ".git").exists():
        return current_path
    else:
        # Fallback to environment variable or current working directory
        return Path(os.getenv("PROJECT_ROOT", os.getcwd()))

def resolve_storage_dir(dir_path: str) -> str:
    """Resolve storage directory path, same logic as workspace-server"""
    if not dir_path:
        return ''
    path = Path(dir_path)
    if path.is_absolute():
        return str(path)
    return str(get_project_root() / dir_path)

# Environment configuration - independent absolute paths
PROJECT_ROOT = Path(os.getenv("PROJECT_ROOT", str(get_project_root())))
FORBIDDEN_PATH = Path(os.getenv("FORBIDDEN_PATH", str(PROJECT_ROOT / "benchmark_tasks_and_results")))  # Default fallback only

# Get configuration from environment variables with defaults
CODE_STORAGE_DIR = resolve_storage_dir(os.getenv("CODE_STORAGE_DIR", "deep_solver_benchmark/temp_code"))
SAVED_FILES_DIR = resolve_storage_dir(os.getenv("SAVED_FILES_DIR", "deep_solver_benchmark/saved_code"))

# Default environment settings
ENV_CONFIG = {
    "type": os.getenv("ENV_TYPE", "conda"),
    "conda_name": os.getenv("CONDA_ENV_NAME"),
    "venv_path": os.getenv("VENV_PATH"),
    "uv_venv_path": os.getenv("UV_VENV_PATH")
}

# Validate environment settings
if not CODE_STORAGE_DIR:
    raise ValueError("Missing required environment variable: CODE_STORAGE_DIR")
if not SAVED_FILES_DIR:
    raise ValueError("Missing required environment variable: SAVED_FILES_DIR")

# Only validate environment settings if they are explicitly set
if ENV_CONFIG["type"] == "conda" and ENV_CONFIG["conda_name"] is None:
    # Try to use default conda environment if available
    if os.getenv("CONDA_DEFAULT_ENV"):
        ENV_CONFIG["conda_name"] = os.getenv("CONDA_DEFAULT_ENV")
    else:
        # Fall back to venv if conda is not properly configured
        ENV_CONFIG["type"] = "venv"
        ENV_CONFIG["venv_path"] = os.getenv("VENV_PATH", str(PROJECT_ROOT / ".venv"))
elif ENV_CONFIG["type"] == "venv" and ENV_CONFIG["venv_path"] is None:
    # Use default venv path
    ENV_CONFIG["venv_path"] = str(PROJECT_ROOT / ".venv")
elif ENV_CONFIG["type"] == "venv-uv" and ENV_CONFIG["uv_venv_path"] is None:
    # Use default uv venv path
    ENV_CONFIG["uv_venv_path"] = str(PROJECT_ROOT / ".venv")

# Ensure storage directories exist
Path(CODE_STORAGE_DIR).mkdir(parents=True, exist_ok=True)
Path(SAVED_FILES_DIR).mkdir(parents=True, exist_ok=True)


# ============================================================================
# UTILITY FUNCTIONS
# ============================================================================

def is_path_forbidden(target_path: str) -> bool:
    """Check if a path is forbidden (same logic as workspace-server)"""
    abs_target = Path(target_path).resolve()
    
    # Check if path is inside the forbidden directory (forbidden)
    try:
        forbidden_rel = abs_target.relative_to(FORBIDDEN_PATH)
        # If we can get a relative path, check if it's inside forbidden directory
        if str(forbidden_rel) == '.' or not str(forbidden_rel).startswith('..'):
            return True  # Inside forbidden directory
    except ValueError:
        pass  # Not inside forbidden directory
    
    # Check if path is outside the project scope (forbidden)
    try:
        project_rel = abs_target.relative_to(PROJECT_ROOT)
        # Check for parent directory traversal
        if str(project_rel).startswith('..'):
            return True
    except ValueError:
        return True  # Outside project scope
    
    return False  # Inside project scope, not in forbidden directory

def get_path_forbidden_reason(target_path: str) -> str | None:
    """Get the specific reason why a path is forbidden (same logic as workspace-server)"""
    # Expand environment variables and home directory before path resolution
    expanded_path = target_path
    if target_path.startswith('$'):
        # Handle $VAR and ${VAR} syntax
        if target_path.startswith('${'):
            env_var = target_path[2:-1]
        else:
            env_var = target_path[1:]
        env_value = os.environ.get(env_var)
        if env_value:
            expanded_path = env_value
    elif target_path.startswith('~'):
        # Handle ~ and ~user syntax
        expanded_path = os.path.expanduser(target_path)
    
    abs_target = Path(expanded_path).resolve()
    abs_forbidden_path = FORBIDDEN_PATH.resolve()
    
    # Check if path is inside the forbidden directory FIRST
    try:
        forbidden_rel = abs_target.relative_to(abs_forbidden_path)
        # If we can get a relative path, check if it's inside forbidden directory
        if str(forbidden_rel) == '.' or not str(forbidden_rel).startswith('..'):
            return 'Cannot access forbidden directory. Please check your path.'
    except ValueError:
        pass  # Not inside forbidden directory
    
    # Check if path is outside the project scope
    try:
        project_rel = abs_target.relative_to(PROJECT_ROOT)
        if str(project_rel).startswith('..'):
            return 'Cannot access files outside project root. Please check your path.'
    except ValueError:
        return 'Cannot access files outside project root. Please check your path.'
    
    return None  # Path is allowed

def extract_paths_from_command(command: str) -> List[str]:
    """Extract potential file paths from shell commands (same logic as workspace-server)"""
    import re
    paths = []
    
    # Simple regex to handle quoted strings and basic word splitting
    token_regex = r'"[^"]*"|\'[^\']*\'|\S+'
    words = re.findall(token_regex, command)
    
    for word in words:
        # Remove quotes if present
        if ((word.startswith('"') and word.endswith('"')) or 
            (word.startswith("'") and word.endswith("'"))):
            word = word[1:-1]
        
        # Skip command names and flags
        if word.startswith('-') or word.startswith('--') or \
           word in ['ls', 'cat', 'find', 'grep', 'head', 'tail', 'wc', 'sort', 'uniq', 'xargs']:
            continue
        
        # Check if word looks like a path (contains / or starts with . or ~ or $)
        # But skip shell redirection syntax
        if (('/' in word or word.startswith('.') or word.startswith('~') or word.startswith('$')) and
            not re.match(r'^(\d+>|>|>>|&>)', word)):  # Skip redirection syntax like "2>", ">", ">>", "&>"
            paths.append(word)
    
    # Also check for forbidden directory name specifically
    forbidden_dir_name = FORBIDDEN_PATH.name
    if forbidden_dir_name in command:
        paths.append(forbidden_dir_name)
    
    # Note: Glob pattern checking is handled separately in the main security checks
    
    return paths

def get_platform_specific_command(python_command: str) -> tuple[str, Dict[str, Any]]:
    """Get platform-specific command for environment activation and execution (same logic as workspace-server)"""
    is_windows = platform.system() == "Windows"
    command = ''
    options = {}
    
    if ENV_CONFIG["type"] == "conda":
        if not ENV_CONFIG["conda_name"]:
            raise ValueError("conda_name is required for conda environment")
        if is_windows:
            command = f'conda run -n {ENV_CONFIG["conda_name"]} {python_command}'
            options = {"shell": True}
        else:
            command = f'source $(conda info --base)/etc/profile.d/conda.sh && conda activate {ENV_CONFIG["conda_name"]} && {python_command}'
            options = {"shell": True, "executable": "/bin/bash"}
    
    elif ENV_CONFIG["type"] == "venv":
        if not ENV_CONFIG["venv_path"]:
            raise ValueError("venv_path is required for virtualenv")
        venv_path = Path(ENV_CONFIG["venv_path"])
        if is_windows:
            activate_script = venv_path / "Scripts" / "activate.bat"
            command = f'"{activate_script}" && {python_command}'
            options = {"shell": True}
        else:
            activate_script = venv_path / "bin" / "activate"
            command = f'source "{activate_script}" && {python_command}'
            options = {"shell": True, "executable": "/bin/bash"}
    
    elif ENV_CONFIG["type"] == "venv-uv":
        if not ENV_CONFIG["uv_venv_path"]:
            raise ValueError("uv_venv_path is required for uv virtualenv")
        uv_venv_path = Path(ENV_CONFIG["uv_venv_path"])
        if is_windows:
            activate_script = uv_venv_path / "Scripts" / "activate.bat"
            command = f'"{activate_script}" && {python_command}'
            options = {"shell": True}
        else:
            activate_script = uv_venv_path / "bin" / "activate"
            command = f'source "{activate_script}" && {python_command}'
            options = {"shell": True, "executable": "/bin/bash"}
    
    else:
        raise ValueError(f"Unsupported environment type: {ENV_CONFIG['type']}")
    
    return command, options

def detect_package_manager() -> str:
    """Detect available package manager (same logic as workspace-server)"""
    try:
        # Check if uv is available
        subprocess.run(['uv', '--version'], capture_output=True, timeout=2)
        return 'uv'
    except (subprocess.TimeoutExpired, FileNotFoundError):
        try:
            # Check if conda is available and we're in a conda env
            if os.getenv("CONDA_DEFAULT_ENV"):
                subprocess.run(['conda', '--version'], capture_output=True, timeout=2)
                return 'conda'
        except (subprocess.TimeoutExpired, FileNotFoundError):
            pass
    return 'pip'

def get_package_manager_commands(package_manager: str, packages: Optional[List[str]] = None) -> Dict[str, Any]:
    """Get package manager commands (same logic as workspace-server)"""
    is_windows = platform.system() == "Windows"
    python_cmd = "python" if is_windows else "python3"
    
    commands = {
        "pip": {
            "list": [python_cmd, "-m", "pip", "list", "--format=freeze"],
            "show": lambda pkg: [python_cmd, "-m", "pip", "show", pkg],
            "install": lambda pkgs: [python_cmd, "-m", "pip", "install"] + pkgs
        },
        "uv": {
            "list": ["uv", "pip", "list", "--format=freeze"],
            "show": lambda pkg: ["uv", "pip", "show", pkg],
            "install": lambda pkgs: ["uv", "pip", "install"] + pkgs
        },
        "conda": {
            "list": ["conda", "list", "--export"],
            "show": lambda pkg: ["conda", "list", pkg],
            "install": lambda pkgs: ["conda", "install", "-y"] + pkgs
        }
    }
    
    return commands[package_manager]


# ============================================================================
# MAIN TOOL FUNCTIONS
# ============================================================================

# Internal functions without decorators for testing
async def _execute_code_internal(
    code: str,
    filename: Optional[str] = None
) -> str:
    """Internal implementation of execute_code"""
    try:
        # Handle filename - resolve relative paths from CODE_STORAGE_DIR
        if filename:
            if Path(filename).is_absolute():
                # For absolute paths, use as-is
                abs_file_path = Path(filename)
            else:
                # For relative paths, resolve from CODE_STORAGE_DIR
                abs_file_path = Path(CODE_STORAGE_DIR) / filename
            
            # Check if path is forbidden
            error_reason = get_path_forbidden_reason(str(abs_file_path))
            if error_reason:
                return json.dumps({
                    "status": "error",
                    "error": error_reason,
                    "file_path": str(abs_file_path)
                })
        else:
            # Generate random filename in CODE_STORAGE_DIR
            final_filename = f"code_{random.randint(1000, 9999)}.py"
            abs_file_path = Path(CODE_STORAGE_DIR) / final_filename
        
        # Write code to file
        abs_file_path.write_text(code, encoding='utf-8')
        
        # Get platform-specific command
        python_cmd = f'python -u "{abs_file_path}"' if platform.system() == "Windows" else f'python3 -u "{abs_file_path}"'
        command, options = get_platform_specific_command(python_cmd)
        
        # Execute code
        env = os.environ.copy()
        env["PYTHONUNBUFFERED"] = "1"
        
        result = subprocess.run(
            command,
            cwd=CODE_STORAGE_DIR,
            env=env,
            capture_output=True,
            text=True,
            timeout=300,  # 5 minutes timeout
            **options
        )
        
        # Return all output
        response_output = []
        if result.stderr:
            response_output.append(result.stderr)
        if result.stdout:
            response_output.append(result.stdout)
        
        response = {
            "status": "error" if result.stderr else "success",
            "output": "\n".join(response_output),
            "file_path": str(abs_file_path)
        }
        
        return json.dumps(response)
        
    except subprocess.TimeoutExpired:
        return json.dumps({
            "status": "error",
            "error": "Code execution timed out after 5 minutes",
            "file_path": filename or "unknown"
        })
    except Exception as error:
        return json.dumps({
            "status": "error",
            "error": str(error),
            "file_path": filename or "unknown"
        })

async def _read_file_internal(
    file_path: str
) -> str:
    """Internal implementation of read_file"""
    try:
        # Handle both absolute and relative paths
        if Path(file_path).is_absolute():
            # For absolute paths, use as-is
            abs_file_path = Path(file_path)
        else:
            # For relative paths, resolve from PROJECT_ROOT
            abs_file_path = PROJECT_ROOT / file_path
        
        # Check if path is forbidden
        error_reason = get_path_forbidden_reason(str(abs_file_path))
        if error_reason:
            return json.dumps({
                "status": "error",
                "error": error_reason,
                "file_path": file_path
            })
        
        # Read file content
        content = abs_file_path.read_text(encoding='utf-8')
        
        return json.dumps({
            "status": "success",
            "content": content,
            "file_path": file_path
        })
        
    except FileNotFoundError:
        return json.dumps({
            "status": "error",
            "error": f"File not found: {file_path}",
            "file_path": file_path
        })
    except Exception as error:
        return json.dumps({
            "status": "error",
            "error": str(error),
            "file_path": file_path
        })

async def _install_dependencies_internal(
    packages: List[str]
) -> str:
    """Internal implementation of install_dependencies"""
    try:
        if not packages or len(packages) == 0:
            return json.dumps({
                "status": "error",
                "error": "No packages specified"
            })
        
        # Filter out empty strings and normalize package names
        packages = [pkg.strip() for pkg in packages if pkg and isinstance(pkg, str) and pkg.strip()]
        
        if len(packages) == 0:
            return json.dumps({
                "status": "error",
                "error": "No valid package names provided"
            })
        
        # Auto-detect package manager or use environment-specified one
        if ENV_CONFIG["type"] == "conda":
            package_manager = "conda"
        elif ENV_CONFIG["type"] == "venv-uv":
            package_manager = "uv"
        elif ENV_CONFIG["type"] == "venv":
            package_manager = detect_package_manager()
        else:
            package_manager = detect_package_manager()
        
        commands = get_package_manager_commands(package_manager, packages)
        
        # Build the appropriate command based on detected package manager
        if package_manager == "conda" and ENV_CONFIG["conda_name"]:
            install_cmd = ["conda", "install", "-y", "-n", ENV_CONFIG["conda_name"]] + packages
        else:
            install_cmd = commands["install"](packages)
        
        # Create a temporary Python script to install packages
        temp_id = random.randint(1000, 9999)
        install_script_path = Path(CODE_STORAGE_DIR) / f"install_packages_{temp_id}.py"
        
        install_script = f'''
import subprocess
import sys
import json

def install_packages():
    """Install packages in the current environment."""
    try:
        result = subprocess.run({install_cmd}, 
                              capture_output=True, text=True, check=True)
        return {{
            "status": "success",
            "output": result.stdout,
            "warnings": result.stderr
        }}
    except Exception as e:
        return {{"status": "error", "error": str(e)}}

# Install packages
install_result = install_packages()

# Return the result
print(json.dumps(install_result))
'''
        
        install_script_path.write_text(install_script, encoding='utf-8')
        
        # Execute the install script
        python_cmd = f'python -u "{install_script_path}"' if platform.system() == "Windows" else f'python3 -u "{install_script_path}"'
        command, options = get_platform_specific_command(python_cmd)
        
        env = os.environ.copy()
        env["PYTHONUNBUFFERED"] = "1"
        
        result = subprocess.run(
            command,
            cwd=CODE_STORAGE_DIR,
            env=env,
            capture_output=True,
            text=True,
            timeout=300,  # 5 minutes timeout
            **options
        )
        
        # Clean up the temporary script
        try:
            install_script_path.unlink()
        except:
            pass
        
        if result.stderr:
            return json.dumps({
                "status": "error",
                "env_type": ENV_CONFIG["type"],
                "package_manager": package_manager,
                "error": result.stderr
            })
        
        # Parse the response from the Python script
        try:
            parsed = json.loads(result.stdout.strip())
            return json.dumps(parsed)
        except json.JSONDecodeError:
            return json.dumps({
                "status": "error",
                "env_type": ENV_CONFIG["type"],
                "package_manager": package_manager,
                "error": f"Failed to parse output: {result.stdout}"
            })
        
    except Exception as error:
        return json.dumps({
            "status": "error",
            "env_type": ENV_CONFIG["type"],
            "error": str(error)
        })

async def _check_installed_packages_internal() -> str:
    """Internal implementation of check_installed_packages"""
    try:
        # Auto-detect package manager or use environment-specified one
        if ENV_CONFIG["type"] == "conda":
            package_manager = "conda"
        elif ENV_CONFIG["type"] == "venv-uv":
            package_manager = "uv"
        elif ENV_CONFIG["type"] == "venv":
            package_manager = detect_package_manager()
        else:
            package_manager = detect_package_manager()
        
        commands = get_package_manager_commands(package_manager)
        
        # Create a temporary Python script to get all installed packages
        temp_id = random.randint(1000, 9999)
        check_script_path = Path(CODE_STORAGE_DIR) / f"list_packages_{temp_id}.py"
        
        # Build the appropriate command based on detected package manager
        if package_manager == "conda" and ENV_CONFIG["conda_name"]:
            list_cmd = ["conda", "list", "-n", ENV_CONFIG["conda_name"], "--export"]
        else:
            list_cmd = commands["list"]
        
        check_script = f'''
import subprocess
import sys
import json

def get_all_installed_packages():
    """Get all installed packages in the current environment."""
    try:
        result = subprocess.run({list_cmd}, 
                              capture_output=True, text=True, check=True)
        packages = []
        for line in result.stdout.strip().split('\\n'):
            if line and ('==' in line or '=' in line):
                # Handle both pip freeze format (==) and conda format (=)
                if '==' in line:
                    name, version = line.split('==', 1)
                else:
                    parts = line.split('=')
                    name, version = parts[0], parts[1] if len(parts) > 1 else 'unknown'
                packages.append({{
                    "name": name.strip(),
                    "version": version.strip()
                }})
        return packages
    except Exception as e:
        return {{"error": str(e)}}

# Get all installed packages
all_packages = get_all_installed_packages()

# Return the result
if isinstance(all_packages, list):
    result = {{
        "status": "success",
        "total_packages": len(all_packages),
        "packages": all_packages,
        "package_manager": "{package_manager}"
    }}
else:
    result = {{
        "status": "error",
        "error": all_packages.get("error", "Unknown error"),
        "package_manager": "{package_manager}"
    }}

print(json.dumps(result))
'''
        
        check_script_path.write_text(check_script, encoding='utf-8')
        
        # Execute the check script
        python_cmd = f'python -u "{check_script_path}"' if platform.system() == "Windows" else f'python3 -u "{check_script_path}"'
        command, options = get_platform_specific_command(python_cmd)
        
        env = os.environ.copy()
        env["PYTHONUNBUFFERED"] = "1"
        
        result = subprocess.run(
            command,
            cwd=CODE_STORAGE_DIR,
            env=env,
            capture_output=True,
            text=True,
            timeout=60,  # 1 minute timeout
            **options
        )
        
        # Clean up the temporary script
        try:
            check_script_path.unlink()
        except:
            pass
        
        if result.stderr:
            return json.dumps({
                "status": "error",
                "env_type": ENV_CONFIG["type"],
                "package_manager": package_manager,
                "error": result.stderr
            })
        
        # Parse the response from the Python script
        try:
            parsed = json.loads(result.stdout.strip())
        except json.JSONDecodeError:
            return json.dumps({
                "status": "error",
                "env_type": ENV_CONFIG["type"],
                "package_manager": package_manager,
                "error": f"Failed to parse output: {result.stdout}"
            })
        
        # Build the final response
        response = {
            "status": parsed.get("status", "error"),
            "env_type": ENV_CONFIG["type"],
            "package_manager": parsed.get("package_manager", package_manager),
            "total_packages": parsed.get("total_packages", 0),
            "installed_packages": parsed.get("packages", []),
            "error": parsed.get("error")
        }
        
        return json.dumps(response)
        
    except Exception as error:
        return json.dumps({
            "status": "error",
            "env_type": ENV_CONFIG["type"],
            "error": str(error)
        })

async def _check_package_version_internal(
    packages: List[str]
) -> str:
    """Internal implementation of check_package_version"""
    try:
        if not packages or len(packages) == 0:
            return json.dumps({
                "status": "error",
                "error": "No packages specified"
            })
        
        # Filter out empty strings and normalize package names
        packages = [pkg.strip() for pkg in packages if pkg and isinstance(pkg, str) and pkg.strip()]
        
        if len(packages) == 0:
            return json.dumps({
                "status": "error",
                "error": "No valid package names provided"
            })
        
        # Auto-detect package manager or use environment-specified one
        if ENV_CONFIG["type"] == "conda":
            package_manager = "conda"
        elif ENV_CONFIG["type"] == "venv-uv":
            package_manager = "uv"
        elif ENV_CONFIG["type"] == "venv":
            package_manager = detect_package_manager()
        else:
            package_manager = detect_package_manager()
        
        commands = get_package_manager_commands(package_manager)
        
        results = []
        for package_name in packages:
            version = "unknown"
            package_path = "unknown"
            location = "unknown"
            error = None
            
            # Step 1: Try to get version using appropriate package manager show command
            try:
                if package_manager == "conda" and ENV_CONFIG["conda_name"]:
                    show_cmd = ["conda", "list", "-n", ENV_CONFIG["conda_name"], package_name]
                else:
                    show_cmd = commands["show"](package_name)
                
                command, options = get_platform_specific_command(" ".join(show_cmd))
                
                result = subprocess.run(
                    command,
                    cwd=CODE_STORAGE_DIR,
                    env=os.environ.copy(),
                    capture_output=True,
                    text=True,
                    timeout=30,
                    **options
                )
                
                lines = result.stdout.split("\n")
                for line in lines:
                    if line.startswith("Version:"):
                        version = line.split(":")[1].strip()
                    if line.startswith("Location:"):
                        location = line.split(":")[1].strip()
                        # Try different possible package paths
                        possible_paths = [
                            os.path.join(location, package_name),
                            os.path.join(location, package_name.replace("-", "_")),
                            os.path.join(location, package_name.replace("-", "/")),
                        ]
                        
                        # For packages ending with -py, try removing the suffix
                        if package_name.endswith("-py"):
                            possible_paths.append(os.path.join(location, package_name[:-3]))
                        
                        # For packages with multiple hyphens, try first part only
                        if "-" in package_name and package_name.count("-") > 1:
                            first_part = package_name.split("-")[0]
                            possible_paths.append(os.path.join(location, first_part))
                        
                        # Find the first existing path
                        package_path = "unknown"
                        for path in possible_paths:
                            if os.path.exists(path):
                                package_path = path
                                break
                        
                        # Fallback to original logic if none found
                        if package_path == "unknown":
                            package_path = os.path.join(location, package_name)
                        
            except Exception as e:
                error = f"{package_manager} show failed: {e}"
            
            # Step 2: Try multiple import strategies to find the correct module
            try:
                # Generate all possible variations of the package name
                variations = {package_name}
                
                # Replace hyphens with underscores
                if '-' in package_name:
                    variations.add(package_name.replace('-', '_'))
                
                # Replace hyphens with dots
                if '-' in package_name:
                    variations.add(package_name.replace('-', '.'))
                
                # Replace underscores with hyphens
                if '_' in package_name:
                    variations.add(package_name.replace('_', '-'))
                
                # Replace dots with hyphens
                if '.' in package_name:
                    variations.add(package_name.replace('.', '-'))
                
                # Replace dots with underscores
                if '.' in package_name:
                    variations.add(package_name.replace('.', '_'))
                
                # Try each variation until one succeeds
                success = False
                for variation in variations:
                    try:
                        py_cmd = f'python -c "import {variation}; print({variation}.__file__); print(getattr({variation}, \'__version__\', \'no __version__\'))"'
                        command, options = get_platform_specific_command(py_cmd)
                        
                        result = subprocess.run(
                            command,
                            cwd=CODE_STORAGE_DIR,
                            env=os.environ.copy(),
                            capture_output=True,
                            text=True,
                            timeout=30,
                            **options
                        )
                        
                        if result.stdout.strip():
                            lines = result.stdout.strip().split("\n")
                            # Check if we got a valid file path (not None)
                            if len(lines) >= 1 and lines[0] != 'None':
                                location = lines[0]
                                if package_path == "unknown" and location:
                                    package_path = str(Path(location).parent)
                            elif len(lines) >= 1 and lines[0] == 'None':
                                # Handle namespace packages where __file__ is None
                                # Try to get the path from __path__
                                try:
                                    path_cmd = f'python -c "import {variation}; print(str({variation}.__path__))"'
                                    command, options = get_platform_specific_command(path_cmd)
                                    
                                    result = subprocess.run(
                                        command,
                                        cwd=CODE_STORAGE_DIR,
                                        env=os.environ.copy(),
                                        capture_output=True,
                                        text=True,
                                        timeout=30,
                                        **options
                                    )
                                    
                                    if result.stdout.strip() and result.stdout.strip() != 'None':
                                        # Extract path from _NamespacePath format
                                        import re
                                        path_match = re.search(r"_NamespacePath\(\[['\"]([^'\"]+)['\"]\]\)", result.stdout.strip())
                                        if path_match:
                                            location = path_match.group(1)
                                            if package_path == "unknown" and location:
                                                package_path = str(Path(location).parent)
                                except:
                                    pass
                            
                            if len(lines) >= 2 and lines[1] != "no __version__":
                                version = lines[1]
                            success = True
                            break  # Found a working variation
                            
                    except:
                        continue
                
                # Final fallback: try importlib with original name
                if not success:
                    py_cmd = f'python -c "import importlib; pkg = importlib.import_module(\'{package_name}\'); print(pkg.__file__); print(getattr(pkg, \'__version__\', \'no __version__\'))"'
                    command, options = get_platform_specific_command(py_cmd)
                    
                    result = subprocess.run(
                        command,
                        cwd=CODE_STORAGE_DIR,
                        env=os.environ.copy(),
                        capture_output=True,
                        text=True,
                        timeout=30,
                        **options
                    )
                    
                    if result.stdout.strip():
                        lines = result.stdout.strip().split("\n")
                        # Check if we got a valid file path (not None)
                        if len(lines) >= 1 and lines[0] != 'None':
                            location = lines[0]
                            if package_path == "unknown" and location:
                                package_path = str(Path(location).parent)
                        elif len(lines) >= 1 and lines[0] == 'None':
                            # Handle namespace packages where __file__ is None
                            try:
                                path_cmd = f'python -c "import importlib; pkg = importlib.import_module(\'{package_name}\'); print(str(pkg.__path__))"'
                                command, options = get_platform_specific_command(path_cmd)
                                
                                result = subprocess.run(
                                    command,
                                    cwd=CODE_STORAGE_DIR,
                                    env=os.environ.copy(),
                                    capture_output=True,
                                    text=True,
                                    timeout=30,
                                    **options
                                )
                                
                                if result.stdout.strip() and result.stdout.strip() != 'None':
                                    # Extract path from _NamespacePath format
                                    import re
                                    path_match = re.search(r"_NamespacePath\(\[['\"]([^'\"]+)['\"]\]\)", result.stdout.strip())
                                    if path_match:
                                        location = path_match.group(1)
                                        if package_path == "unknown" and location:
                                            package_path = str(Path(location).parent)
                            except:
                                pass
                        
                        if len(lines) >= 2 and lines[1] != "no __version__":
                            version = lines[1]
                        success = True
                        
            except Exception as e:
                if not error:
                    error = f"python import failed: {e}"
            
            # Final fix: Use location from import
            if location and location.endswith('__init__.py'):
                dir_from_location = location[:location.rfind('/__init__.py')]
                if dir_from_location:
                    package_path = dir_from_location
            
            results.append({
                "package_name": package_name,
                "version": version,
                "package_path": package_path,
                "location": location,
                "error": error,
                "summary": f"Package {package_name} version {version} at {package_path}"
            })
        
        return json.dumps({
            "status": "success",
            "env_type": ENV_CONFIG["type"],
            "package_manager": package_manager,
            "venv_path": ENV_CONFIG.get("venv_path"),
            "package_details": results
        })
        
    except Exception as error:
        return json.dumps({
            "status": "error",
            "error": str(error),
            "env_type": ENV_CONFIG["type"],
            "venv_path": ENV_CONFIG.get("venv_path")
        })

async def _save_file_internal(
    content: str,
    filename: str
) -> str:
    """Internal implementation of save_file"""
    try:
        # Handle filename - resolve relative paths from SAVED_FILES_DIR
        if Path(filename).is_absolute():
            # For absolute paths, use as-is
            file_path = Path(filename)
        else:
            # For relative paths, resolve from SAVED_FILES_DIR
            file_path = Path(SAVED_FILES_DIR) / filename
        
        # Check if path is forbidden
        error_reason = get_path_forbidden_reason(str(file_path))
        if error_reason:
            return json.dumps({
                "status": "error",
                "error": error_reason,
                "filename": filename
            })
        
        # Ensure filename has .py extension if it's a relative path
        if not Path(filename).is_absolute() and not file_path.name.endswith('.py'):
            file_path = file_path.with_suffix('.py')
        
        # Write content to file
        file_path.write_text(content, encoding='utf-8')
        
        return json.dumps({
            "status": "success",
            "message": "File saved successfully",
            "file_path": str(file_path),
            "filename": filename
        })
        
    except Exception as error:
        return json.dumps({
            "status": "error",
            "error": str(error)
        })

async def _execute_shell_command_internal(
    command: str,
    working_dir: Optional[str] = None
) -> str:
    """Internal implementation of execute_shell_command with 6-layer security architecture"""
    try:
        # Security check for working directory (Layer 1)
        resolved_working_dir = working_dir
        if working_dir:
            # Handle both absolute and relative paths
            if Path(working_dir).is_absolute():
                abs_working_dir = Path(working_dir)
            else:
                abs_working_dir = PROJECT_ROOT / working_dir
            
            error_reason = get_path_forbidden_reason(str(abs_working_dir))
            if error_reason:
                return json.dumps({
                    "status": "error",
                    "error": error_reason
                })
            resolved_working_dir = str(abs_working_dir)
        
        # Enhanced security checks for shell commands using comprehensive forbidden path detection
        
        # 1. Extract and check potential file paths from the command (Layer 2)
        potential_paths = extract_paths_from_command(command)
        for path in potential_paths:
            error_reason = get_path_forbidden_reason(path)
            if error_reason:
                return json.dumps({
                    "status": "error",
                    "error": error_reason
                })
        
        # 2. Check for dangerous commands that could access forbidden paths (Layer 3)
        forbidden_dir_name = FORBIDDEN_PATH.name
        dangerous_patterns = [
            rf'find\s+.*{forbidden_dir_name}',
            rf'grep\s+.*{forbidden_dir_name}',
            rf'cat\s+.*{forbidden_dir_name}',
            rf'cp\s+.*{forbidden_dir_name}',
            rf'mv\s+.*{forbidden_dir_name}',
            rf'rm\s+.*{forbidden_dir_name}',
            rf'ls\s+.*{forbidden_dir_name}',
            rf'tar\s+.*{forbidden_dir_name}',
            rf'zip\s+.*{forbidden_dir_name}',
            rf'unzip\s+.*{forbidden_dir_name}',
        ]
        
        import re
        for pattern in dangerous_patterns:
            if re.search(pattern, command, re.IGNORECASE):
                return json.dumps({
                    "status": "error",
                    "error": f"Command contains explicit reference to forbidden directory: {command}"
                })
        
        # 3. Check for glob patterns that could access forbidden content (Layer 4)
        if '*' in command:
            # Check if the command uses dangerous relative paths that could access forbidden content
            # Only block "find ." or "find ./" (without specific subdirectory)
            if (re.search(r'find\s+\.\s', command) or re.search(r'find\s+\.$', command) or
                re.search(r'find\s+\.\/\s', command) or re.search(r'find\s+\.\/$', command)):
                # Commands like "find ." or "find ./" could search in forbidden directories
                return json.dumps({
                    "status": "error",
                    "error": f"Glob pattern with relative path could access forbidden content. Please specify a specific directory: {command}"
                })
            
            # Also check for unrestricted glob patterns without any path restrictions
            # But allow commands that specify a specific directory (like "find deep_solver_benchmark -name '*.py'")
            if '/' not in command and '\\' not in command and not re.search(r'find\s+\w+', command):
                return json.dumps({
                    "status": "error",
                    "error": f"Unrestricted glob pattern could access forbidden content. Please specify a directory: {command}"
                })
        
        # 4. Forbid dangerous commands (Layer 5)
        dangerous_commands = [
            'rm -rf', 'sudo', 'su', 'chmod 777', 'chown root',
            'dd if=', 'mkfs', 'fdisk', 'mount', 'umount',
            'systemctl', 'service', 'init', 'telinit',
            'curl', 'wget', 'nc', 'netcat', 'ssh', 'scp', 'rsync'
        ]
        
        for dangerous_cmd in dangerous_commands:
            if dangerous_cmd in command:
                return json.dumps({
                    "status": "error",
                    "error": f"Dangerous commands are forbidden: {dangerous_cmd}"
                })
        
        # Execute command with output-based security monitoring
        actual_working_dir = resolved_working_dir if working_dir else str(PROJECT_ROOT)
        
        result = subprocess.run(
            command,
            cwd=actual_working_dir,
            env=os.environ.copy(),
            capture_output=True,
            text=True,
            timeout=300,  # 5 minutes timeout
            shell=True
        )
        
        # Check if output contains forbidden directory content (Layer 6)
        forbidden_path = str(FORBIDDEN_PATH)
        
        # More comprehensive check for forbidden directory content
        forbidden_patterns = [
            f'"{forbidden_dir_name}"',           # "benchmark"
            f'/{forbidden_dir_name}/',           # /benchmark/
            f'{forbidden_dir_name}/',            # benchmark/
            forbidden_path,                     # Full absolute path
            str(Path(forbidden_path).relative_to(PROJECT_ROOT)), # Relative path from project root
        ]
        
        output_to_check = result.stdout + result.stderr
        for pattern in forbidden_patterns:
            if pattern and pattern in output_to_check:
                return json.dumps({
                    "status": "error",
                    "error": f"Command output contains content from forbidden directory: {pattern}"
                })
        
        response = {
            "status": "warning" if result.stderr else "success",
            "stdout": result.stdout,
            "stderr": result.stderr
        }
        
        return json.dumps(response)
        
    except subprocess.TimeoutExpired:
        return json.dumps({
            "status": "error",
            "error": "Command execution timed out after 5 minutes"
        })
    except Exception as error:
        return json.dumps({
            "status": "error",
            "error": "Command execution failed"
        })

async def _create_and_execute_script_internal(
    script_content: str,
    filename: Optional[str] = None,
    interpreter: Optional[str] = None
) -> str:
    """Internal implementation of create_and_execute_script"""
    try:
        # Generate filename if not provided
        if not filename:
            filename = f"script_{random.randint(1000, 9999)}.sh"
        
        # Handle filename - resolve relative paths from CODE_STORAGE_DIR
        if Path(filename).is_absolute():
            # For absolute paths, use as-is
            file_path = Path(filename)
        else:
            # For relative paths, resolve from CODE_STORAGE_DIR
            file_path = Path(CODE_STORAGE_DIR) / filename
        
        # Check if path is forbidden
        error_reason = get_path_forbidden_reason(str(file_path))
        if error_reason:
            return json.dumps({
                "status": "error",
                "error": error_reason,
                "filename": filename
            })
        
        # Add shebang if not present and interpreter is specified
        final_content = script_content
        if interpreter and not script_content.startswith('#!'):
            final_content = f"#!/usr/bin/env {interpreter}\n\n{script_content}"
        elif not script_content.startswith('#!'):
            # Default to bash if no shebang
            final_content = f"#!/usr/bin/env bash\n\n{script_content}"
        
        # Write script to file
        file_path.write_text(final_content, encoding='utf-8')
        
        # Make script executable (Unix-like systems)
        if platform.system() != "Windows":
            os.chmod(file_path, 0o755)
        
        # Execute the script
        result = subprocess.run(
            str(file_path),
            cwd=CODE_STORAGE_DIR,
            env=os.environ.copy(),
            capture_output=True,
            text=True,
            timeout=300,  # 5 minutes timeout
            shell=True
        )
        
        response = {
            "status": "warning" if result.stderr else "success",
            "stdout": result.stdout,
            "stderr": result.stderr,
            "script_path": str(file_path),
            "script_content": final_content
        }
        
        return json.dumps(response)
        
    except subprocess.TimeoutExpired:
        return json.dumps({
            "status": "error",
            "error": "Script execution timed out after 5 minutes",
            "script_path": filename or "unknown",
            "script_content": script_content
        })
    except Exception as error:
        return json.dumps({
            "status": "error",
            "error": str(error),
            "script_path": filename or "unknown",
            "script_content": script_content
        })

@function_tool
async def read_file(
    file_path: str
) -> str:
    f"""
    Read the content of any text file (Python code, log files, output files, etc.). Use this to examine local package code files, output files from external programs, log files, or any text-based files to extract specific information
    
    PATH HANDLING: Prefer absolute paths. If using relative paths, they are resolved from {PROJECT_ROOT}
    
    Args:
        file_path: Path to the file to read (supports any text file format: .py, .log, .txt, .out, .xyz, etc.). Using absolute paths (preferred) or relative paths from {PROJECT_ROOT}
    
    Returns:
        JSON string with file content and status
    """
    return await _read_file_internal(file_path)

@function_tool
async def install_dependencies(
    packages: List[str]
) -> str:
    """
    Install missing Python dependencies in the configured environment. Use this tool to install packages that are required for your code to run. Example: {"packages": ["package1", "package2", "package3"]}
    
    Args:
        packages: Array of package names to install. Each package should be a string. Example: ["package1", "package2", "package3"]
    
    Returns:
        JSON string with installation results
    """
    return await _install_dependencies_internal(packages)

@function_tool
async def check_installed_packages() -> str:
    """
    List all installed packages in the current Python environment. Use this tool to check what packages are already available before attempting to install new ones
    
    Returns:
        JSON string with all installed packages information
    """
    return await _check_installed_packages_internal()

@function_tool
async def check_package_version(
    packages: List[str]
) -> str:
    """
    Check if specific packages are installed and get their version, package path, and module location information. Use this tool to verify specific package installations. Example: {"packages": ["package1", "package2", "package3"]}
    
    Args:
        packages: Array of package names to check. Each package should be a string. Example: ["package1", "package2", "package3"]
    
    Returns:
        JSON string with package version and location information
    """
    return await _check_package_version_internal(packages)

@function_tool
async def save_file(
    content: str,
    filename: str
) -> str:
    f"""
    Save a file to {SAVED_FILES_DIR} with the specified filename
    
    Args:
        content: Content of the file
        filename: Filename of the file
    
    Returns:
        JSON string with save results
    """
    return await _save_file_internal(content, filename)

@function_tool
async def execute_shell_command(
    command: str,
    working_dir: Optional[str] = None
) -> str:
    f"""
    Execute a shell command and return the result. Default working directory is {PROJECT_ROOT}
    
    PATH HANDLING: Prefer absolute paths for working_dir. If using relative paths for working_dir, they are resolved from {PROJECT_ROOT}
    
    Args:
        command: Shell command to execute
        working_dir: Working directory for the command (absolute path or relative path from {PROJECT_ROOT}). Default is {PROJECT_ROOT}
    
    Returns:
        JSON string with command execution results
    """
    return await _execute_shell_command_internal(command, working_dir)

@function_tool
async def create_and_execute_script(
    script_content: str,
    filename: Optional[str] = None,
    interpreter: Optional[str] = None
) -> str:
    f"""
    Create and execute a shell script. Scripts are created in {CODE_STORAGE_DIR}
    
    Args:
        script_content: Content of the script
        filename: Optional name of the script file
        interpreter: Optional interpreter for the script
    
    Returns:
        JSON string with script execution results
    """
    return await _create_and_execute_script_internal(script_content, filename, interpreter)


# Create decorated versions for agents framework
@function_tool
async def execute_code(
    code: str,
    filename: Optional[str] = None
) -> str:
    f"""
    Execute Python code in the configured environment. Code is saved to {CODE_STORAGE_DIR} and executed. Use this tool to run Python scripts and get their output.
    
    Args:
        code: Python code to execute
        filename: Optional name of the file to save the code (default: generated UUID)
    
    Returns:
        JSON string with execution results including status, output, and file path
    """
    return await _execute_code_internal(code, filename)

# Export all tools
__all__ = [
    "execute_code",
    "read_file", 
    "install_dependencies",
    "check_installed_packages",
    "check_package_version",
    "save_file",
    "execute_shell_command",
    "create_and_execute_script"
]

```


================================================================================
=== FILE: mcp_servers_and_tools\direct_tools\__init__.py ===
================================================================================

```python
#!/usr/bin/env python3
"""
Direct Tools Package
Direct implementations of tools without MCP server overhead for faster execution
"""

from .search_tool import tavily_search
from .research_tools import (
    extract_code_from_url,
    retrieve_extracted_code,
    quick_introspect,
    runtime_probe_snippet,
    parse_local_package,
    query_knowledge_graph
)
from .workspace_tools import (
    execute_code,
    read_file,
    install_dependencies,
    check_installed_packages,
    check_package_version,
    save_file,
    execute_shell_command,
    create_and_execute_script
)

__all__ = [
    # Search tools
    "tavily_search",
    # Research tools
    "extract_code_from_url",
    "retrieve_extracted_code",
    "quick_introspect",
    "runtime_probe_snippet",
    "parse_local_package",
    "query_knowledge_graph",
    # Workspace tools
    "execute_code",
    "read_file",
    "install_dependencies",
    "check_installed_packages",
    "check_package_version",
    "save_file",
    "execute_shell_command",
    "create_and_execute_script",
]

# Memory tools are NOT imported by default to avoid side effects at import time
# (creates mem0 instance, requires OPENAI_API_KEY, SUPABASE_DATABASE_URL, Neo4j)
# Import directly when needed:
#   from mcp_servers_and_tools.direct_tools.memory_tools import search_memory, save_to_memory
```


================================================================================
=== FILE: mcp_servers_and_tools\memory_server\pyproject.toml ===
================================================================================

```toml
[project]
name = "memory-server"
version = "0.1.0"
description = "Memory Server - Specialized MCP server for memory consolidation and retrieval using mem0 with hybrid vector-graph architecture"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "mem0ai[graph]==0.1.118",
    "mcp==1.13.0",
    "supabase==2.18.1",
    "openai==1.102.0",
    "python-dotenv==1.1.1",
    "neo4j==5.28.2",
    "vecs==0.4.5",
]

```


================================================================================
=== FILE: mcp_servers_and_tools\memory_server\README.md ===
================================================================================

```markdown
# Memory Server

A specialized MCP (Model Context Protocol) server for memory consolidation and retrieval using [mem0](https://github.com/mem0ai/mem0).

## Overview

The Memory Server provides comprehensive memory capabilities for AI agents through a hybrid dual-store architecture:

- **Vector Store (Supabase)**: Semantic similarity search over embedded memories
- **Graph Store (Neo4j)**: Entity-relationship extraction and querying

## Features

- **Semantic Memory Search**: Find relevant past interactions using natural language queries
- **Dual-Path Save Strategy**:
  1. LLM-extracted facts (searchable key points) - stored in both vector and graph stores
  2. Complete original content (verbatim) - stored in vector store only
- **Custom Prompts**: Optimized for materials science and chemistry research contexts
- **User-Scoped Memories**: All operations are scoped by user_id for personalization

## Tools

### `search_memory`
Search through stored memories for relevant information.

**Parameters:**
- `query` (str): What to search for
- `user_id` (str): User identifier for scoping memories

**Returns:** Relevant memories and entity relationships

### `save_to_memory`
Save important information to memory for future reference.

**Parameters:**
- `content` (str): Content to save (queries, solutions, preferences, etc.)
- `user_id` (str): User identifier for scoping memories

**Returns:** Status message

## Installation

```bash
cd memory_server
pip install -e .
```

## Configuration

Copy `.env.example` to `.env` and configure:

```bash
cp .env.example .env
```

## Running the Server

```bash
# Using Python directly
python src/memory_mcp.py

# Or using uv
uv run src/memory_mcp.py
```

## Database Setup

### Supabase (Vector Store)

1. Create a Supabase project at [supabase.com](https://supabase.com)
2. Get your database connection string from Project Settings -> Connect -> Session pooler
3. The required tables are created automatically by mem0 on first use

### Neo4j (Graph Store)

1. Install Neo4j from [neo4j.com/download](https://neo4j.com/download/)
2. Start the database and set a password
3. Configure `NEO4J_URI`, `NEO4J_USER`, and `NEO4J_PASSWORD`

To disable graph memory (use vector store only), set `ENABLE_GRAPH_MEMORY=false`.

```


================================================================================
=== FILE: mcp_servers_and_tools\memory_server\test_memory_server.py ===
================================================================================

```python
#!/usr/bin/env python3
"""
Test for Memory Server MCP tools.
Tests search_memory and save_to_memory tools.
"""
import asyncio
import os
import sys
import logging
from pathlib import Path

# Suppress noisy logging from external libraries
logging.basicConfig(level=logging.WARNING)
for logger_name in ["httpx", "openai", "mcp", "supabase", "neo4j", "mem0", "vecs"]:
    logging.getLogger(logger_name).setLevel(logging.WARNING)

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from dotenv import load_dotenv

# Load environment variables from project root
project_root = Path(__file__).parent.parent
load_dotenv(project_root / '.env')


def get_server_params() -> StdioServerParameters:
    """Get server parameters for MCP client connection."""
    return StdioServerParameters(
        command="python",
        args=["-u", "src/memory_mcp.py"],
        cwd=str(Path(__file__).parent),
        env={
            **os.environ,  # Pass all current environment variables
            "SUPABASE_DATABASE_URL": os.environ.get("SUPABASE_DATABASE_URL", ""),
            "NEO4J_URI": os.environ.get("NEO4J_URI", "bolt://localhost:7687"),
            "NEO4J_USER": os.environ.get("NEO4J_USER", "neo4j"),
            "NEO4J_PASSWORD": os.environ.get("NEO4J_PASSWORD", "password"),
            "ENABLE_GRAPH_MEMORY": os.environ.get("ENABLE_GRAPH_MEMORY", "true"),
            "OPENAI_API_KEY": os.environ.get("OPENAI_API_KEY", ""),
        }
    )


async def test_list_tools(session: ClientSession) -> None:
    """Test listing available tools."""
    print("\n" + "=" * 60)
    print("TEST: List Available Tools")
    print("=" * 60)

    tools = await session.list_tools()
    print(f"Found {len(tools.tools)} tools:")
    for tool in tools.tools:
        print(f"  - {tool.name}: {tool.description[:80]}...")

    # Verify expected tools exist
    tool_names = [t.name for t in tools.tools]
    expected_tools = ["search_memory", "save_to_memory"]
    for expected in expected_tools:
        if expected in tool_names:
            print(f"  âœ… {expected} found")
        else:
            print(f"  âŒ {expected} NOT found")


async def test_save_memory(session: ClientSession, user_id: str) -> None:
    """Test saving memory."""
    print("\n" + "=" * 60)
    print("TEST: Save Memory")
    print("=" * 60)

    test_content = """Acâ‚‚InGa is Heusler structured and crystallizes in the cubic FmÌ…3m space group. Ac is bonded in a body-centered cubic geometry to four equivalent In and four equivalent Ga atoms. All Ac-In bond lengths are 3.49 Ã…. All Ac-Ga bond lengths are 3.49 Ã…. In is bonded in a body-centered cubic geometry to eight equivalent Ac atoms. Ga is bonded in a body-centered cubic geometry to eight equivalent Ac atoms."""

    print(f"Saving test content for user: {user_id}")
    print(f"Content: {test_content}")

    result = await session.call_tool("save_to_memory", {
        "content": test_content,
        "user_id": user_id
    })

    if result.content:
        print(f"Result: {result.content[0].text}")
    else:
        print("No content returned")


async def test_search_memory(session: ClientSession, user_id: str) -> None:
    """Test searching memory."""
    print("\n" + "=" * 60)
    print("TEST: Search Memory")
    print("=" * 60)

    queries = [
        "space group of Acâ‚‚InGa"
    ]

    for query in queries:
        print(f"\nSearching for: '{query}'")
        result = await session.call_tool("search_memory", {
            "query": query,
            "user_id": user_id
        })

        if result.content:
            text = result.content[0].text
            # Truncate long results
            if len(text) > 500:
                text = text[:500] + "..."
            print(f"Result:\n{text}")
        else:
            print("No content returned")


async def main() -> None:
    """Main test function."""
    print("=" * 60)
    print("Memory Server MCP Test")
    print("=" * 60)

    # Check required environment variables
    required_vars = ["SUPABASE_DATABASE_URL", "OPENAI_API_KEY"]
    missing_vars = [v for v in required_vars if not os.environ.get(v)]

    if missing_vars:
        print(f"âš ï¸  Missing required environment variables: {missing_vars}")
        print("Please set these in your .env file")
        print("Continuing with limited testing...")

    # Use a test user ID
    test_user_id = "test_user_memory_server"

    server_params = get_server_params()

    try:
        async with stdio_client(server_params) as (read, write):
            async with ClientSession(read, write) as session:
                print("\nðŸš€ Initializing MCP session...")
                await session.initialize()
                print("âœ… Session initialized successfully")

                # Run tests
                await test_list_tools(session)

                # Only run data tests if we have the required env vars
                if not missing_vars:
                    await test_save_memory(session, test_user_id)

                    # Wait a moment for the save to complete
                    print("\nâ³ Waiting for memory to be processed...")
                    await asyncio.sleep(2)

                    await test_search_memory(session, test_user_id)
                else:
                    print("\nâš ï¸  Skipping data tests due to missing environment variables")

                print("\n" + "=" * 60)
                print("âœ… All tests completed!")
                print("=" * 60)

    except Exception as e:
        print(f"\nâŒ Error during testing: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())

```


================================================================================
=== FILE: mcp_servers_and_tools\memory_server\test_memory_server_basic.py ===
================================================================================

```python
#!/usr/bin/env python3
"""
Basic test for Memory Server - tests MCP structure without database connection.
"""
import asyncio
import sys

# Test 1: Import test
print("=" * 60)
print("TEST 1: Import Memory Server Module")
print("=" * 60)

try:
    sys.path.insert(0, 'src')
    from memory_mcp import (
        mcp,
        search_memory,
        save_to_memory,
        MATERIALS_SCIENCE_EXTRACTION_PROMPT,
        MATERIALS_SCIENCE_GRAPH_PROMPT,
        ENABLE_GRAPH_MEMORY
    )
    print("âœ… All imports successful")
    print(f"   - Server name: {mcp.name}")
    print(f"   - Graph memory enabled: {ENABLE_GRAPH_MEMORY}")
except Exception as e:
    print(f"âŒ Import failed: {e}")
    sys.exit(1)

# Test 2: Check prompts are defined
print("\n" + "=" * 60)
print("TEST 2: Check Custom Prompts")
print("=" * 60)

if MATERIALS_SCIENCE_EXTRACTION_PROMPT:
    print(f"âœ… Extraction prompt defined ({len(MATERIALS_SCIENCE_EXTRACTION_PROMPT)} chars)")
    print(f"   Preview: {MATERIALS_SCIENCE_EXTRACTION_PROMPT[:100]}...")
else:
    print("âŒ Extraction prompt not defined")

if MATERIALS_SCIENCE_GRAPH_PROMPT:
    print(f"âœ… Graph prompt defined ({len(MATERIALS_SCIENCE_GRAPH_PROMPT)} chars)")
    print(f"   Preview: {MATERIALS_SCIENCE_GRAPH_PROMPT[:100]}...")
else:
    print("âŒ Graph prompt not defined")

# Test 3: Check MCP tools are registered
print("\n" + "=" * 60)
print("TEST 3: Check MCP Tools Registration")
print("=" * 60)

# FastMCP stores tools in _tool_manager
try:
    if hasattr(mcp, '_tool_manager'):
        tools = mcp._tool_manager._tools
        print(f"âœ… Found {len(tools)} registered tools:")
        for name, tool in tools.items():
            print(f"   - {name}")
    else:
        # Alternative way to check
        print("âš ï¸  Cannot directly access tool manager, checking decorated functions...")
        print("   - search_memory: defined" if search_memory else "   - search_memory: NOT defined")
        print("   - save_to_memory: defined" if save_to_memory else "   - save_to_memory: NOT defined")
except Exception as e:
    print(f"âš ï¸  Tool check error: {e}")

print("\n" + "=" * 60)
print("âœ… Basic structure tests completed!")
print("=" * 60)
print("\nNote: To test actual memory operations, ensure:")
print("  1. SUPABASE_DATABASE_URL is correctly set")
print("  2. OPENAI_API_KEY is set")
print("  3. (Optional) Neo4j is running for graph memory")

```


================================================================================
=== FILE: mcp_servers_and_tools\memory_server\src\memory_mcp.py ===
================================================================================

```python
"""
Memory Tools for Conversational System (MCP Server Version)
Provides the same functionality as direct memory_tools but as an MCP server.

Graph Memory:
- Enabled by default (extracts entities and relationships from conversations)
- Set ENABLE_GRAPH_MEMORY=false to disable and use only vector-based memory
- Requires Neo4j database with NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD for graph memory
"""

# Suppress noisy logging from external libraries
import logging
import sys
logging.basicConfig(level=logging.WARNING)
for logger_name in ["httpx", "openai", "mcp", "supabase", "neo4j", "mem0", "vecs", "numexpr"]:
    logging.getLogger(logger_name).setLevel(logging.WARNING)

# Create logger for this module (outputs to stderr, won't interfere with MCP stdio)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
if not logger.handlers:  # Prevent duplicate handlers on reload
    _handler = logging.StreamHandler(sys.stderr)
    _handler.setFormatter(logging.Formatter('[%(name)s] %(message)s'))
    logger.addHandler(_handler)

from mcp.server.fastmcp import FastMCP, Context
from contextlib import asynccontextmanager
from collections.abc import AsyncIterator
from dataclasses import dataclass
from typing import Any
from dotenv import load_dotenv
from pathlib import Path
from datetime import datetime
import os

# Load environment variables from the project root .env file
# Priority: .env file > shell environment (.bashrc) > code defaults
project_root = Path(__file__).resolve().parent.parent
dotenv_path = project_root / '.env'
load_dotenv(dotenv_path, override=True)

# Also try loading from parent directory (workspace root)
workspace_root = project_root.parent
workspace_dotenv = workspace_root / '.env'
if workspace_dotenv.exists():
    load_dotenv(workspace_dotenv, override=True)

# ============================================================================
# CUSTOM MEMORY PROMPTS (same as conversational_system/config/custom_memory_prompts.py)
# ============================================================================

MATERIALS_SCIENCE_EXTRACTION_PROMPT = f"""You are a Materials Science and Chemistry Research Assistant specialized in accurately storing user preferences, technical solutions, API usage patterns, code implementation details, and scientific knowledge. Your role is to extract relevant information from conversations and organize them into distinct, manageable facts for easy retrieval when solving similar problems in the future.

EXTRACTION PURPOSE: The extracted facts will be used to help solve similar problems in the future. Therefore, focus on actionable implementation details that enable problem-solving, not just descriptions of what was asked.

IMPORTANT:
- For user preferences and configuration: Extract preferences about tools, databases, workflows
- For technical solutions: Extract HOW to implement solutions (methods, functions, parameters, patterns), NOT descriptions of what the user asked
- Every fact should help answer "How do I solve a similar problem?" not just "What did the user ask?"

Types of Information to Remember:

1. Personal Preferences and Configuration:
   - Preferred databases (Materials Project, AFLOW, OQMD, etc.)
   - API key locations and access methods (e.g., os.getenv('MP_API_KEY'))
   - Favorite tools and libraries for a specific task (pymatgen, ASE, VASP, etc.)
   - Research focus areas and materials of interest

2. Technical Implementation Details:
   - Specific API methods and functions
   - Function calls and their parameters
   - Data retrieval patterns and workflows
   - Field names and object attributes accessed
   - Computational methods and their parameters
   - Code patterns that successfully solved problems

3. Scientific Knowledge:
   - Material properties and characteristics
   - Crystal structures and space groups
   - Computational methods and techniques
   - Analysis procedures and best practices

4. API and Library Usage:
   - Specific methods and their purposes
   - Required parameters and field names
   - Object attributes and how to access them
   - Common usage patterns
   - Library versions and compatibility notes

5. Project Context:
   - Research goals and objectives
   - Current projects and their requirements
   - Collaboration details and data sources
   - Important dates and milestones

6. Other Information:
   - Common errors and troubleshooting solutions
   - Performance tips and optimizations
   - Any other technical details that help solve similar problems

Few-Shot Examples:

Input: Hi.
Output: {{"facts": []}}

Input: I prefer using the Materials Project API for crystal structure data.
Output: {{"facts": ["Prefers Materials Project API for crystal structure data"]}}

Input: My MP API key is stored in the MP_API_KEY environment variable.
Output: {{"facts": ["MP API key is accessed via os.getenv('MP_API_KEY')"]}}

Input: User asked: How to get formation energy? Solution: Use MPRester and call mpr.materials.summary.search() with fields=['formation_energy_per_atom'], then access via docs[0].formation_energy_per_atom.
Output: {{"facts": ["Use mpr.materials.summary.search() to retrieve formation energy", "Pass fields=['formation_energy_per_atom'] to materials.summary.search()", "Access formation energy via docs[0].formation_energy_per_atom"]}}

Input: I'm working with Silicon for my semiconductor research and prefer pymatgen for structure analysis.
Output: {{"facts": ["Works with Silicon for research", "Research focus is semiconductors", "Prefers pymatgen for structure analysis"]}}

Return the facts in JSON format as shown above.

Guidelines:
- Today's date is {datetime.now().strftime("%Y-%m-%d")}.
- Purpose: Extract information that helps solve similar problems in the future.
- For user preferences: Extract preferences about tools, databases, materials, workflows.
- For technical solutions: Extract implementation details - specific methods, functions, parameters, code patterns.
- When code/solutions are provided, extract HOW it works: API methods, function calls, parameters, field access patterns.
- Focus on actionable technical information that enables solving similar problems.
- If no relevant information is found, return an empty list for the "facts" key.
- Detect the language of user input and record facts in the same language. Normally, the user input is in English.
- The response must be valid JSON with a "facts" key containing a list of strings.

Following is a conversation between the user and the assistant. Extract relevant facts about user preferences, API keys, technical implementation details, API methods, code patterns, and scientific knowledge from the conversation and return them in JSON format as shown above.
"""

# Graph Memory Custom Prompt (for entity and relationship extraction)
MATERIALS_SCIENCE_GRAPH_PROMPT = """Focus on extracting technical implementation details and relationships in materials science code and research context. The purpose is to help solve similar problems in the future by capturing HOW solutions are implemented.

IMPORTANT:
- Extract specific API methods, function calls, and technical components from code
- Focus on implementation details that show how to solve problems, not just what was asked
- Capture relationships between methods, parameters, and data fields

Example Entity Types (adapt based on actual content):
   - Materials: Chemical elements, compounds, alloys, material IDs
   - Properties: Material properties
   - Tools/Libraries: Software and libraries
   - API Methods: Specific API methods and functions
   - Functions: Function calls and methods
   - Parameters: Function parameters and field names
   - Data Fields: Object attributes and accessed fields
   - Databases: Data sources (Materials Project, AFLOW, OQMD, etc.)
   - API Keys: Configuration methods
   - Units: Measurement units

Relationship Guidelines:
   - Extract relationships that help solve similar problems: both user preferences AND technical implementation
   - For user preferences: Capture tool preferences, database choices, workflow patterns (e.g., "user â†’ prefers â†’ Materials_Project")
   - For technical implementation: Show HOW code works through method calls, parameter passing, field access
   - Prefer specific technical relationships (e.g., "calls_method" over generic "uses")
   - Include relationships between methods and the data they retrieve
   - Include relationships between parameters and functions that accept them
   - Focus on actionable relationships that enable future problem-solving
"""

# ============================================================================
# MEMORY CONFIGURATION
# ============================================================================
# MEM0_USE_LLM: Whether to use LLM for intelligent memory extraction (default: true)
#   - true: LLM extracts key facts + saves original content (two saves)
#   - false: Only saves original content directly (one save, no LLM needed)
#
# MEM0_LLM_MODEL: Which model to use for memory extraction (default: gpt-4o-mini)
#
# ENABLE_GRAPH_MEMORY: Whether to use Neo4j graph for entity relationships (default: true)
# ============================================================================

MEM0_USE_LLM = os.getenv('MEM0_USE_LLM', 'true').lower() == 'true'
MEM0_LLM_MODEL = os.getenv('MEM0_LLM_MODEL', 'gpt-4o-mini')
ENABLE_GRAPH_MEMORY = os.getenv('ENABLE_GRAPH_MEMORY', 'true').lower() == 'true'


# Memory Server Context
@dataclass
class MemoryContext:
    """Manages mem0 instance for memory operations."""
    mem0: Any
    enable_graph: bool
    use_llm: bool


@asynccontextmanager
async def memory_lifespan(server: FastMCP) -> AsyncIterator[MemoryContext]:
    """Initialize and manage the mem0 memory instance."""
    from mem0 import Memory

    # Build configuration (same as memory_tools.py)
    config_dict = {
        "vector_store": {
            "provider": "supabase",
            "config": {
                "connection_string": os.getenv('SUPABASE_DATABASE_URL'),
                "collection_name": "conversational_memories"
            }
        },
    }

    # Add LLM config only if enabled
    if MEM0_USE_LLM:
        config_dict["llm"] = {
            "provider": "openai",
            "config": {
                "model": MEM0_LLM_MODEL
            }
        }
        config_dict["custom_fact_extraction_prompt"] = MATERIALS_SCIENCE_EXTRACTION_PROMPT
        logger.info(f"Memory LLM ENABLED - Using {MEM0_LLM_MODEL} for intelligent extraction")
    else:
        logger.info("Memory LLM DISABLED - Direct storage only (no extraction)")

    # Add graph store if enabled
    if ENABLE_GRAPH_MEMORY:
        config_dict["graph_store"] = {
            "provider": "neo4j",
            "config": {
                "url": os.getenv('NEO4J_URI', 'bolt://localhost:7687'),
                "username": os.getenv('NEO4J_USER', 'neo4j'),
                "password": os.getenv('NEO4J_PASSWORD', 'password')
            },
            "custom_prompt": MATERIALS_SCIENCE_GRAPH_PROMPT
        }
        logger.info("Graph memory ENABLED - Entity relationships will be tracked")
    else:
        logger.info("Graph memory DISABLED - Using vector-based memory only")

    # Create mem0 instance
    mem0_instance = Memory.from_config(config_dict)

    try:
        yield MemoryContext(
            mem0=mem0_instance,
            enable_graph=ENABLE_GRAPH_MEMORY,
            use_llm=MEM0_USE_LLM
        )
    finally:
        logger.info("Memory Server shutting down...")


# Memory Server - FastMCP Application
mcp = FastMCP(
    "mcp_servers_and_tools/memory_server",
    lifespan=memory_lifespan,
    host=os.getenv("HOST", "127.0.0.1"),
    port=int(os.getenv("PORT", "8053"))
)


@mcp.tool()
async def search_memory(ctx: Context, query: str, user_id: str) -> str:
    """
    Search through stored memories

    Returns relevant memories that may help solve the current problem
    Use this to retrieve user preferences, API keys, similar problems, solutions, or relevant experience from past memories

    Args:
        ctx: MCP context
        query: What to search for
        user_id: User identifier extracted from message

    Returns:
        Formatted string with relevant memories (from vector store) and entity relationships (from graph store)
    """
    try:
        mem0 = ctx.request_context.lifespan_context.mem0
        enable_graph = ctx.request_context.lifespan_context.enable_graph

        search_result = mem0.search(query, user_id=user_id, limit=5)

        output_parts = []

        # Add vector store results
        if search_result and search_result.get('results'):
            vector_results = search_result['results']
            output_parts.append("Relevant memories:")
            for mem in vector_results:
                output_parts.append(f"- {mem['memory']}")

        # Add graph store results (entity relationships)
        if enable_graph and search_result and search_result.get('relations'):
            relations = search_result['relations'][:5]  # Limit to top 5 relations
            if relations:
                output_parts.append("\nRelated entity relationships:")
                for rel in relations:
                    source = rel.get('source', 'unknown')
                    relationship = rel.get('relationship', 'relates_to')
                    destination = rel.get('destination', 'unknown')
                    output_parts.append(f"- {source} â†’ {relationship} â†’ {destination}")

        if output_parts:
            return "\n".join(output_parts)

        return "No relevant memories found."
    except Exception as e:
        logger.error(f"Error searching memory: {e}")
        return "Error searching memories."


@mcp.tool()
async def save_to_memory(ctx: Context, content: str, user_id: str) -> str:
    """
    Save important information to memory

    IMPORTANT:
    - For solutions: Only call this AFTER the user confirms they are satisfied
    - For preferences/configuration: Can be called anytime
    - Do NOT save failed solutions or solutions the user is not happy with

    Behavior depends on MEM0_USE_LLM environment variable:
    - If MEM0_USE_LLM=true: Saves both LLM-extracted facts AND complete original content
    - If MEM0_USE_LLM=false: Only saves complete original content (no LLM processing)

    This builds a knowledge base for future reference

    Args:
        ctx: MCP context
        content: Content to save (e.g., user's query, solution code, explanation,
                 user preferences, API keys, how to access API keys, etc.)
        user_id: User identifier extracted from message

    Returns:
        Status message
    """
    try:
        mem0 = ctx.request_context.lifespan_context.mem0
        use_llm = ctx.request_context.lifespan_context.use_llm

        messages = [{"role": "user", "content": content}]

        if use_llm:
            # Save 1: LLM extracts key facts (vector + graph memory with custom prompts)
            mem0.add(messages, user_id=user_id, infer=True)

            # Save 2: Store complete original content as-is (vector only, no LLM processing)
            # Temporarily disable graph to avoid duplicate graph updates for same content
            original_enable_graph = mem0.enable_graph
            mem0.enable_graph = False
            mem0.add(messages, user_id=user_id, infer=False)
            mem0.enable_graph = original_enable_graph

            return "Information saved to memory successfully (facts + complete content)."
        else:
            # No LLM: Only save complete original content directly
            mem0.add(messages, user_id=user_id, infer=False)
            return "Information saved to memory successfully (direct storage)."
    except Exception as e:
        logger.error(f"Error saving to memory: {e}")
        return f"Error saving to memory: {str(e)}"


if __name__ == "__main__":
    mcp.run()

```


================================================================================
=== FILE: mcp_servers_and_tools\research_server\extracted_code.sql ===
================================================================================

```sql
-- Enable pgvector extension for vector operations
-- This must be run first before creating tables with vector columns
CREATE EXTENSION IF NOT EXISTS vector;

-- Create extracted_code table for individual code blocks with separate embeddings
CREATE TABLE IF NOT EXISTS extracted_code (
    id BIGSERIAL PRIMARY KEY,
    url TEXT NOT NULL,
    code_text TEXT NOT NULL,
    summary TEXT NOT NULL,
    context_before TEXT,
    context_after TEXT,
    code_type TEXT,
    language TEXT,
    index INTEGER NOT NULL,
    extraction_method TEXT NOT NULL DEFAULT 'single_page',
    embedding vector(1536),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Create indexes for efficient querying
CREATE INDEX IF NOT EXISTS idx_extracted_code_url ON extracted_code(url);
CREATE INDEX IF NOT EXISTS idx_extracted_code_type ON extracted_code(code_type);
CREATE INDEX IF NOT EXISTS idx_extracted_code_language ON extracted_code(language);
CREATE INDEX IF NOT EXISTS idx_extracted_code_embedding ON extracted_code USING ivfflat (embedding vector_cosine_ops);
CREATE INDEX IF NOT EXISTS idx_extracted_code_created_at ON extracted_code(created_at);

-- Create unique constraint to prevent duplicate code blocks from same URL
CREATE UNIQUE INDEX IF NOT EXISTS idx_extracted_code_url_index ON extracted_code(url, index);

DROP FUNCTION IF EXISTS match_code_blocks(vector(1536), integer, jsonb);

-- Create function for semantic search of code blocks
CREATE OR REPLACE FUNCTION match_code_blocks(
    query_embedding vector(1536),
    match_count int DEFAULT 10,
    filter_metadata jsonb DEFAULT '{}'
)
RETURNS TABLE (
    id bigint,
    url text,
    code_text text,
    summary text,
    context_before text,
    context_after text,
    code_type text,
    language text,
    index int,
    extraction_method text,
    similarity float
)
LANGUAGE plpgsql
AS $$
BEGIN
    RETURN QUERY
    SELECT 
        ec.id,
        ec.url,
        ec.code_text,
        ec.summary,
        ec.context_before,
        ec.context_after,
        ec.code_type,
        ec.language,
        ec.index,
        ec.extraction_method,
        1 - (ec.embedding <=> query_embedding) as similarity
    FROM extracted_code ec
    WHERE 
        (filter_metadata->>'language' IS NULL OR ec.language = filter_metadata->>'language')
        AND (filter_metadata->>'code_type' IS NULL OR ec.code_type = filter_metadata->>'code_type')
        AND (filter_metadata->>'url' IS NULL OR ec.url = filter_metadata->>'url')
    ORDER BY ec.embedding <=> query_embedding
    LIMIT match_count;
END;
$$;

-- Add comments to columns
COMMENT ON COLUMN extracted_code.url IS 'The URL from which the code block was extracted';
COMMENT ON COLUMN extracted_code.code_text IS 'The actual code text content';
COMMENT ON COLUMN extracted_code.summary IS 'AI-generated summary of the code block';
COMMENT ON COLUMN extracted_code.context_before IS 'Text context before the code block';
COMMENT ON COLUMN extracted_code.context_after IS 'Text context after the code block';
COMMENT ON COLUMN extracted_code.code_type IS 'Type of code block (markdown_code_block, command_example, etc.)';
COMMENT ON COLUMN extracted_code.language IS 'Programming language of the code block';
COMMENT ON COLUMN extracted_code.index IS 'Index/position of the code block within the URL';
COMMENT ON COLUMN extracted_code.extraction_method IS 'Method used for extraction (single_page, smart_crawl, etc.)';
COMMENT ON COLUMN extracted_code.embedding IS 'Vector embedding of the code block (code + summary + context) for semantic search';
COMMENT ON COLUMN extracted_code.created_at IS 'Timestamp when the record was created'; 
```


================================================================================
=== FILE: mcp_servers_and_tools\research_server\pyproject.toml ===
================================================================================

```toml
[project]
name = "research-server"
version = "0.1.0"
description = "Research Server - Specialized MCP server for comprehensive code research, analysis, and discovery workflows"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "crawl4ai>=0.7.3",
    "mcp>=1.7.1",
    "supabase>=2.18.1",
    "openai>=1.81.0",
    "python-dotenv>=1.0.1",
    "neo4j>=5.28.2",
]

```


================================================================================
=== FILE: mcp_servers_and_tools\research_server\README.md ===
================================================================================

```markdown
# Research Server

A specialized MCP server for code research and analysis workflows. Provides tools for extracting, analyzing, and discovering code patterns across repositories and documentation.

## Features

- **Code Extraction**: Extract code blocks from URLs and store in database
- **Semantic Search**: Retrieve code examples using natural language queries
- **Quick Introspection**: Inspect packages, classes, methods, and functions
- **Runtime Probing**: Probe code snippets for errors
- **Knowledge Graph**: Parse local packages and query code patterns (optional)

## Tools

| Tool | Description |
|------|-------------|
| `extract_code_from_url` | Extract code blocks from URLs and store in database |
| `retrieve_extracted_code` | Search and retrieve extracted code examples |
| `quick_introspect` | Quick introspection of packages, classes, methods, and functions |
| `runtime_probe_snippet` | Runtime probing of code snippets for KeyError and AttributeError |
| `parse_local_package` | Parse and analyze local Python packages (requires USE_KNOWLEDGE_GRAPH=true) |
| `query_knowledge_graph` | Query Neo4j knowledge graph for code patterns (requires USE_KNOWLEDGE_GRAPH=true) |

## Installation

```bash
cd research_server
pip install -e .
```

## Configuration

Copy `.env.example` to `.env` and configure:

```bash
cp .env.example .env
```

## Running the Server

```bash
# Using Python directly
python src/research_mcp.py

# Or using uv
uv run src/research_mcp.py
```

## Database Setup

### Supabase

1. Go to [supabase.com](https://supabase.com) and create a new project
2. In your project dashboard, go to **Project Settings**:
   - **Data API** â†’ Copy **Project URL** â†’ `SUPABASE_URL`
   - **API Keys** â†’ Click **Legacy anon, service_role API keys** â†’ Click **Reveal** on service_role â†’ `SUPABASE_SERVICE_KEY`
3. Go to **SQL Editor** and run the schema from `extracted_code.sql`

### Neo4j (Optional)

Required only if `USE_KNOWLEDGE_GRAPH=true`:

1. Install Neo4j from [neo4j.com/download](https://neo4j.com/download/)
2. Start the database and set a password
3. Configure `NEO4J_URI`, `NEO4J_USER`, and `NEO4J_PASSWORD`

```


================================================================================
=== FILE: mcp_servers_and_tools\research_server\test_research_server_extract_code_from_url_and_retrieve.py ===
================================================================================

```python
#!/usr/bin/env python3
"""
Test for extract_code_from_url MCP tool to extract code from web URLs.
"""
import asyncio
import json
import subprocess
from pathlib import Path

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client


def get_server_params() -> StdioServerParameters:
    # Provide minimal env to allow server startup
    import os
    return StdioServerParameters(
        command="python3",
        args=["-u", "src/research_mcp.py"],
        cwd=str(Path(__file__).parent),
        env={
            **os.environ,  # Pass all current environment variables
            "SUPABASE_URL": os.environ.get("SUPABASE_URL", "dummy"),
            "SUPABASE_SERVICE_KEY": os.environ.get("SUPABASE_SERVICE_KEY", "dummy"),
            "NEO4J_URI": os.environ.get("NEO4J_URI"),
            "NEO4J_USER": os.environ.get("NEO4J_USER"),
            "NEO4J_PASSWORD": os.environ.get("NEO4J_PASSWORD"),
            "USE_KNOWLEDGE_GRAPH": "true",
            "GENERATE_CODE_SUMMARY": "false",
            "OPENAI_EMBEDDING_MODEL": "text-embedding-3-small",
            "OPENAI_API_KEY": os.environ.get("OPENAI_API_KEY", "dummy"),
            "MCP_QUIET": "0",  # Changed to 0 for more verbose output
            "NODE_ENV": "development",  # Changed to development for more debug info
            "TRANSPORT": "stdio"  # Use stdio transport for MCP client
        }
    )


async def clear_supabase_tables(session):
    """Clear extracted_code table in Supabase before testing"""
    print("\nðŸ§¹ Clearing extracted_code table in Supabase...")
    
    try:
        # Import the supabase utils to clear tables directly
        import sys
        import os
        from pathlib import Path
        
        # Add the utils directory to Python path
        current_dir = Path(__file__).parent
        utils_path = current_dir.parent.parent / "utils"  # Go up to CASCADE root, then into utils
        sys.path.insert(0, str(utils_path))
        
        from supabase_utils import clear_supabase_tables as clear_tables
        
        # Get Supabase client
        import os
        from supabase import create_client
        
        url = os.environ.get("SUPABASE_URL")
        key = os.environ.get("SUPABASE_SERVICE_KEY")
        
        if url and key:
            supabase_client = create_client(url, key)
            clear_tables(supabase_client)
            print("âœ… Successfully cleared extracted_code table")
        else:
            print("âš ï¸ Missing Supabase credentials (continuing without clearing)")
            
    except Exception as e:
        print(f"âš ï¸ Clear tables skipped: {e}")


async def test_retrieve_extracted_code(session, query: str, description: str):
    """Test retrieve_extracted_code MCP tool with a query"""
    print(f"\n{'='*60}")
    print(f"ðŸ” Testing Retrieve Extracted Code: {description}")
    print(f"ðŸ” Query: {query}")
    print(f"{'='*60}")
    
    try:
        # Call the MCP tool with a query
        print("\nðŸ“¤ Calling retrieve_extracted_code MCP tool...")
        result = await session.call_tool("retrieve_extracted_code", {
            "query": query
        })
        
        if result.content:
            text = result.content[0].text
            print("\nðŸ“¥ MCP tool response received")
            
            # Parse JSON response
            try:
                data = json.loads(text)
            except json.JSONDecodeError as e:
                print(f"\nâŒ JSON Parse Error: {e}")
                print("\nðŸ“„ Raw response text:")
                print("-"*40)
                print(text[:1000])  # Print first 1000 chars
                print("-"*40)
                return
            
            # Print retrieval results
            print("\nðŸ“Š Retrieval Results:")
            print("-"*40)
            
            # Basic info
            success = data.get('success', False)
            error = data.get('error', '')
            
            print(f"âœ“ Success: {success}")
            
            if not success:
                print(f"âœ— Error: {error}")
                return
            
            # Successful retrieval info
            print(f"âœ“ Query: {data.get('query', query)}")
            
            # Print retrieved code blocks
            if data.get('results'):
                results = data['results']
                print(f"\nðŸ“¦ Retrieved {len(results)} code blocks:")
                print("="*60)
                
                for i, result_item in enumerate(results):
                    print(f"\n{'='*60}")
                    print(f"ðŸ“Œ Result #{i+1}/{len(results)}")
                    print(f"{'='*60}")
                    
                    # Print result details
                    print(f"âœ“ Source URL: {result_item.get('source_url', 'unknown')}")
                    print(f"âœ“ Language: {result_item.get('language', 'unknown')}")
                    print(f"âœ“ Type: {result_item.get('type', 'unknown')}")
                    print(f"âœ“ Similarity Score: {result_item.get('similarity_score', 'N/A')}")
                    print(f"âœ“ Code Length: {len(result_item.get('code', ''))} chars")
                    
                    # Print the code content
                    code_content = result_item.get('code', '')
                    if code_content:
                        print(f"\nðŸ“ Code Content:")
                        print("-"*40)
                        print(code_content[:500])  # Show first 500 chars
                        if len(code_content) > 500:
                            print("...")
                        print("-"*40)
                    
                    # Print context if available
                    context_before = result_item.get('context_before', '')
                    context_after = result_item.get('context_after', '')
                    if context_before or context_after:
                        print(f"\nðŸ“‹ Context:")
                        if context_before:
                            print(f"Before: {context_before[:200]}...")
                        if context_after:
                            print(f"After: {context_after[:200]}...")
                
                print(f"\n{'='*60}")
                print(f"âœ… Total results retrieved: {len(results)}")
                
                # Summary statistics
                total_chars = sum(len(result_item.get('code', '')) for result_item in results)
                languages = {}
                for result_item in results:
                    lang = result_item.get('language', 'unknown')
                    languages[lang] = languages.get(lang, 0) + 1
                
                print(f"\nðŸ“Š Retrieval Statistics:")
                print(f"  â€¢ Total code characters: {total_chars:,}")
                print(f"  â€¢ Language distribution:")
                for lang, count in sorted(languages.items(), key=lambda x: x[1], reverse=True):
                    print(f"    - {lang}: {count} results")
                
            else:
                print("\nâš ï¸ No results found for the query")
            
            # Save complete response to file for detailed inspection
            output_file = f"code_retrieval_{description.replace(' ', '_').replace('-', '_')}.json"
            with open(output_file, 'w') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            print(f"\nðŸ’¾ Full results saved to: {output_file}")
            
            print("\n" + "="*60)
            
        else:
            print("âŒ No content returned from MCP tool")
            
    except Exception as e:
        print(f"\nâŒ MCP tool call failed with exception:")
        print(f"  Type: {type(e).__name__}")
        print(f"  Message: {str(e)}")
        
        # Print full traceback
        import traceback
        print("\nðŸ“‹ Full Traceback:")
        print("-"*40)
        traceback.print_exc()
        print("-"*40)


async def test_single_url_debug(session, url: str, description: str):
    """Test extract_code_from_url MCP tool with a single URL for debugging"""
    print(f"\n{'='*60}")
    print(f"ðŸ§ª Testing Single URL: {description}")
    print(f"ðŸ”— URL: {url}")
    print(f"{'='*60}")
    
    try:
        # Call the MCP tool with a single URL
        print("\nðŸ“¤ Calling MCP tool with single URL...")
        result = await session.call_tool("extract_code_from_url", {
            "url": url
        })
        
        if result.content:
            text = result.content[0].text
            print("\nðŸ“¥ MCP tool response received")
            
            # Parse JSON response
            try:
                data = json.loads(text)
            except json.JSONDecodeError as e:
                print(f"\nâŒ JSON Parse Error: {e}")
                print("\nðŸ“„ Raw response text:")
                print("-"*40)
                print(text[:1000])  # Print first 1000 chars
                print("-"*40)
                return
            
            # Print extraction results
            print("\nðŸ“Š Extraction Results:")
            print("-"*40)
            
            # Basic info
            success = data.get('success', False)
            error = data.get('error', '')
            
            print(f"âœ“ Success: {success}")
            
            if not success:
                print(f"âœ— Error: {error}")
                print(f"âœ— Error Type: {data.get('error_type', 'unknown')}")
                
                # Print any additional error context
                if 'traceback' in data:
                    print("\nðŸ“‹ Traceback:")
                    print(data['traceback'])
                    
                # Check if it's a cache result
                if data.get('cached'):
                    print("ðŸ“¦ This was a cached result")
                    
                return
            
            # Successful extraction info
            print(f"âœ“ Crawl Method: {data.get('crawl_method', 'unknown')}")
            print(f"âœ“ Extraction Method: {data.get('extraction_method', 'unknown')}")
            print(f"âœ“ Doc System: {data.get('doc_system', 'unknown')}")
            print(f"âœ“ Content Length: {data.get('content_length', 0):,} chars")
            print(f"âœ“ Code Blocks Found: {data.get('code_blocks_found', 0)}")
            print(f"âœ“ Cached: {data.get('cached', False)}")
            
            # Print complete JSON for each code block
            if data.get('extracted_code'):
                code_blocks = data['extracted_code']
                print(f"\nðŸ“¦ Complete JSON for all {len(code_blocks)} code blocks:")
                print("="*60)
                
                for i, block in enumerate(code_blocks):
                    print(f"\n{'='*60}")
                    print(f"ðŸ“Œ Code Block #{i+1}/{len(code_blocks)}")
                    print(f"{'='*60}")
                    
                    # Print the complete JSON for this block with nice formatting
                    print(json.dumps(block, indent=2, ensure_ascii=False))
                
                print(f"\n{'='*60}")
                print(f"âœ… Total blocks extracted: {len(code_blocks)}")
                
                # Summary statistics
                total_chars = sum(len(block.get('code', '')) for block in code_blocks)
                total_lines = sum(len(block.get('code', '').split('\n')) for block in code_blocks)
                languages = {}
                for block in code_blocks:
                    lang = block.get('language', 'unknown')
                    languages[lang] = languages.get(lang, 0) + 1
                
                print(f"\nðŸ“Š Overall Statistics:")
                print(f"  â€¢ Total code characters: {total_chars:,}")
                print(f"  â€¢ Total code lines: {total_lines:,}")
                print(f"  â€¢ Language distribution:")
                for lang, count in sorted(languages.items(), key=lambda x: x[1], reverse=True):
                    print(f"    - {lang}: {count} blocks")
                
            else:
                print("\nâš ï¸ No code blocks extracted")
                if 'message' in data:
                    print(f"  Message: {data['message']}")
            
            # Save complete response to file for detailed inspection
            output_file = f"code_extraction_single_{description.replace(' ', '_').replace('-', '_')}.json"
            with open(output_file, 'w') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            print(f"\nðŸ’¾ Full results saved to: {output_file}")
            
            # Also save just the code blocks for easier access
            if data.get('extracted_code'):
                blocks_file = f"code_blocks_single_{description.replace(' ', '_').replace('-', '_')}.json"
                with open(blocks_file, 'w') as f:
                    json.dump(data['extracted_code'], f, indent=2, ensure_ascii=False)
                print(f"ðŸ’¾ Code blocks only saved to: {blocks_file}")
            
            print("\n" + "="*60)
            
        else:
            print("âŒ No content returned from MCP tool")
            
    except Exception as e:
        print(f"\nâŒ MCP tool call failed with exception:")
        print(f"  Type: {type(e).__name__}")
        print(f"  Message: {str(e)}")
        
        # Print full traceback
        import traceback
        print("\nðŸ“‹ Full Traceback:")
        print("-"*40)
        traceback.print_exc()
        print("-"*40)


async def test_url_list_debug(session, urls: list, description: str):
    """Test extract_code_from_url MCP tool with a list of URLs for debugging"""
    print(f"\n{'='*60}")
    print(f"ðŸ§ª Testing URL List: {description}")
    print(f"ðŸ”— URLs: {len(urls)} URLs")
    for i, url in enumerate(urls, 1):
        print(f"  {i}. {url}")
    print(f"{'='*60}")
    
    try:
        # Call the MCP tool with a list of URLs
        print("\nðŸ“¤ Calling MCP tool with URL list...")
        result = await session.call_tool("extract_code_from_url", {
            "urls": urls
        })
        
        if result.content:
            text = result.content[0].text
            print("\nðŸ“¥ MCP tool response received")
            
            # Parse JSON response
            try:
                data = json.loads(text)
            except json.JSONDecodeError as e:
                print(f"\nâŒ JSON Parse Error: {e}")
                print("\nðŸ“„ Raw response text:")
                print("-"*40)
                print(text[:1000])  # Print first 1000 chars
                print("-"*40)
                return
            
            # Print extraction results
            print("\nðŸ“Š Extraction Results:")
            print("-"*40)
            
            # Check if this is a URL list response (multiple URLs)
            if 'per_url_results' in data:
                print(f"âœ“ Processing URL list response with {len(data['per_url_results'])} URLs")
                
                total_blocks = 0
                successful_urls = 0
                per_url_results = data['per_url_results']
                
                for i, url_result in enumerate(per_url_results):
                    print(f"\n{'='*40}")
                    print(f"ðŸ“Œ URL Result #{i+1}/{len(per_url_results)}")
                    print(f"{'='*40}")
                    
                    success = url_result.get('success', False)
                    url = url_result.get('url', f'URL_{i+1}')
                    code_blocks_found = url_result.get('code_blocks_found', 0)
                    
                    print(f"âœ“ URL: {url}")
                    print(f"âœ“ Success: {success}")
                    print(f"âœ“ Code Blocks Found: {code_blocks_found}")
                    
                    if success:
                        successful_urls += 1
                        total_blocks += code_blocks_found
                        
                        # Print extraction method info
                        print(f"âœ“ Crawl Method: {url_result.get('crawl_method', 'unknown')}")
                        print(f"âœ“ Extraction Method: {url_result.get('extraction_method', 'unknown')}")
                        print(f"âœ“ Doc System: {url_result.get('doc_system', 'unknown')}")
                        print(f"âœ“ Cached: {url_result.get('cached', False)}")
                        
                        # Show first few code blocks
                        if url_result.get('extracted_code'):
                            code_blocks = url_result['extracted_code']
                            print(f"\nðŸ“¦ First 3 code blocks (out of {len(code_blocks)}):")
                            
                            for j, block in enumerate(code_blocks[:3]):
                                print(f"\n  ðŸ“Œ Block {j+1}:")
                                print(f"    Language: {block.get('language', 'unknown')}")
                                print(f"    Type: {block.get('type', 'unknown')}")
                                print(f"    Length: {len(block.get('code', ''))} chars")
                                print(f"    Preview: {block.get('code', '')[:100]}...")
                    else:
                        error = url_result.get('error', 'Unknown error')
                        print(f"âœ— Error: {error}")
                
                # Print overall summary
                summary = data.get('summary', {})
                print(f"\n{'='*60}")
                print(f"ðŸ“Š Overall Summary:")
                print(f"  â€¢ Total URLs processed: {summary.get('total_unique_urls', len(per_url_results))}")
                print(f"  â€¢ Successful extractions: {successful_urls}")
                print(f"  â€¢ Total code blocks extracted: {data.get('total_code_blocks_found', total_blocks)}")
                print(f"  â€¢ Cached URLs: {summary.get('cached_urls', 0)}")
                print(f"  â€¢ Newly extracted URLs: {summary.get('newly_extracted_urls', 0)}")
                print(f"  â€¢ Success rate: {successful_urls/len(per_url_results)*100:.1f}%")
                
            else:
                # Single URL result
                print("âœ“ Processing single URL result")
                success = data.get('success', False)
                print(f"âœ“ Success: {success}")
                
                if success:
                    code_blocks = data.get('extracted_code', [])
                    print(f"âœ“ Code Blocks Found: {len(code_blocks)}")
                    print(f"âœ“ Crawl Method: {data.get('crawl_method', 'unknown')}")
                    print(f"âœ“ Extraction Method: {data.get('extraction_method', 'unknown')}")
                    print(f"âœ“ Doc System: {data.get('doc_system', 'unknown')}")
                    print(f"âœ“ Cached: {data.get('cached', False)}")
                else:
                    error = data.get('error', 'Unknown error')
                    print(f"âœ— Error: {error}")
            
            # Save complete response to file for detailed inspection
            output_file = f"code_extraction_list_{description.replace(' ', '_').replace('-', '_')}.json"
            with open(output_file, 'w') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            print(f"\nðŸ’¾ Full results saved to: {output_file}")
            
            print("\n" + "="*60)
            
        else:
            print("âŒ No content returned from MCP tool")
            
    except Exception as e:
        print(f"\nâŒ MCP tool call failed with exception:")
        print(f"  Type: {type(e).__name__}")
        print(f"  Message: {str(e)}")
        
        # Print full traceback
        import traceback
        print("\nðŸ“‹ Full Traceback:")
        print("-"*40)
        traceback.print_exc()
        print("-"*40)


async def test_extract_code_debug():
    """Test extract_code_from_url MCP tool with debug output"""
    print("\nðŸš€ Starting extract_code_from_url MCP tool test...")
    
    # Test URLs
    test_urls = [
        "https://xtb-docs.readthedocs.io/en/latest/hessian.html"
    ]
    
    server_params = get_server_params()
    
    print("\nðŸ“¡ Starting MCP server...")
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            print("âœ… MCP session initialized")
            await session.initialize()

            # Clear tables first
            await clear_supabase_tables(session)
            
            # Test 1: Extract code from URL
            await test_url_list_debug(session, [test_urls[0]], "")
            # await test_url_list_debug(session, test_urls, "XTB + ... Combined")
           
            # Test 2: Retrieve extracted code with query
            await test_retrieve_extracted_code(session, "single point hessian", "XTB SPH Query")
            
            print("\n" + "="*60)
            print("âœ… All tests completed")
            print("="*60)


async def main() -> None:
    print("=" * 60)
    print("ðŸ”¬ Extract Code from URL and Retrieve - Debug Test")
    print("=" * 60)
    
    try:
        await test_extract_code_debug()
    except KeyboardInterrupt:
        print("\nâš ï¸ Test interrupted by user")
    except Exception as e:
        print(f"\nâŒ Test failed with unexpected error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
```


================================================================================
=== FILE: mcp_servers_and_tools\research_server\test_research_server_introspect.py ===
================================================================================

```python
#!/usr/bin/env python3
"""
Test research-server quick_introspect tool

This script will:
1. Connect to MCP server
2. Call quick_introspect with provided code content
"""

import asyncio
import json
import subprocess
import sys
import os
from pathlib import Path
from typing import Dict, Any

# MCP client related imports
try:
    from mcp import ClientSession, StdioServerParameters
    from mcp.client.stdio import stdio_client
    from mcp.types import TextContent
except ImportError:
    print("âŒ Need to install MCP client library")
    print("Please run: pip install mcp")
    sys.exit(1)

async def test_quick_introspect():
    """Test quick_introspect tool with provided code content"""
    
    print("ðŸš€ Starting quick_introspect test...")
    
    # Code content to test
    code_content = """import os
from mp_api import MPRester
from pymatgen.computed_entries import Computedentries
from pymatgen.core import composition"""
    
    print("\n" + "="*60)
    print("1. Connect to MCP server and test quick_introspect")
    print("="*60)
    
    try:
        # Set server parameters
        server_params = StdioServerParameters(
            command="python",
            args=["src/research_mcp.py"],
            cwd=str(Path(__file__).parent),
            env={
                **os.environ,  # Pass all current environment variables
                "SUPABASE_URL": os.environ.get("SUPABASE_URL"),
                "SUPABASE_SERVICE_KEY": os.environ.get("SUPABASE_SERVICE_KEY"),
                "NEO4J_URI": os.environ.get("NEO4J_URI"),
                "NEO4J_USER": os.environ.get("NEO4J_USER"),
                "NEO4J_PASSWORD": os.environ.get("NEO4J_PASSWORD"),
                "USE_KNOWLEDGE_GRAPH": "true",
                "GENERATE_CODE_SUMMARY": "true",
                "TRANSPORT": "stdio"
            }
        )
        
        async with stdio_client(server_params) as (read, write):
            async with ClientSession(read, write) as session:
                print("âœ“ Successfully connected to MCP server")
                
                # Get server information (optional)
                print("\nðŸ“‹ Getting server information...")
                try:
                    info = await session.initialize()
                    server_name = getattr(info, 'server_name', 'Unknown')
                    server_version = getattr(info, 'server_version', 'Unknown')
                    print(f"Server name: {server_name}")
                    print(f"Server version: {server_version}")
                except Exception as e:
                    print(f"âš ï¸  Failed to get server information: {e}")

                # Test 1: quick_introspect tool with code_content
                print("\nðŸ§  Test 1: quick_introspect with code_content...")
                print(f"Code content to analyze:")
                print("-" * 40)
                print(code_content)
                print("-" * 40)
                
                try:
                    qi_inputs = {
                        "code_content": code_content,
                        "repo_hint": "mp_api",
                        "max_suggestions": 3,
                        "method_hint": "chemsys",
                        "no_imports": False,
                    }
                    qi_result = await session.call_tool("quick_introspect", qi_inputs)
                    if qi_result.content:
                        try:
                            qi_data = json.loads(qi_result.content[0].text)
                            print("âœ“ quick_introspect result:")
                            print(f"  Success: {qi_data.get('success')}")
                            report = qi_data.get("report", "")
                            print("  Report:")
                            print(report)
                        except json.JSONDecodeError:
                            print("  Raw quick_introspect result:")
                            print(qi_result.content[0].text)
                    else:
                        print("  âš ï¸  No return result from quick_introspect")
                except Exception as e:
                    print(f"  âŒ quick_introspect failed: {e}")
                    import traceback
                    traceback.print_exc()

                # Test 2: quick_introspect with repo_hint and method_hint (no code_content)
                print("\nðŸ§  Test 2: quick_introspect with repo_hint and method_hint...")
                print("Testing: package_path='mp_api', method_hint='search'")
                
                try:
                    qi_inputs_2 = {
                        "package_path": "mp_api",
                        "method_hint": "search",
                        "class_hint": "synthesis",
                        "max_suggestions": 3,
                        "no_imports": False,
                    }
                    qi_result_2 = await session.call_tool("quick_introspect", qi_inputs_2)
                    if qi_result_2.content:
                        try:
                            qi_data_2 = json.loads(qi_result_2.content[0].text)
                            print("âœ“ quick_introspect result:")
                            print(f"  Success: {qi_data_2.get('success')}")
                            report_2 = qi_data_2.get("report", "")
                            print("  Report:")
                            print(report_2)
                        except json.JSONDecodeError:
                            print("  Raw quick_introspect result:")
                            print(qi_result_2.content[0].text)
                    else:
                        print("  âš ï¸  No return result from quick_introspect")
                except Exception as e:
                    print(f"  âŒ quick_introspect failed: {e}")
                    import traceback
                    traceback.print_exc()

                print("\n" + "="*60)
                print("Test completed")
                print("="*60)
                
    except Exception as e:
        print(f"âŒ Error occurred during testing: {e}")
        import traceback
        traceback.print_exc()

def main():
    """Main function"""
    print("quick_introspect test")
    print("=" * 50)
    
    asyncio.run(test_quick_introspect())

if __name__ == "__main__":
    main() 
```


================================================================================
=== FILE: mcp_servers_and_tools\research_server\test_research_server_parse_local_package_and_query_KG.py ===
================================================================================

```python
#!/usr/bin/env python3
"""
Test for parse_local_package MCP tool to check pip show package path detection.
"""
import asyncio
import json
import subprocess
from pathlib import Path

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client


def get_server_params() -> StdioServerParameters:
    # Provide minimal env to allow server startup
    import os
    return StdioServerParameters(
        command="python",
        args=["-u", "src/research_mcp.py"],
        cwd=str(Path(__file__).parent),
        env={
            **os.environ,  # Pass all current environment variables
            "SUPABASE_URL": os.environ.get("SUPABASE_URL", "dummy"),
            "SUPABASE_SERVICE_KEY": os.environ.get("SUPABASE_SERVICE_KEY", "dummy"),
            "NEO4J_URI": os.environ.get("NEO4J_URI"),
            "NEO4J_USER": os.environ.get("NEO4J_USER"),
            "NEO4J_PASSWORD": os.environ.get("NEO4J_PASSWORD"),
            "USE_KNOWLEDGE_GRAPH": "true",
            "GENERATE_CODE_SUMMARY": "true",
            "TRANSPORT": "stdio"
        }
    )


async def test_parse_local_package():
    """Test parse_local_package MCP tool with mlip package"""
    print("\nðŸ§ª Testing parse_local_package MCP tool...")
    
    server_params = get_server_params()
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()

            print("\n=== Testing parse_local_package ===")
            try:
                result = await session.call_tool("parse_local_package", {
                    "package_path": "mp_api"
                })
                
                if result.content:
                    text = result.content[0].text
                    print("ðŸ“„ MCP tool result:")
                    
                    # Try to parse as JSON to see the structure
                    try:
                        data = json.loads(text)
                        print(f"âœ… Success: {data.get('success', 'unknown')}")
                        
                        if 'package_path' in data:
                            print(f"ðŸ“ Package path detected by MCP tool: {data['package_path']}")
                        
                        if 'error' in data:
                            print(f"âŒ Error: {data['error']}")
                            
                        # Print first 500 chars of response for overview
                        print(f"\nðŸ“‹ Response preview (first 1500 chars):")
                        print(text[:500] + "..." if len(text) > 1500 else text)
                        
                    except json.JSONDecodeError:
                        print("ðŸ“„ Raw result (not JSON):")
                        print(text[:1500] + "..." if len(text) > 1500 else text)
                        
                else:
                    print("âŒ No content returned from MCP tool")
                    
            except Exception as e:
                print(f"âŒ MCP tool call failed: {e}")
            
            # After testing parse_local_package, test knowledge graph queries
            await test_query_knowledge_graph(session)


async def test_query_knowledge_graph(session):
    """Test query_knowledge_graph to explore mp_api repository and find BondsRester"""
    print("\n=== Testing query_knowledge_graph for mp_api ===")
    
    try:
        # First, check if mp_api repository exists
        print("ðŸ” Checking available repositories...")
        repos_result = await session.call_tool("query_knowledge_graph", {
            "command": "repos"
        })
        
        if repos_result.content:
            repos_text = repos_result.content[0].text
            print("ðŸ“‹ Available repositories:")
            print(repos_text[:300] + "..." if len(repos_text) > 300 else repos_text)
            
            # Check if mp_api is in the repositories
            repos_data = json.loads(repos_text)
            if repos_data.get('success') and 'mp_api' in repos_data.get('data', {}).get('repositories', []):
                print("âœ… mp_api repository found!")
                
                # Explore mp_api repository
                print("\nðŸ” Exploring mp_api repository...")
                explore_result = await session.call_tool("query_knowledge_graph", {
                    "command": "explore mp_api"
                })
                
                if explore_result.content:
                    explore_text = explore_result.content[0].text
                    explore_data = json.loads(explore_text)
                    print(f"ðŸ“Š mp_api statistics: {explore_data.get('data', {}).get('statistics', {})}")
                
                # Search for classes in mp_api
                print("\nðŸ” Searching for classes in mp_api...")
                classes_result = await session.call_tool("query_knowledge_graph", {
                    "command": "classes mp_api"
                })
                
                if classes_result.content:
                    classes_text = classes_result.content[0].text
                    classes_data = json.loads(classes_text)
                    classes_list = classes_data.get('data', {}).get('classes', [])
                    print(f"ðŸ“‹ Found {len(classes_list)} classes in mp_api")
                    
                    # Look for BondsRester specifically
                    bonds_rester_classes = [cls for cls in classes_list if 'bondsrester' in cls.get('name', '').lower()]
                    if bonds_rester_classes:
                        print(f"ðŸŽ¯ Found BondsRester-related classes: {[cls['name'] for cls in bonds_rester_classes]}")
                        
                        # Get detailed info for the first BondsRester class
                        for cls in bonds_rester_classes:
                            print(f"\nðŸ” Exploring class: {cls['name']}")
                            class_result = await session.call_tool("query_knowledge_graph", {
                                "command": f"class {cls['name']}"
                            })
                            
                            if class_result.content:
                                class_text = class_result.content[0].text
                                class_data = json.loads(class_text)
                                if class_data.get('success'):
                                    class_info = class_data.get('data', {}).get('class', {})
                                    methods = class_info.get('methods', [])
                                    print(f"ðŸ“‹ Class {cls['name']} has {len(methods)} methods:")
                                    for method in methods[:5]:  # Show first 5 methods
                                        print(f"  - {method['name']}({', '.join(method.get('parameters', []))})")
                                    if len(methods) > 5:
                                        print(f"  ... and {len(methods) - 5} more methods")
                            break
                    else:
                        print("ðŸ” No BondsRester-related classes found. Showing first 10 classes:")
                        for cls in classes_list[:10]:
                            print(f"  - {cls['name']}")
                        
                        # Try a custom query to search for 'BondsRester' in class names
                        print("\nðŸ” Searching for 'BondsRester' in class names using custom query...")
                        search_result = await session.call_tool("query_knowledge_graph", {
                            "command": "query \"MATCH (r:Repository {name: 'mp_api'})-[:CONTAINS]->(f:File)-[:DEFINES]->(c:Class) WHERE toLower(c.name) CONTAINS 'bondsrester' OR toLower(c.full_name) CONTAINS 'bondsrester' RETURN c.name as class_name, c.full_name as full_name LIMIT 10\""
                        })
                        
                        if search_result.content:
                            search_text = search_result.content[0].text
                            search_data = json.loads(search_text)
                            if search_data.get('success'):
                                results = search_data.get('data', {}).get('results', [])
                                if results:
                                    print(f"ðŸŽ¯ Found {len(results)} classes containing 'BondsRester':")
                                    for result in results:
                                        print(f"  - {result['class_name']} ({result['full_name']})")
                                else:
                                    print("âŒ No classes containing 'BondsRester' found")
            else:
                print("âŒ mp_api repository not found in available repositories")
                
    except Exception as e:
        print(f"âŒ Query knowledge graph failed: {e}")


async def main() -> None:
    print("=" * 60)
    print("ðŸ”¬ Testing parse_local_package Package Path Detection")
    print("=" * 60)
    
    # First check what pip show returns directly
    # check_pip_show_location()
    
    # Then test the MCP tool
    await test_parse_local_package()
    
    print("\n" + "=" * 60)
    print("âœ… Test completed")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())


```


================================================================================
=== FILE: mcp_servers_and_tools\research_server\test_research_server_probe.py ===
================================================================================

```python
#!/usr/bin/env python3
"""
Minimal test for runtime_probe_snippet MCP tool.
Calls the tool with each snippet type and prints the returned snippet.
"""
import asyncio
from pathlib import Path

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client


def get_server_params() -> StdioServerParameters:
    # Provide minimal env to allow server startup
    import os
    return StdioServerParameters(
        command="python",
        args=["-u", "src/research_mcp.py"],
        cwd=str(Path(__file__).parent),
        env={
            **os.environ,  # Pass all current environment variables
            "SUPABASE_URL": os.environ.get("SUPABASE_URL", "dummy"),
            "SUPABASE_SERVICE_KEY": os.environ.get("SUPABASE_SERVICE_KEY", "dummy"),
            "NEO4J_URI": os.environ.get("NEO4J_URI"),
            "NEO4J_USER": os.environ.get("NEO4J_USER"),
            "NEO4J_PASSWORD": os.environ.get("NEO4J_PASSWORD"),
            "USE_KNOWLEDGE_GRAPH": "true",
            "GENERATE_CODE_SUMMARY": "true",
            "TRANSPORT": "stdio"
        }
    )


async def main() -> None:
    server_params = get_server_params()
    async with stdio_client(server_params) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()

            for kind in ["try_get_key", "try_get_attr"]:
                print("\n=== Request:", kind, "===")
                result = await session.call_tool("runtime_probe_snippet", {"snippet": kind})
                if result.content:
                    text = result.content[0].text
                    print(text)
                else:
                    print("No content returned")


if __name__ == "__main__":
    asyncio.run(main())

```


================================================================================
=== FILE: mcp_servers_and_tools\research_server\introspection_and_probe\quick_introspect_core.py ===
================================================================================

```python
#!/usr/bin/env python3
"""
Core implementation of quick introspection utilities for an MCP tool.

Capabilities:
 - Import diagnostics and suggestions
 - Class/method fuzzy discovery and signatures
 - Top-level function fuzzy discovery and signatures (optionally constrained by module)

Design notes:
 - Jedi-based static scanning is preferred first to avoid import side-effects
 - Falls back to safe runtime imports and `inspect` when static analysis yields nothing
 - Output is assembled into a textual report; the MCP tool returns this in JSON
 - Parameter validation raises ValueError with actionable guidance
"""

from __future__ import annotations

import ast
import os
import sys
import importlib
import importlib.util
import inspect
import pkgutil
import io
import difflib
from types import ModuleType
from typing import List, Tuple, Optional, Dict, Iterable, Any

# Optional dependency: Jedi
try:
    import jedi  # type: ignore
except Exception:  # pragma: no cover
    jedi = None  # type: ignore


# ---------- debug helper ----------
def _debug_engine(msg: str) -> None:
    try:
        if os.getenv("QI_DEBUG_ENGINE", "0") == "1":
            print(f"ENGINE: {msg}")
    except Exception:
        pass


# ---------- small utils ----------
def normalize(name: str) -> str:
    return (name or "").replace("_", "").lower()


def similarity(a: str, b: str) -> float:
    return difflib.SequenceMatcher(a=normalize(a), b=normalize(b)).ratio()


def _silence_stdio_context():
    class _Silencer:
        def __enter__(self):
            self._old_out, self._old_err = sys.stdout, sys.stderr
            sys.stdout, sys.stderr = io.StringIO(), io.StringIO()
            return self

        def __exit__(self, exc_type, exc, tb):
            sys.stdout, sys.stderr = self._old_out, self._old_err

    return _Silencer()


def safe_import(module_name: str) -> Optional[ModuleType]:
    try:
        with _silence_stdio_context():
            return importlib.import_module(module_name)
    except Exception:
        return None


def iter_submodules(pkg: ModuleType, limit: int = 300) -> Iterable[str]:
    if not hasattr(pkg, "__path__"):
        return []
    count = 0
    try:
        for _finder, modname, _ispkg in pkgutil.walk_packages(
            pkg.__path__, prefix=pkg.__name__ + ".", onerror=lambda _name: None
        ):
            yield modname
            count += 1
            if count >= limit:
                break
    except Exception:
        return []


def list_attrs(module: ModuleType) -> Dict[str, Any]:
    try:
        return {n: getattr(module, n) for n in dir(module)}
    except Exception:
        return {}


def is_public(name: str) -> bool:
    return name and not name.startswith("_")


def rank_names(cands: List[str], target: str) -> List[str]:
    t = normalize(target)
    return sorted(
        cands,
        key=lambda n: (
            normalize(n) != t,
            0 if normalize(n).startswith(t) else 1,
            -similarity(n, target),
            len(n),
        ),
    )


def rank_pairs(pairs: List[Tuple[str, str]], target: str) -> List[Tuple[str, str]]:
    return sorted(
        pairs,
        key=lambda p: (
            normalize(p[0]) != normalize(target),
            0 if normalize(p[0]).startswith(normalize(target)) else 1,
            -similarity(p[0], target),
            len(p[1]),
        ),
    )


def extract_imports(code: str) -> List[Tuple[str, List[str]]]:
    out = []
    tree = ast.parse(code)
    for node in ast.walk(tree):
        if isinstance(node, ast.ImportFrom):
            mod = node.module or ""
            names = [a.name for a in node.names if a.name != "*"]
            if mod:
                out.append((mod, names))
        elif isinstance(node, ast.Import):
            for alias in node.names:
                out.append((alias.name, []))
    return out


def top_repo_of(module_path: str) -> str:
    return (module_path or "").split(".")[0]


def module_belongs_to_root(mod: ModuleType, package_root: Optional[str]) -> bool:
    if not package_root:
        return True
    try:
        root = os.path.abspath(package_root)
        if hasattr(mod, "__file__") and mod.__file__:
            return os.path.abspath(mod.__file__).startswith(root)
        if hasattr(mod, "__path__") and mod.__path__:
            for p in mod.__path__:
                if os.path.abspath(p).startswith(root):
                    return True
            return False
    except Exception:
        return True


def _get_repo_roots(repo_name: str, pkg_root: Optional[str]) -> List[str]:
    if pkg_root and os.path.isdir(pkg_root):
        return [pkg_root]
    try:
        spec = importlib.util.find_spec(repo_name)
    except Exception:
        spec = None
    roots: List[str] = []
    if spec is not None:
        if getattr(spec, "submodule_search_locations", None):
            roots.extend(list(spec.submodule_search_locations))
        elif getattr(spec, "origin", None):
            import os as _os
            roots.append(_os.path.dirname(spec.origin))
    return [r for r in roots if r and os.path.isdir(r)]


def _module_name_from_path(file_path: str, roots: List[str]) -> Optional[str]:
    file_path = os.path.abspath(file_path)
    for root in roots:
        root_abs = os.path.abspath(root)
        if file_path.startswith(root_abs):
            rel = os.path.relpath(file_path, root_abs)
            if rel.endswith("__init__.py"):
                rel = os.path.dirname(rel)
            mod = rel[:-3] if rel.endswith(".py") else rel
            mod = mod.replace(os.sep, ".")
            top_pkg = os.path.basename(os.path.normpath(root_abs))
            if mod:
                return f"{top_pkg}.{mod}" if mod else top_pkg
            return top_pkg
    return None


# ---------- search helpers (static-first with Jedi, runtime fallback) ----------
def search_symbol_in_package(repo: str, symbol: str, package_root: Optional[str] = None) -> List[Tuple[str, str]]:
    results: List[Tuple[str, str]] = []

    def static_search_with_jedi() -> List[Tuple[str, str]]:
        if jedi is None:
            return []
        roots = _get_repo_roots(repo, package_root)
        if not roots:
            return []
        matches: List[Tuple[str, str]] = []
        max_files = 400
        seen_files = 0
        import os as _os
        for root in roots:
            for dirpath, _dirs, files in _os.walk(root):
                for fn in files:
                    if not fn.endswith('.py'):
                        continue
                    file_path = _os.path.join(dirpath, fn)
                    try:
                        names = jedi.api.names(path=file_path, all_scopes=True, definitions=True, references=False)  # type: ignore[attr-defined]
                    except Exception:
                        names = []
                    for nm in names:
                        try:
                            typ = getattr(nm, 'type', '')
                            nm_str = getattr(nm, 'name', '')
                            if typ not in ('class', 'function'):
                                continue
                            if not nm_str or not is_public(nm_str):
                                continue
                            if nm_str == symbol or normalize(symbol) in normalize(nm_str) or similarity(nm_str, symbol) > 0.7:
                                modname = None
                                full_name = getattr(nm, 'full_name', None)
                                if isinstance(full_name, str) and '.' in full_name:
                                    modname = '.'.join(full_name.split('.')[:-1])
                                if not modname:
                                    modname = _module_name_from_path(file_path, roots)
                                if modname:
                                    matches.append((nm_str, modname))
                        except Exception:
                            continue
                    seen_files += 1
                    if seen_files >= max_files:
                        break
                if seen_files >= max_files:
                    break
        dedup: List[Tuple[str, str]] = []
        seen: set = set()
        for n, m in rank_pairs(matches, symbol):
            if (n, m) not in seen:
                seen.add((n, m))
                dedup.append((n, m))
        return dedup[:30]

    results = static_search_with_jedi()
    if results:
        _debug_engine("class/function search: jedi")
        return results

    pkg = safe_import(repo)
    if not pkg:
        return []
    attrs = list_attrs(pkg)
    for n, obj in attrs.items():
        if is_public(n) and (n == symbol or normalize(symbol) in normalize(n) or similarity(n, symbol) > 0.7):
            if inspect.isclass(obj) or inspect.isfunction(obj):
                results.append((n, pkg.__name__))
    submods = list(iter_submodules(pkg, limit=400))
    sym_norm = normalize(symbol)
    submods_ranked = sorted(
        submods,
        key=lambda m: (
            sym_norm not in m.replace(".", "").lower(),
            len(m),
        ),
    )
    for modname in submods_ranked:
        mod = safe_import(modname)
        if not mod:
            continue
        if not module_belongs_to_root(mod, package_root):
            continue
        attrs = list_attrs(mod)
        for n, obj in attrs.items():
            if not is_public(n):
                continue
            if n == symbol or normalize(symbol) in normalize(n) or similarity(n, symbol) > 0.7:
                if inspect.isclass(obj) or inspect.isfunction(obj):
                    results.append((n, mod.__name__))
    seen = set()
    ranked = []
    for n, m in rank_pairs(results, symbol):
        key = (n, m)
        if key not in seen:
            seen.add(key)
            ranked.append((n, m))
    return ranked[:30]


def search_functions_in_package(
    repo: str,
    func_fragment: str,
    module_hint: Optional[str] = None,
    package_root: Optional[str] = None,
) -> List[Tuple[str, str, str]]:
    results: List[Tuple[str, str, str]] = []

    def static_collect_with_jedi() -> List[Tuple[str, str, str]]:
        if jedi is None:
            return []
        roots = _get_repo_roots(repo, package_root)
        if not roots:
            return []
        collected: List[Tuple[str, str, str]] = []
        max_files = 400
        seen_files = 0
        mod_hint_norm = normalize(module_hint or '')
        import os as _os
        for root in roots:
            for dirpath, _dirs, files in _os.walk(root):
                for fn in files:
                    if not fn.endswith('.py'):
                        continue
                    file_path = _os.path.join(dirpath, fn)
                    try:
                        names = jedi.api.names(path=file_path, all_scopes=False, definitions=True, references=False)  # type: ignore[attr-defined]
                    except Exception:
                        names = []
                    for nm in names:
                        try:
                            if getattr(nm, 'type', '') != 'function':
                                continue
                            func_name = getattr(nm, 'name', '')
                            if not is_public(func_name):
                                continue
                            if not (func_name == func_fragment or normalize(func_fragment) in normalize(func_name) or similarity(func_name, func_fragment) > 0.7):
                                continue
                            modname = None
                            full_name = getattr(nm, 'full_name', None)
                            if isinstance(full_name, str) and '.' in full_name:
                                modname = '.'.join(full_name.split('.')[:-1])
                            if not modname:
                                modname = _module_name_from_path(file_path, roots)
                            if not modname:
                                continue
                            if module_hint and mod_hint_norm not in normalize(modname):
                                continue
                            collected.append((f"from {modname} import {func_name}", func_name, "(... )".replace(' ', '')))
                        except Exception:
                            continue
                    seen_files += 1
                    if seen_files >= max_files:
                        break
                if seen_files >= max_files:
                    break
        return collected

    static_res = static_collect_with_jedi()
    if static_res:
        _debug_engine("function search: jedi")
        return static_res

    def collect_from_module(mod: ModuleType):
        attrs = list_attrs(mod)
        for n, obj in attrs.items():
            if not is_public(n):
                continue
            if inspect.isfunction(obj):
                if n == func_fragment or normalize(func_fragment) in normalize(n) or similarity(n, func_fragment) > 0.7:
                    try:
                        sig = str(inspect.signature(obj))
                    except Exception:
                        sig = "(...)"
                    results.append((f"from {mod.__name__} import {n}", n, sig))

    if module_hint:
        mod = safe_import(module_hint)
        if mod and module_belongs_to_root(mod, package_root):
            collect_from_module(mod)
            if hasattr(mod, "__path__"):
                for sub in iter_submodules(mod, limit=400):
                    subm = safe_import(sub)
                    if subm and module_belongs_to_root(subm, package_root):
                        collect_from_module(subm)
            return results
        pkg = safe_import(repo)
        if pkg:
            frag = module_hint.replace("_", "").replace(".", "").lower()
            candidates = []
            for sub in iter_submodules(pkg, limit=400):
                key = sub.replace("_", "").replace(".", "").lower()
                if frag in key:
                    candidates.append(sub)
            base_token = module_hint.split(".")[-1]
            candidates = sorted(set(candidates), key=lambda s: (s.find(base_token) if base_token in s else 9999, len(s)))[:20]
            for cand in candidates:
                subm = safe_import(cand)
                if subm and module_belongs_to_root(subm, package_root):
                    collect_from_module(subm)
            if not results:
                norm_hint = frag
                sim_pool = []
                import difflib as _difflib
                for sub in iter_submodules(pkg, limit=400):
                    norm_sub = sub.replace("_", "").replace(".", "").lower()
                    score = _difflib.SequenceMatcher(a=norm_hint, b=norm_sub).ratio()
                    if score >= 0.7:
                        sim_pool.append((score, sub))
                for _score, cand in sorted(sim_pool, key=lambda t: (-t[0], len(t[1])))[:20]:
                    subm = safe_import(cand)
                    if subm and module_belongs_to_root(subm, package_root):
                        collect_from_module(subm)
        return results

    pkg = safe_import(repo)
    if not pkg:
        return results
    collect_from_module(pkg)
    for sub in iter_submodules(pkg, limit=400):
        subm = safe_import(sub)
        if subm and module_belongs_to_root(subm, package_root):
            collect_from_module(subm)
    return results


def method_suggestions(repo: str, class_hint: str, method_hint: Optional[str], package_root: Optional[str] = None) -> List[Tuple[str, str, str]]:
    out: List[Tuple[str, str, str]] = []

    def extract_methods_from_file(file_path: str, cls_name: str) -> List[Tuple[str, str]]:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            tree = ast.parse(content)
        except Exception:
            return []
        methods: List[Tuple[str, str]] = []
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef) and node.name == cls_name:
                for item in node.body:
                    if isinstance(item, (ast.FunctionDef, ast.AsyncFunctionDef)) and is_public(item.name):
                        try:
                            args = []
                            for a in getattr(item.args, 'posonlyargs', []) or []:
                                args.append(a.arg)
                            for a in item.args.args:
                                if a.arg != 'self':
                                    args.append(a.arg)
                            if item.args.vararg:
                                args.append('*' + item.args.vararg.arg)
                            for a in item.args.kwonlyargs:
                                args.append(a.arg)
                            if item.args.kwarg:
                                args.append('**' + item.args.kwarg.arg)
                            sig = '(' + ', '.join(args) + ')'
                        except Exception:
                            sig = '(...)'
                        methods.append((item.name, sig))
                break
        return methods

    def static_collect_with_jedi() -> List[Tuple[str, str, str]]:
        if jedi is None:
            return []
        roots = _get_repo_roots(repo, package_root)
        if not roots:
            return []
        collected: List[Tuple[str, str, str]] = []
        max_files = 400
        seen_files = 0
        import os as _os
        for root in roots:
            for dirpath, _dirs, files in _os.walk(root):
                for fn in files:
                    if not fn.endswith('.py'):
                        continue
                    file_path = _os.path.join(dirpath, fn)
                    try:
                        names = jedi.api.names(path=file_path, all_scopes=True, definitions=True, references=False)  # type: ignore[attr-defined]
                    except Exception:
                        names = []
                    for nm in names:
                        try:
                            if getattr(nm, 'type', '') != 'class':
                                continue
                            cls_name = getattr(nm, 'name', '')
                            if not is_public(cls_name):
                                continue
                            if not (cls_name == class_hint or normalize(class_hint) in normalize(cls_name) or similarity(cls_name, class_hint) > 0.7):
                                continue
                            modname = None
                            full_name = getattr(nm, 'full_name', None)
                            if isinstance(full_name, str) and '.' in full_name:
                                modname = '.'.join(full_name.split('.')[:-1])
                            if not modname:
                                modname = _module_name_from_path(file_path, roots)
                            if not modname:
                                continue
                            methods = extract_methods_from_file(file_path, cls_name)
                            method_names = [m for m in methods if not method_hint or (m[0] == method_hint or normalize(method_hint) in normalize(m[0]) or similarity(m[0], method_hint) > 0.7)]
                            if method_hint:
                                ranked = sorted(method_names, key=lambda x: (normalize(x[0]) != normalize(method_hint), -similarity(x[0], method_hint), len(x[0])))[:8]
                            else:
                                ranked = sorted(method_names, key=lambda x: x[0])[:20]
                            for mname, sig in ranked:
                                collected.append((f"from {modname} import {cls_name}", mname, sig))
                        except Exception:
                            continue
                    seen_files += 1
                    if seen_files >= max_files:
                        break
                if seen_files >= max_files:
                    break
        return collected[:20]

    static_res = static_collect_with_jedi()
    if static_res:
        _debug_engine("method suggestions: jedi")
        return static_res

    class_cands = search_symbol_in_package(repo, class_hint, package_root=package_root)
    pruned = []
    for sym, modname in class_cands:
        mod = safe_import(modname)
        if not mod:
            continue
        obj = getattr(mod, sym, None)
        if inspect.isclass(obj):
            pruned.append((sym, modname, obj))
    for sym, modname, cls in pruned[:10]:
        methods = {n: f for n, f in inspect.getmembers(cls, predicate=inspect.isfunction) if is_public(n)}
        if method_hint:
            ranked = rank_names(list(methods.keys()), method_hint)[:8]
        else:
            ranked = sorted(methods.keys())
        for n in ranked:
            try:
                sig = str(inspect.signature(methods[n]))
            except Exception:
                sig = "(...)"
            out.append((f"from {modname} import {sym}", n, sig))
    return out[:20]


def method_suggestions_across_repo(repo: str, method_hint: str, package_root: Optional[str] = None) -> List[Tuple[str, str, str]]:
    results: List[Tuple[str, str, str]] = []

    def extract_methods_from_file(file_path: str) -> List[Tuple[str, str, str]]:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            tree = ast.parse(content)
        except Exception:
            return []
        triples: List[Tuple[str, str, str]] = []
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef) and is_public(node.name):
                cls_name = node.name
                methods = []
                for item in node.body:
                    if isinstance(item, (ast.FunctionDef, ast.AsyncFunctionDef)) and is_public(item.name):
                        if item.name == method_hint or normalize(method_hint) in normalize(item.name) or similarity(item.name, method_hint) > 0.7:
                            try:
                                args = []
                                for a in getattr(item.args, 'posonlyargs', []) or []:
                                    args.append(a.arg)
                                for a in item.args.args:
                                    if a.arg != 'self':
                                        args.append(a.arg)
                                if item.args.vararg:
                                    args.append('*' + item.args.vararg.arg)
                                for a in item.args.kwonlyargs:
                                    args.append(a.arg)
                                if item.args.kwarg:
                                    args.append('**' + item.args.kwarg.arg)
                                sig = '(' + ', '.join(args) + ')'
                            except Exception:
                                sig = '(...)'
                            methods.append((cls_name, item.name, sig))
                for cls_name2, mname, sig in methods:
                    triples.append((cls_name2, mname, sig))
        return triples

    if jedi is not None:
        roots = _get_repo_roots(repo, package_root)
        if roots:
            max_files = 400
            seen_files = 0
            import os as _os
            for root in roots:
                for dirpath, _dirs, files in _os.walk(root):
                    for fn in files:
                        if not fn.endswith('.py'):
                            continue
                        file_path = _os.path.join(dirpath, fn)
                        triples = extract_methods_from_file(file_path)
                        if triples:
                            modname = _module_name_from_path(file_path, roots)
                            if modname:
                                for cls_name, mname, sig in triples:
                                    results.append((f"from {modname} import {cls_name}", mname, sig))
                        seen_files += 1
                        if seen_files >= max_files:
                            break
                    if seen_files >= max_files:
                        break
            if results:
                _debug_engine("method suggestions across repo: jedi")
                return results

    pkg = safe_import(repo)
    if not pkg:
        return results

    def collect_from_module(mod: ModuleType):
        try:
            for name, obj in inspect.getmembers(mod, inspect.isclass):
                if not is_public(name):
                    continue
                cls = obj
                methods = {n: f for n, f in inspect.getmembers(cls, predicate=inspect.isfunction) if is_public(n)}
                for mname, func in methods.items():
                    if mname == method_hint or normalize(method_hint) in normalize(mname) or similarity(mname, method_hint) > 0.7:
                        try:
                            sig = str(inspect.signature(func))
                        except Exception:
                            sig = "(...)"
                        results.append((f"from {mod.__name__} import {name}", mname, sig))
        except Exception:
            pass

    collect_from_module(pkg)
    for sub in iter_submodules(pkg, limit=400):
        subm = safe_import(sub)
        if subm and module_belongs_to_root(subm, package_root):
            collect_from_module(subm)
    return results


def run_quick_introspect(
    *,
    code_content: Optional[str] = None,
    class_hint: Optional[str] = None,
    method_hint: Optional[str] = None,
    package_path: Optional[str] = None,
    function_hint: Optional[str] = None,
    module_hint: Optional[str] = None,
    repo_hint: Optional[str] = None,
    max_suggestions: Optional[int] = None,
    no_imports: bool = False,
) -> str:
    """
    Execute quick introspection using provided parameters and return a human-readable report string.

    Parameter relationships (enforced):
    - repo_hint vs package_path: mutually exclusive; provide at most one
    - module_hint requires function_hint
    - If any of class_hint/method_hint is provided, one of repo_hint or package_path is required
    - If function_hint is provided, one of repo_hint or package_path is required

    Notes:
    - repo_hint is the top-level import module name (may differ from pip distribution name)
    - package_path must be an absolute path to the package root directory
    - method_hint can be provided without class_hint to trigger a repo-wide search (noisy)
    - code_content is OPTIONAL unless you want import diagnostics. If you want import checks/suggestions, you must
      provide code_content with non-empty code. If not provided, import diagnostics will be skipped and only 
      symbol suggestions (class/method/function) will be generated based on hints
    """

    # Validate relationships
    if repo_hint and package_path:
        raise ValueError("Provide only one of repo_hint or package_path.")
    if module_hint and not function_hint:
        raise ValueError("module_hint must be used together with function_hint.")


    # Prepare package root and optional inferred package name
    package_root: Optional[str] = None
    package_name_from_path: Optional[str] = None
    if package_path:
        candidate = os.path.abspath(package_path)
        if os.path.isdir(candidate):
            package_root = candidate
            parent = os.path.dirname(candidate)
            if parent not in sys.path:
                sys.path.insert(0, parent)
            package_name_from_path = os.path.basename(candidate.rstrip(os.sep))
        else:
            raise ValueError(f"package_path does not exist or is not a directory: {package_path}")

    # High-level coupling: class/method or function searches require repo context
    any_hint = bool(class_hint or method_hint or function_hint)
    if any_hint and repo_hint and not safe_import(repo_hint):
        # Build a dynamic site-packages hint for actionable guidance
        try:
            import sysconfig as _sc, site as _site
            _pure = _sc.get_paths().get("purelib") or ""
            _sites = []
            try:
                _sites = _site.getsitepackages() or []
            except Exception:
                _sites = []
            _hint = _pure or (_sites[0] if _sites else "<site-packages>")
        except Exception:
            _hint = "<site-packages>"
        raise ValueError(
            f"Your repo_hint could not be imported. Provide package_path instead (absolute path, or relative path starting from {_hint}); or use your check_package_version tool (if you have it) to obtain the absolute package_path and rerun this tool with package_path."
        )

    if (class_hint or method_hint) and not (repo_hint or package_root):
        raise ValueError(
            "For class/method search, provide repo_hint (top-level import name) or package_path (absolute path to package)."
        )

    if function_hint and not (repo_hint or package_root):
        raise ValueError(
            "For function search, provide repo_hint (top-level import name) or package_path (absolute path to package)."
        )

    # Helper to enforce suggestion limits
    def limit_list(xs: List[Any]) -> List[Any]:
        return xs if max_suggestions is None else xs[: max(0, max_suggestions)]

    buffer = io.StringIO()
    def _writeln(line: str = ""):
        buffer.write(line + "\n")

    # Read code first (optional)
    code: str = code_content or ""
    
    # If no code and no symbol hints are provided, return actionable guidance
    any_hint = bool(class_hint or method_hint or function_hint)
    if not code.strip() and not any_hint:
        _writeln("No sufficient parameters provided for quick introspection.")
        _writeln("\nHOW TO USE (provide parameters based on your error message):")
        _writeln("- Import errors: pass code_content to enable import diagnostics.")
        _writeln("- Class issues: provide class_hint and repo_hint (or package_path).")
        _writeln("- Method issues: provide method_hint and repo_hint (or package_path); preferably also class_hint to narrow.")
        _writeln("- Function issues: provide function_hint and repo_hint (or package_path); optionally module_hint to narrow.")
        _writeln("Notes: repo_hint must be the top-level import name, and is preferred over package_path.")
        return buffer.getvalue()

    # Parse imports (only if code provided and import diagnostics enabled)
    imports: List[Tuple[str, List[str]]] = []
    if code.strip() and not no_imports:
        try:
            imports = extract_imports(code)
        except Exception:
            _writeln("AST_PARSE_ERROR")
            raise

    # Import diagnostics
    if imports and not no_imports:
        _writeln("=== Import Check & Suggestions ===")
        for module, names in imports:
            repo = top_repo_of(module)
            ok_mod = bool(safe_import(module))
            ok_repo = bool(safe_import(repo)) if repo else False
            _writeln(f"\n[Module] {module}  (repo={repo})  -> {'OK' if ok_mod else 'ImportError'}")
            sugg = suggest_import_fixes(module, names, package_root=package_root)
            for name, lines in sugg.items():
                if name is None:
                    continue
                status = "OK" if any(l.endswith(" # OK") for l in lines) else "FIX"
                _writeln(f"  - Symbol: {name}  [{status}]")
                for i, line in enumerate(limit_list(lines), 1):
                    _writeln(f"      {i}. {line}")
            if not ok_mod:
                if not ok_repo:
                    _writeln("  TIP: The current top-level import root may be incorrect.")
                    _writeln("       Try other top-level import names or use your check_package_version tool to obtain an absolute package_path, then rerun this tool with package_path.")
                else:
                    _writeln("  NOTE: Top-level import exists; the submodule path is likely incorrect. Use the suggestions above to fix it.")
    elif code.strip() and not imports and not no_imports:
        _writeln("No imports found in the provided code. Add a line like: from pkg.sub import Symbol")
    elif not code.strip() and not no_imports:
        _writeln("Import diagnostics skipped: no code provided. Provide code_content to analyze imports if you want import diagnostics.")

    # Method suggestions with class context
    if class_hint:
        if repo_hint:
            repo = repo_hint
        elif package_name_from_path:
            repo = package_name_from_path
        elif imports:
            repo = top_repo_of(imports[0][0])
        else:
            repo = top_repo_of(class_hint)
        _writeln(f"\n=== Method Suggestions (repo={repo}, classâ‰ˆ{class_hint}, methodâ‰ˆ{method_hint or ''}) ===")
        results = method_suggestions(repo, class_hint, method_hint, package_root=package_root)
        for imp, mname, sig in limit_list(results):
            _writeln(f"  {imp}    # method: {mname}{sig}")

    # Repo-wide method search when only method_hint is provided
    if method_hint and not class_hint and (repo_hint or package_root):
        if repo_hint:
            repo = repo_hint
        elif package_name_from_path:
            repo = package_name_from_path
        elif imports:
            repo = top_repo_of(imports[0][0])
        else:
            repo = ""
        if repo:
            _writeln(f"\n=== Method Suggestions (repo={repo}, methodâ‰ˆ{method_hint}) â€” repo-wide search ===")
            # Warn about noise when class_hint is omitted
            _writeln("NOTE: --method_hint provided without --class_hint. Searching across all classes in the repository (may be noisy). Consider adding a fuzzy or exact --class_hint to narrow the scope and rerun this tool if needed.")
            widish = method_suggestions_across_repo(repo, method_hint, package_root=package_root)
            for imp, mname, sig in limit_list(widish):
                _writeln(f"  {imp}    # method: {mname}{sig}")

    # Function suggestions
    if function_hint:
        if repo_hint:
            repo = repo_hint
        elif package_name_from_path:
            repo = package_name_from_path
        elif imports:
            repo = top_repo_of(imports[0][0])
        elif module_hint:
            repo = top_repo_of(module_hint)
        else:
            repo = ""
        _writeln(f"\n=== Function Suggestions (repoâ‰ˆ{repo or 'unknown'}, module_hintâ‰ˆ{module_hint or ''}, functionâ‰ˆ{function_hint}) ===")
        fres = search_functions_in_package(repo, function_hint, module_hint=module_hint, package_root=package_root) if repo else []
        if not fres and module_hint:
            mod = safe_import(module_hint)
            if not mod:
                _writeln("NOTE: module_hint could not be identified. Consider removing module_hint and rerunning this tool with only function_hint and repo_hint to broaden search.")
        for imp, fname, sig in limit_list(fres):
            _writeln(f"  {imp}    # function: {fname}{sig}")

    return buffer.getvalue()


# ---------- import fix helpers ----------
def suggest_import_fixes(module: str, names: List[str], package_root: Optional[str] = None) -> Dict[str, List[str]]:
    suggestions: Dict[str, List[str]] = {}
    repo = top_repo_of(module)

    mod = safe_import(module)
    for name in names or [None]:
        if name is None:
            suggestions[module] = [f"import {module}  # {'OK' if mod else 'ImportError'}"]
            continue

        cands: List[str] = []
        if mod and hasattr(mod, name):
            cands.append(f"from {module} import {name}  # OK")
        else:
            syms = search_symbol_in_package(repo, name, package_root=package_root)
            for sym, modname in syms:
                cands.append(f"from {modname} import {sym}")
            if mod:
                names_in_mod = [n for n in dir(mod) if is_public(n)]
                for n in rank_names(names_in_mod, name)[:5]:
                    if hasattr(mod, n):
                        cands.append(f"from {module} import {n}")
        uniq = []
        seen = set()
        for s in cands:
            if s not in seen:
                seen.add(s)
                uniq.append(s)
        suggestions[name] = uniq[:10]
    return suggestions


__all__ = [
    "run_quick_introspect",
]


```


================================================================================
=== FILE: mcp_servers_and_tools\research_server\introspection_and_probe\runtime_probe.py ===
================================================================================

```python
"""
Lightweight runtime probing helpers for debugging KeyError/AttributeError.

Usage examples:

# Probe a mapping for a key; on KeyError prints available keys
value = try_get_key(mapping, "target_key")

# Probe an object attribute; on AttributeError prints accessible attributes and suggestions
attr_value = try_get_attr(obj, "attribute_name")
"""

from __future__ import annotations

from typing import Any, List
import inspect
import difflib


def _iter_public_dir(obj: Any) -> list[str]:
    try:
        names = [n for n in dir(obj) if not n.startswith("_")]
        names.sort()
        return names
    except Exception:
        return []


def show_all_keys_or_attrs(obj: Any) -> None:
    """Print all accessible keys (for mappings) or all public attributes (for objects)."""
    try:
        if isinstance(obj, dict):
            try:
                keys = sorted(obj.keys())  # type: ignore[arg-type]
            except Exception:
                keys = list(obj.keys())  # type: ignore[arg-type]
            print("DICT_KEYS:", keys)
            print("TYPE:", type(obj).__name__)
            return

        # pydantic v2 style models
        if hasattr(obj, "model_dump") and callable(getattr(obj, "model_dump")):
            try:
                keys = sorted(list(getattr(obj, "model_dump")().keys()))
            except Exception:
                keys = []
            print("MODEL_KEYS:", keys)
            print("TYPE:", type(obj).__name__)
            return

        # generic object
        names = _iter_public_dir(obj)
        print("ATTRS:", names)
        print("TYPE:", type(obj).__name__)
        print("HINT: Some attributes may themselves contain nested attributes; probe them individually if needed (e.g., obj.attr).")
    except Exception as e:  # pragma: no cover
        print("PROBE_FAIL:", e)

def try_get_key(mapping: Any, key: Any) -> Any:
    """Attempt mapping[key]. 

    On success: print a concise success note (or a None-value warning) and return the value.
    On KeyError: print available keys for the mapping and re-raise KeyError.
    """
    try:
        value = mapping[key]
        try:
            if value is None:
                print(f"[probe_key] Found key {key!r} but value is None; verify the intended key or upstream logic.")
            else:
                print(
                    f"[probe_key] OK: key {key!r} is present (type={type(value).__name__}). "
                    f"This may mean either the earlier KeyError was at a different site so you should probe the correct line and re-run, "
                    f"or you are now probing a different key than the one that caused the earlier KeyError and have successfully debugged the issue."
                )
        except Exception:
            pass
        return value
    except KeyError:
        print(f"KeyError: missing key -> {key!r}")
        show_all_keys_or_attrs(mapping)
        raise


def try_get_attr(obj: Any, name: str) -> Any:
    """Attempt to access an attribute.

    On success: print a concise success note (or a None-value warning) and return the value.
    On AttributeError: print accessible attributes and suggest similar names, then re-raise AttributeError.
    """
    try:
        value = getattr(obj, name)
        try:
            if value is None:
                print(f"[probe_attr] Found attribute {name!r} but value is None; verify the intended attribute or upstream logic.")
            else:
                print(
                    f"[probe_attr] OK: attribute {name!r} is present (type={type(value).__name__}). "
                    f"This may mean either the earlier AttributeError was at a different site so you should probe the correct line and re-run, "
                    f"or you are now probing a different attribute than the one that caused the earlier AttributeError and have successfully debugged the issue."
                )
        except Exception:
            pass
        return value
    except AttributeError:
        print(f"AttributeError: missing attribute -> {name!r}")
        show_all_keys_or_attrs(obj)
        # Additionally, suggest similar attribute names (best-effort, includes class properties)
        try:
            def _normalize(s: str) -> str:
                return s.replace("_", "").lower()

            def _similar(a: str, b: str) -> float:
                return difflib.SequenceMatcher(a=_normalize(a), b=_normalize(b)).ratio()

            candidates: List[str] = []
            # Instance-visible names
            try:
                candidates.extend([n for n in dir(obj) if not n.startswith("_")])
            except Exception:
                pass
            # Class-level properties/descriptors across MRO
            try:
                for base in inspect.getmro(type(obj)):
                    for n, desc in getattr(base, "__dict__", {}).items():
                        if n.startswith("_"):
                            continue
                        if isinstance(desc, property) or inspect.isdatadescriptor(desc) or inspect.ismethoddescriptor(desc):
                            candidates.append(n)
            except Exception:
                pass
            # Rank and print top suggestions
            uniq = sorted(set(candidates))
            ranked = sorted(uniq, key=lambda n: (_normalize(n) != _normalize(name), 0 if _normalize(n).startswith(_normalize(name)) else 1, -_similar(n, name), len(n)))
            top = ranked[:10]
            if top:
                print("SUGGEST_ATTRS (maybe related by name similarity):", top)
                print("HINT: Suggestions are based on string similarity; try them directly or probe nested attributes if needed.")
        except Exception:
            pass
        raise


```


================================================================================
=== FILE: mcp_servers_and_tools\research_server\knowledge_graphs\parse_repo_into_neo4j.py ===
================================================================================

```python
"""
Direct Neo4j GitHub Code Repository Extractor

Creates nodes and relationships directly in Neo4j:
- File nodes
- Class nodes  
- Method nodes
- Function nodes
- Import relationships

Bypasses all LLM processing for maximum speed.
"""

import asyncio
import logging
import os
import subprocess
import shutil
from datetime import datetime, timezone
from pathlib import Path
from typing import List, Optional, Dict, Any, Set
import ast

from dotenv import load_dotenv
from neo4j import AsyncGraphDatabase

# Configure logging - respect quiet mode environment variables
log_level = logging.ERROR if os.getenv('MCP_QUIET', '0') == '1' else logging.INFO
logging.basicConfig(
    level=log_level,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
)
logger = logging.getLogger(__name__)


class Neo4jCodeAnalyzer:
    """Analyzes code for direct Neo4j insertion"""
    
    def __init__(self):
        # External modules to ignore
        self.external_modules = {
            # Python standard library
            'os', 'sys', 'json', 'logging', 'datetime', 'pathlib', 'typing', 'collections',
            'asyncio', 'subprocess', 'ast', 're', 'string', 'urllib', 'http', 'email',
            'time', 'uuid', 'hashlib', 'base64', 'itertools', 'functools', 'operator',
            'contextlib', 'copy', 'pickle', 'tempfile', 'shutil', 'glob', 'fnmatch',
            'io', 'codecs', 'locale', 'platform', 'socket', 'ssl', 'threading', 'queue',
            'multiprocessing', 'concurrent', 'warnings', 'traceback', 'inspect',
            'importlib', 'pkgutil', 'types', 'weakref', 'gc', 'dataclasses', 'enum',
            'abc', 'numbers', 'decimal', 'fractions', 'math', 'cmath', 'random', 'statistics',
            
            # Common third-party libraries
            'requests', 'urllib3', 'httpx', 'aiohttp', 'flask', 'django', 'fastapi',
            'pydantic', 'sqlalchemy', 'alembic', 'psycopg2', 'pymongo', 'redis',
            'celery', 'pytest', 'unittest', 'mock', 'faker', 'factory', 'hypothesis',
            'numpy', 'pandas', 'matplotlib', 'seaborn', 'scipy', 'sklearn', 'torch',
            'tensorflow', 'keras', 'opencv', 'pillow', 'boto3', 'botocore', 'azure',
            'google', 'openai', 'anthropic', 'langchain', 'transformers', 'huggingface_hub',
            'click', 'typer', 'rich', 'colorama', 'tqdm', 'python-dotenv', 'pyyaml',
            'toml', 'configargparse', 'marshmallow', 'attrs', 'dataclasses-json',
            'jsonschema', 'cerberus', 'voluptuous', 'schema', 'jinja2', 'mako',
            'cryptography', 'bcrypt', 'passlib', 'jwt', 'authlib', 'oauthlib'
        }
    
    def analyze_python_file(self, file_path: Path, repo_root: Path, project_modules: Set[str]) -> Dict[str, Any]:
        """Extract structure for direct Neo4j insertion"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            tree = ast.parse(content)
            relative_path = str(file_path.relative_to(repo_root))
            module_name = self._get_importable_module_name(file_path, repo_root, relative_path)
            
            # Extract structure
            classes = []
            functions = []
            imports = []
            
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    # Extract class with its methods and attributes
                    methods = []
                    attributes = []
                    
                    for item in node.body:
                        if isinstance(item, (ast.FunctionDef, ast.AsyncFunctionDef)):
                            if not item.name.startswith('_'):  # Public methods only
                                # Extract comprehensive parameter info
                                params = self._extract_function_parameters(item)
                                
                                # Get return type annotation
                                return_type = self._get_name(item.returns) if item.returns else 'Any'
                                
                                # Create detailed parameter list for Neo4j storage
                                params_detailed = []
                                for p in params:
                                    param_str = f"{p['name']}:{p['type']}"
                                    if p['optional'] and p['default'] is not None:
                                        param_str += f"={p['default']}"
                                    elif p['optional']:
                                        param_str += "=None"
                                    if p['kind'] != 'positional':
                                        param_str = f"[{p['kind']}] {param_str}"
                                    params_detailed.append(param_str)
                                
                                methods.append({
                                    'name': item.name,
                                    'params': params,  # Full parameter objects
                                    'params_detailed': params_detailed,  # Detailed string format
                                    'return_type': return_type,
                                    'args': [arg.arg for arg in item.args.args if arg.arg != 'self']  # Keep for backwards compatibility
                                })
                        elif isinstance(item, ast.AnnAssign) and isinstance(item.target, ast.Name):
                            # Type annotated attributes
                            if not item.target.id.startswith('_'):
                                attributes.append({
                                    'name': item.target.id,
                                    'type': self._get_name(item.annotation) if item.annotation else 'Any'
                                })
                    
                    classes.append({
                        'name': node.name,
                        'full_name': f"{module_name}.{node.name}",
                        'methods': methods,
                        'attributes': attributes
                    })
                
                elif isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                    # Only top-level functions
                    if not any(node in cls_node.body for cls_node in ast.walk(tree) if isinstance(cls_node, ast.ClassDef)):
                        if not node.name.startswith('_'):
                            # Extract comprehensive parameter info
                            params = self._extract_function_parameters(node)
                            
                            # Get return type annotation
                            return_type = self._get_name(node.returns) if node.returns else 'Any'
                            
                            # Create detailed parameter list for Neo4j storage
                            params_detailed = []
                            for p in params:
                                param_str = f"{p['name']}:{p['type']}"
                                if p['optional'] and p['default'] is not None:
                                    param_str += f"={p['default']}"
                                elif p['optional']:
                                    param_str += "=None"
                                if p['kind'] != 'positional':
                                    param_str = f"[{p['kind']}] {param_str}"
                                params_detailed.append(param_str)
                            
                            # Simple format for backwards compatibility
                            params_list = [f"{p['name']}:{p['type']}" for p in params]
                            
                            functions.append({
                                'name': node.name,
                                'full_name': f"{module_name}.{node.name}",
                                'params': params,  # Full parameter objects
                                'params_detailed': params_detailed,  # Detailed string format
                                'params_list': params_list,  # Simple string format for backwards compatibility
                                'return_type': return_type,
                                'args': [arg.arg for arg in node.args.args]  # Keep for backwards compatibility
                            })
                
                elif isinstance(node, (ast.Import, ast.ImportFrom)):
                    # Track internal imports only
                    if isinstance(node, ast.Import):
                        for alias in node.names:
                            if self._is_likely_internal(alias.name, project_modules):
                                imports.append(alias.name)
                    elif isinstance(node, ast.ImportFrom) and node.module:
                        if (node.module.startswith('.') or self._is_likely_internal(node.module, project_modules)):
                            imports.append(node.module)
            
            return {
                'module_name': module_name,
                'file_path': relative_path,
                'classes': classes,
                'functions': functions,
                'imports': list(set(imports)),  # Remove duplicates
                'line_count': len(content.splitlines())
            }
            
        except Exception as e:
            logger.warning(f"Could not analyze {file_path}: {e}")
            return None
    
    def _is_likely_internal(self, import_name: str, project_modules: Set[str]) -> bool:
        """Check if an import is likely internal to the project"""
        if not import_name:
            return False
        
        # Relative imports are definitely internal
        if import_name.startswith('.'):
            return True
        
        # Check if it's a known external module
        base_module = import_name.split('.')[0]
        if base_module in self.external_modules:
            return False
        
        # Check if it matches any project module
        for project_module in project_modules:
            if import_name.startswith(project_module):
                return True
        
        # If it's not obviously external, consider it internal
        if (not any(ext in base_module.lower() for ext in ['test', 'mock', 'fake']) and
            not base_module.startswith('_') and
            len(base_module) > 2):
            return True
        
        return False
    
    def _get_importable_module_name(self, file_path: Path, repo_root: Path, relative_path: str) -> str:
        """Determine the actual importable module name for a Python file"""
        # Start with the default: convert file path to module path
        default_module = relative_path.replace('/', '.').replace('\\', '.').replace('.py', '')
        
        # Common patterns to detect the actual package root
        path_parts = Path(relative_path).parts
        
        # Look for common package indicators
        package_roots = []
        
        # Check each directory level for __init__.py to find package boundaries
        current_path = repo_root
        for i, part in enumerate(path_parts[:-1]):  # Exclude the .py file itself
            current_path = current_path / part
            if (current_path / '__init__.py').exists():
                # This is a package directory, mark it as a potential root
                package_roots.append(i)
        
        if package_roots:
            # Use the first (outermost) package as the root
            package_start = package_roots[0]
            module_parts = path_parts[package_start:]
            module_name = '.'.join(module_parts).replace('.py', '')
            return module_name
        
        # Fallback: look for common Python project structures
        # Skip common non-package directories
        skip_dirs = {'src', 'lib', 'source', 'python', 'pkg', 'packages'}
        
        # Find the first directory that's not in skip_dirs
        filtered_parts = []
        for part in path_parts:
            if part.lower() not in skip_dirs or filtered_parts:  # Once we start including, include everything
                filtered_parts.append(part)
        
        if filtered_parts:
            module_name = '.'.join(filtered_parts).replace('.py', '')
            return module_name
        
        # Final fallback: use the default
        return default_module
    
    def _extract_function_parameters(self, func_node):
        """Comprehensive parameter extraction from function definition"""
        params = []
        
        # Regular positional arguments
        for i, arg in enumerate(func_node.args.args):
            if arg.arg == 'self':
                continue
                
            param_info = {
                'name': arg.arg,
                'type': self._get_name(arg.annotation) if arg.annotation else 'Any',
                'kind': 'positional',
                'optional': False,
                'default': None
            }
            
            # Check if this argument has a default value
            defaults_start = len(func_node.args.args) - len(func_node.args.defaults)
            if i >= defaults_start:
                default_idx = i - defaults_start
                if default_idx < len(func_node.args.defaults):
                    param_info['optional'] = True
                    param_info['default'] = self._get_default_value(func_node.args.defaults[default_idx])
            
            params.append(param_info)
        
        # *args parameter
        if func_node.args.vararg:
            params.append({
                'name': f"*{func_node.args.vararg.arg}",
                'type': self._get_name(func_node.args.vararg.annotation) if func_node.args.vararg.annotation else 'Any',
                'kind': 'var_positional',
                'optional': True,
                'default': None
            })
        
        # Keyword-only arguments (after *)
        for i, arg in enumerate(func_node.args.kwonlyargs):
            param_info = {
                'name': arg.arg,
                'type': self._get_name(arg.annotation) if arg.annotation else 'Any',
                'kind': 'keyword_only',
                'optional': True,  # All kwonly args are optional unless explicitly required
                'default': None
            }
            
            # Check for default value
            if i < len(func_node.args.kw_defaults) and func_node.args.kw_defaults[i] is not None:
                param_info['default'] = self._get_default_value(func_node.args.kw_defaults[i])
            else:
                param_info['optional'] = False  # No default = required kwonly arg
            
            params.append(param_info)
        
        # **kwargs parameter
        if func_node.args.kwarg:
            params.append({
                'name': f"**{func_node.args.kwarg.arg}",
                'type': self._get_name(func_node.args.kwarg.annotation) if func_node.args.kwarg.annotation else 'Dict[str, Any]',
                'kind': 'var_keyword',
                'optional': True,
                'default': None
            })
        
        return params
    
    def _get_default_value(self, default_node):
        """Extract default value from AST node"""
        try:
            if isinstance(default_node, ast.Constant):
                return repr(default_node.value)
            elif isinstance(default_node, ast.Name):
                return default_node.id
            elif isinstance(default_node, ast.Attribute):
                return self._get_name(default_node)
            elif isinstance(default_node, ast.List):
                return "[]"
            elif isinstance(default_node, ast.Dict):
                return "{}"
            else:
                return "..."
        except Exception:
            return "..."
    
    def _get_name(self, node):
        """Extract name from AST node, handling complex types safely"""
        if node is None:
            return "Any"
        
        try:
            if isinstance(node, ast.Name):
                return node.id
            elif isinstance(node, ast.Attribute):
                if hasattr(node, 'value'):
                    return f"{self._get_name(node.value)}.{node.attr}"
                else:
                    return node.attr
            elif isinstance(node, ast.Subscript):
                # Handle List[Type], Dict[K,V], etc.
                base = self._get_name(node.value)
                if hasattr(node, 'slice'):
                    if isinstance(node.slice, ast.Name):
                        return f"{base}[{node.slice.id}]"
                    elif isinstance(node.slice, ast.Tuple):
                        elts = [self._get_name(elt) for elt in node.slice.elts]
                        return f"{base}[{', '.join(elts)}]"
                    elif isinstance(node.slice, ast.Constant):
                        return f"{base}[{repr(node.slice.value)}]"
                    elif isinstance(node.slice, ast.Attribute):
                        return f"{base}[{self._get_name(node.slice)}]"
                    elif isinstance(node.slice, ast.Subscript):
                        return f"{base}[{self._get_name(node.slice)}]"
                    else:
                        # Try to get the name of the slice, fallback to Any if it fails
                        try:
                            slice_name = self._get_name(node.slice)
                            return f"{base}[{slice_name}]"
                        except:
                            return f"{base}[Any]"
                return base
            elif isinstance(node, ast.Constant):
                return str(node.value)
            elif isinstance(node, ast.Str):  # Python < 3.8
                return f'"{node.s}"'
            elif isinstance(node, ast.Tuple):
                elts = [self._get_name(elt) for elt in node.elts]
                return f"({', '.join(elts)})"
            elif isinstance(node, ast.List):
                elts = [self._get_name(elt) for elt in node.elts]
                return f"[{', '.join(elts)}]"
            else:
                # Fallback for complex types - return a simple string representation
                return "Any"
        except Exception:
            # If anything goes wrong, return a safe default
            return "Any"


class DirectNeo4jExtractor:
    """Creates nodes and relationships directly in Neo4j"""
    
    def __init__(self, neo4j_uri: str, neo4j_user: str, neo4j_password: str):
        self.neo4j_uri = neo4j_uri
        self.neo4j_user = neo4j_user
        self.neo4j_password = neo4j_password
        self.driver = None
        self.analyzer = Neo4jCodeAnalyzer()
    
    async def initialize(self):
        """Initialize Neo4j connection"""
        logger.info("Initializing Neo4j connection...")
        self.driver = AsyncGraphDatabase.driver(
            self.neo4j_uri, 
            auth=(self.neo4j_user, self.neo4j_password)
        )
        
        # Create constraints and indexes
        logger.info("Creating constraints and indexes...")
        async with self.driver.session() as session:
            # Create constraints - using MERGE-friendly approach
            await session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (c:Class) REQUIRE c.full_name IS UNIQUE")
            
            # Create indexes for performance
            await session.run("CREATE INDEX IF NOT EXISTS FOR (f:File) ON (f.name)")
            await session.run("CREATE INDEX IF NOT EXISTS FOR (f:File) ON (f.path)")
            await session.run("CREATE INDEX IF NOT EXISTS FOR (c:Class) ON (c.name)")
            await session.run("CREATE INDEX IF NOT EXISTS FOR (m:Method) ON (m.name)")
        
        logger.info("Neo4j initialized successfully")
    
    async def clear_repository_data(self, repo_name: str):
        """Clear all data for a specific repository"""
        logger.info(f"Clearing existing data for repository: {repo_name}")
        async with self.driver.session() as session:
            # Delete in specific order to avoid constraint issues
            
            # 1. Delete methods and attributes (they depend on classes)
            await session.run("""
                MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(c:Class)-[:HAS_METHOD]->(m:Method)
                DETACH DELETE m
            """, repo_name=repo_name)
            
            await session.run("""
                MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(c:Class)-[:HAS_ATTRIBUTE]->(a:Attribute)
                DETACH DELETE a
            """, repo_name=repo_name)
            
            # 2. Delete functions (they depend on files)
            await session.run("""
                MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(func:Function)
                DETACH DELETE func
            """, repo_name=repo_name)
            
            # 3. Delete classes (they depend on files)
            await session.run("""
                MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(c:Class)
                DETACH DELETE c
            """, repo_name=repo_name)
            
            # 4. Delete files (they depend on repository)
            await session.run("""
                MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)
                DETACH DELETE f
            """, repo_name=repo_name)
            
            # 5. Finally delete the repository
            await session.run("""
                MATCH (r:Repository {name: $repo_name})
                DETACH DELETE r
            """, repo_name=repo_name)
            
        logger.info(f"Cleared data for repository: {repo_name}")
    
    async def check_repository_version_exists(self, repo_name: str, version: str) -> bool:
        """Check if a specific repository version already exists in Neo4j"""
        async with self.driver.session() as session:
            # Check if repository exists with the specific version
            query = """
            MATCH (r:Repository {name: $repo_name, version: $version})
            RETURN count(r) as count
            """
            result = await session.run(query, repo_name=repo_name, version=version)
            record = await result.single()
            return record['count'] > 0 if record else False
    
    async def check_and_create_repository_placeholder(self, repo_name: str, version: str) -> bool:
        """Atomically check if repository exists and create placeholder if not. Returns True if created, False if already exists."""
        async with self.driver.session() as session:
            # Use MERGE with conditional creation to handle race conditions
            # We'll use a different approach: check if the node was created in this transaction
            query = """
            MERGE (r:Repository {name: $repo_name, version: $version})
            ON CREATE SET r.created_at = datetime(), r.placeholder = true
            ON MATCH SET r.placeholder = true
            RETURN r.created_at = datetime() as was_created
            """
            result = await session.run(query, repo_name=repo_name, version=version)
            record = await result.single()
            return record['was_created'] if record else False
    
    async def clear_repository_by_name(self, repo_name: str):
        """
        Thoroughly clear all data for a specific repository using both relationship-based
        and property-based deletion to ensure complete cleanup.
        """
        logger.info(f"Thoroughly clearing all data for repository: {repo_name}")
        async with self.driver.session() as session:
            # Step 1: Delete using relationship chains (your existing method)
            logger.info(f"Step 1: Deleting nodes with intact relationship chains...")
            
            # 1. Delete methods and attributes (they depend on classes)
            await session.run("""
                MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(c:Class)-[:HAS_METHOD]->(m:Method)
                DETACH DELETE m
            """, repo_name=repo_name)
            
            await session.run("""
                MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(c:Class)-[:HAS_ATTRIBUTE]->(a:Attribute)
                DETACH DELETE a
            """, repo_name=repo_name)
            
            # 2. Delete functions (they depend on files)
            await session.run("""
                MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(func:Function)
                DETACH DELETE func
            """, repo_name=repo_name)
            
            # 3. Delete classes (they depend on files)
            await session.run("""
                MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(c:Class)
                DETACH DELETE c
            """, repo_name=repo_name)
            
            # 4. Delete files (they depend on repository)
            await session.run("""
                MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)
                DETACH DELETE f
            """, repo_name=repo_name)
            
            # 5. Finally delete the repository
            await session.run("""
                MATCH (r:Repository {name: $repo_name})
                DETACH DELETE r
            """, repo_name=repo_name)
            
            # Step 2: Clean up any orphaned nodes using property matching
            logger.info(f"Step 2: Cleaning up orphaned nodes using property matching...")
            
            # Delete any remaining File nodes whose path contains repo_name
            await session.run("""
                MATCH (f:File)
                WHERE f.file_id CONTAINS $repo_name OR f.path CONTAINS $repo_name
                DETACH DELETE f
            """, repo_name=repo_name)
            
            # Delete any remaining Class nodes whose full_name contains repo_name
            await session.run("""
                MATCH (c:Class)
                WHERE c.full_name CONTAINS $repo_name
                DETACH DELETE c
            """, repo_name=repo_name)
            
            # Delete any remaining Function nodes whose full_name contains repo_name
            await session.run("""
                MATCH (func:Function)
                WHERE func.full_name CONTAINS $repo_name
                DETACH DELETE func
            """, repo_name=repo_name)
            
            # Delete any remaining Method nodes whose full_name or method_id contains repo_name
            await session.run("""
                MATCH (m:Method)
                WHERE m.full_name CONTAINS $repo_name OR (m.method_id IS NOT NULL AND m.method_id CONTAINS $repo_name)
                DETACH DELETE m
            """, repo_name=repo_name)
            
            # Delete any remaining Attribute nodes whose full_name or attr_id contains repo_name
            await session.run("""
                MATCH (a:Attribute)
                WHERE a.full_name CONTAINS $repo_name OR (a.attr_id IS NOT NULL AND a.attr_id CONTAINS $repo_name)
                DETACH DELETE a
            """, repo_name=repo_name)
            
        logger.info(f"Successfully cleared all data for repository: {repo_name} (two-step deletion)")
    
    async def clear_repository_by_name_except_version(self, repo_name: str, keep_version: str):
        """
        Clear all data for a specific repository except the specified version.
        This keeps only the specified version and removes all other versions.
        """
        logger.info(f"Clearing all data for repository: {repo_name} except version: {keep_version}")
        async with self.driver.session() as session:
            # Step 1: Delete using relationship chains for all versions except keep_version
            logger.info(f"Step 1: Deleting nodes with intact relationship chains for versions other than {keep_version}...")
            
            # 1. Delete methods and attributes for other versions
            await session.run("""
                MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(c:Class)-[:HAS_METHOD]->(m:Method)
                WHERE (r.version IS NULL AND $keep_version IS NOT NULL) OR (r.version IS NOT NULL AND r.version <> $keep_version)
                DETACH DELETE m
            """, repo_name=repo_name, keep_version=keep_version)
            
            await session.run("""
                MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(c:Class)-[:HAS_ATTRIBUTE]->(a:Attribute)
                WHERE (r.version IS NULL AND $keep_version IS NOT NULL) OR (r.version IS NOT NULL AND r.version <> $keep_version)
                DETACH DELETE a
            """, repo_name=repo_name, keep_version=keep_version)
            
            # 2. Delete functions for other versions
            await session.run("""
                MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(func:Function)
                WHERE (r.version IS NULL AND $keep_version IS NOT NULL) OR (r.version IS NOT NULL AND r.version <> $keep_version)
                DETACH DELETE func
            """, repo_name=repo_name, keep_version=keep_version)
            
            # 3. Delete classes for other versions
            await session.run("""
                MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(c:Class)
                WHERE (r.version IS NULL AND $keep_version IS NOT NULL) OR (r.version IS NOT NULL AND r.version <> $keep_version)
                DETACH DELETE c
            """, repo_name=repo_name, keep_version=keep_version)
            
            # 4. Delete files for other versions
            await session.run("""
                MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)
                WHERE (r.version IS NULL AND $keep_version IS NOT NULL) OR (r.version IS NOT NULL AND r.version <> $keep_version)
                DETACH DELETE f
            """, repo_name=repo_name, keep_version=keep_version)
            
            # 5. Delete repositories for other versions
            await session.run("""
                MATCH (r:Repository {name: $repo_name})
                WHERE (r.version IS NULL AND $keep_version IS NOT NULL) OR (r.version IS NOT NULL AND r.version <> $keep_version)
                DETACH DELETE r
            """, repo_name=repo_name, keep_version=keep_version)
            
            # Step 2: Clean up any orphaned nodes using property matching for other versions
            logger.info(f"Step 2: Cleaning up orphaned nodes for versions other than {keep_version}...")
            
            # Note: We can't easily identify orphaned nodes by version in property matching
            # So we'll rely on the relationship-based deletion above
            # The keep_version data should remain intact due to the WHERE clauses
            
        logger.info(f"Successfully cleared all data for repository: {repo_name} except version: {keep_version}")
    
    async def close(self):
        """Close Neo4j connection"""
        if self.driver:
            await self.driver.close()
    
    def clone_repo(self, repo_url: str, target_dir: str, version_tag: str = None) -> str:
        """Clone repository with shallow clone"""
        logger.info(f"Cloning repository to: {target_dir}")
        if os.path.exists(target_dir):
            logger.info(f"Removing existing directory: {target_dir}")
            try:
                def handle_remove_readonly(func, path, exc):
                    try:
                        if os.path.exists(path):
                            os.chmod(path, 0o777)
                            func(path)
                    except PermissionError:
                        logger.warning(f"Could not remove {path} - file in use, skipping")
                        pass
                shutil.rmtree(target_dir, onerror=handle_remove_readonly)
            except Exception as e:
                logger.warning(f"Could not fully remove {target_dir}: {e}. Proceeding anyway...")
        
        logger.info(f"Running git clone from {repo_url}")
        git_command = ['git', 'clone', '--depth', '1', repo_url, target_dir]
        if version_tag:
            git_command.append('-b')
            git_command.append(version_tag)
        subprocess.run(git_command, check=True)
        logger.info("Repository cloned successfully")
        return target_dir
    
    def get_python_files(self, repo_path: str) -> List[Path]:
        """Get Python files, focusing on main source directories"""
        python_files = []
        exclude_dirs = {
            'tests', 'test', '__pycache__', '.git', 'venv', 'env',
            'node_modules', 'build', 'dist', '.pytest_cache', 'docs',
            'examples', 'example', 'demo', 'benchmark'
        }
        
        for root, dirs, files in os.walk(repo_path):
            dirs[:] = [d for d in dirs if d not in exclude_dirs and not d.startswith('.')]
            
            for file in files:
                if file.endswith('.py') and not file.startswith('test_'):
                    file_path = Path(root) / file
                    if (file_path.stat().st_size < 500_000 and 
                        file not in ['setup.py', 'conftest.py']):
                        python_files.append(file_path)
        
        return python_files
    
    async def analyze_repository(self, repo_url: str, version_tag: str = None, temp_dir: str = None):
        """Analyze repository and create nodes/relationships in Neo4j"""
        repo_name = repo_url.split('/')[-1].replace('.git', '')
        logger.info(f"Analyzing repository: {repo_name}")
        
        # Check if this specific version already exists
        if version_tag:
            exists = await self.check_repository_version_exists(repo_name, version_tag)
            if exists:
                logger.info(f"Repository {repo_name} version {version_tag} already exists in Neo4j, clearing other versions...")
                # Clear other versions but keep the current one
                await self.clear_repository_by_name_except_version(repo_name, version_tag)
                return {
                    "success": True,
                    "message": f"Repository {repo_name} version {version_tag} already exists, other versions cleared",
                    "skipped": True
                }
            else:
                logger.info(f"Repository {repo_name} version {version_tag} does not exist, clearing all versions and parsing...")
                # Clear all versions and parse the new one
                await self.clear_repository_by_name(repo_name)
        else:
            # If no version specified, clear existing data before re-processing
            logger.info(f"No version specified for {repo_name}, clearing existing data...")
            await self.clear_repository_by_name(repo_name)
        
        # Set default temp_dir to repos folder at script level
        if temp_dir is None:
            script_dir = Path(__file__).parent
            temp_dir = str(script_dir / "repos" / repo_name)
        
        # Clone and analyze
        repo_path = Path(self.clone_repo(repo_url, temp_dir, version_tag))
        
        try:
            logger.info("Getting Python files...")
            python_files = self.get_python_files(str(repo_path))
            logger.info(f"Found {len(python_files)} Python files to analyze")
            
            # First pass: identify project modules
            logger.info("Identifying project modules...")
            project_modules = set()
            for file_path in python_files:
                relative_path = str(file_path.relative_to(repo_path))
                module_parts = relative_path.replace('/', '.').replace('.py', '').split('.')
                if len(module_parts) > 0 and not module_parts[0].startswith('.'):
                    project_modules.add(module_parts[0])
            
            logger.info(f"Identified project modules: {sorted(project_modules)}")
            
            # Second pass: analyze files and collect data
            logger.info("Analyzing Python files...")
            modules_data = []
            for i, file_path in enumerate(python_files):
                if i % 20 == 0:
                    logger.info(f"Analyzing file {i+1}/{len(python_files)}: {file_path.name}")
                
                analysis = self.analyzer.analyze_python_file(file_path, repo_path, project_modules)
                if analysis:
                    modules_data.append(analysis)
            
            logger.info(f"Found {len(modules_data)} files with content")
            
            # Create nodes and relationships in Neo4j
            logger.info("Creating nodes and relationships in Neo4j...")
            await self._create_graph(repo_name, modules_data, version_tag)
            
            # Print summary
            total_classes = sum(len(mod['classes']) for mod in modules_data)
            total_methods = sum(len(cls['methods']) for mod in modules_data for cls in mod['classes'])
            total_functions = sum(len(mod['functions']) for mod in modules_data)
            total_imports = sum(len(mod['imports']) for mod in modules_data)
            
            print(f"\n=== Direct Neo4j Repository Analysis for {repo_name} ===")
            if version_tag:
                print(f"Version: {version_tag}")
            print(f"Files processed: {len(modules_data)}")
            print(f"Classes created: {total_classes}")
            print(f"Methods created: {total_methods}")
            print(f"Functions created: {total_functions}")
            print(f"Import relationships: {total_imports}")
            
            logger.info(f"Successfully created Neo4j graph for {repo_name}")
            
            return {
                "success": True,
                "repo_name": repo_name,
                "version": version_tag,
                "files_processed": len(modules_data),
                "classes_created": total_classes,
                "methods_created": total_methods,
                "functions_created": total_functions,
                "imports_created": total_imports
            }
            
        finally:
            if os.path.exists(temp_dir):
                logger.info(f"Cleaning up temporary directory: {temp_dir}")
                try:
                    def handle_remove_readonly(func, path, exc):
                        try:
                            if os.path.exists(path):
                                os.chmod(path, 0o777)
                                func(path)
                        except PermissionError:
                            logger.warning(f"Could not remove {path} - file in use, skipping")
                            pass
                    
                    shutil.rmtree(temp_dir, onerror=handle_remove_readonly)
                    logger.info("Cleanup completed")
                except Exception as e:
                    logger.warning(f"Cleanup failed: {e}. Directory may remain at {temp_dir}")
                    # Don't fail the whole process due to cleanup issues

    async def analyze_local_package(self, package_name: str, package_path: str, version: str = None):
        """Analyze local package and create nodes/relationships in Neo4j"""
        logger.info(f"Analyzing local package: {package_name} at {package_path}")
        
        # Use a more robust check-and-create approach to prevent race conditions
        if version:
            # Try to atomically check and reserve the repository
            created = await self.check_and_create_repository_placeholder(package_name, version)
            if not created:
                logger.info(f"Package {package_name} version {version} already exists in Neo4j, clearing other versions...")
                # Clear other versions but keep the current one
                await self.clear_repository_by_name_except_version(package_name, version)
                return {
                    "success": True,
                    "message": f"Package {package_name} version {version} already exists, other versions cleared",
                    "skipped": True
                }
            else:
                logger.info(f"Package {package_name} version {version} reserved for parsing, clearing other versions...")
                # Clear other versions (but not this one we just created)
                await self.clear_repository_by_name_except_version(package_name, version)
        else:
            # If no version specified, clear existing data before re-processing
            logger.info(f"No version specified for {package_name}, clearing existing data...")
            await self.clear_repository_by_name(package_name)
        
        try:
            repo_path = Path(package_path)
            if not repo_path.exists():
                raise FileNotFoundError(f"Package path does not exist: {package_path}")
            
            logger.info("Getting Python files...")
            python_files = self.get_python_files(str(repo_path))
            logger.info(f"Found {len(python_files)} Python files to analyze")
            
            # First pass: identify project modules
            logger.info("Identifying project modules...")
            project_modules = set()
            for file_path in python_files:
                relative_path = str(file_path.relative_to(repo_path))
                module_parts = relative_path.replace('/', '.').replace('.py', '').split('.')
                if len(module_parts) > 0 and not module_parts[0].startswith('.'):
                    project_modules.add(module_parts[0])
            
            logger.info(f"Identified project modules: {sorted(project_modules)}")
            
            # Second pass: analyze files and collect data
            logger.info("Analyzing Python files...")
            modules_data = []
            for i, file_path in enumerate(python_files):
                if i % 20 == 0:
                    logger.info(f"Analyzing file {i+1}/{len(python_files)}: {file_path.name}")
                
                analysis = self.analyzer.analyze_python_file(file_path, repo_path, project_modules)
                if analysis:
                    modules_data.append(analysis)
            
            logger.info(f"Found {len(modules_data)} files with content")
            
            # Create nodes and relationships in Neo4j
            logger.info("Creating nodes and relationships in Neo4j...")
            await self._create_graph(package_name, modules_data, version)
            
            # Print summary
            total_classes = sum(len(mod['classes']) for mod in modules_data)
            total_methods = sum(len(cls['methods']) for mod in modules_data for cls in mod['classes'])
            total_functions = sum(len(mod['functions']) for mod in modules_data)
            total_imports = sum(len(mod['imports']) for mod in modules_data)
            
            # print(f"\n=== Direct Neo4j Local Package Analysis for {package_name} ===")
            # print(f"Package path: {package_path}")
            # if version:
            #     print(f"Version: {version}")
            # print(f"Files processed: {len(modules_data)}")
            # print(f"Classes created: {total_classes}")
            # print(f"Methods created: {total_methods}")
            # print(f"Functions created: {total_functions}")
            # print(f"Import relationships: {total_imports}")
            
            logger.info(f"Successfully created Neo4j graph for local package {package_name}")
            
            return {
                "success": True,
                "package_name": package_name,
                "version": version,
                "files_processed": len(modules_data),
                "classes_created": total_classes,
                "methods_created": total_methods,
                "functions_created": total_functions,
                "imports_created": total_imports
            }
            
        except Exception as e:
            logger.error(f"Failed to analyze local package {package_name}: {e}")
            raise
    
    async def _create_graph(self, repo_name: str, modules_data: List[Dict], version: str = None):
        """Create all nodes and relationships in Neo4j"""
        
        async with self.driver.session() as session:
            # Update Repository node to remove placeholder status and ensure proper setup
            if version is not None:
                await session.run("""
                    MERGE (r:Repository {name: $repo_name, version: $version})
                    ON CREATE SET r.created_at = datetime()
                    REMOVE r.placeholder
                """, repo_name=repo_name, version=version)
            else:
                await session.run("""
                    MERGE (r:Repository {name: $repo_name})
                    ON CREATE SET r.created_at = datetime()
                    SET r.version = NULL
                    REMOVE r.placeholder
                """, repo_name=repo_name)
            
            nodes_created = 0
            relationships_created = 0
            
            for i, mod in enumerate(modules_data):
                # 1. Create File node with unique identifier
                file_id = f"{repo_name}:{mod['file_path']}"
                await session.run("""
                    MERGE (f:File {file_id: $file_id})
                    ON CREATE SET f.name = $name,
                                 f.path = $path,
                                 f.module_name = $module_name,
                                 f.line_count = $line_count,
                                 f.created_at = datetime()
                """, 
                    file_id=file_id,
                    name=mod['file_path'].split('/')[-1],
                    path=mod['file_path'],
                    module_name=mod['module_name'],
                    line_count=mod['line_count']
                )
                nodes_created += 1
                
                # 2. Connect File to Repository - ensure version-specific connection
                if version is not None:
                    await session.run("""
                        MATCH (r:Repository {name: $repo_name, version: $version})
                        MATCH (f:File {file_id: $file_id})
                        MERGE (r)-[:CONTAINS]->(f)
                    """, repo_name=repo_name, version=version, file_id=file_id)
                else:
                    await session.run("""
                        MATCH (r:Repository {name: $repo_name})
                        WHERE r.version IS NULL
                        MATCH (f:File {file_id: $file_id})
                        MERGE (r)-[:CONTAINS]->(f)
                    """, repo_name=repo_name, file_id=file_id)
                relationships_created += 1
                
                # 3. Create Class nodes and relationships
                for cls in mod['classes']:
                    # Create Class node using MERGE to avoid duplicates
                    await session.run("""
                        MERGE (c:Class {full_name: $full_name})
                        ON CREATE SET c.name = $name, c.created_at = datetime()
                    """, name=cls['name'], full_name=cls['full_name'])
                    nodes_created += 1
                    
                    # Connect File to Class
                    await session.run("""
                        MATCH (f:File {file_id: $file_id})
                        MATCH (c:Class {full_name: $class_full_name})
                        MERGE (f)-[:DEFINES]->(c)
                    """, file_id=file_id, class_full_name=cls['full_name'])
                    relationships_created += 1
                    
                    # 4. Create Method nodes - use MERGE to avoid duplicates
                    for method in cls['methods']:
                        method_full_name = f"{cls['full_name']}.{method['name']}"
                        # Create method with unique ID to avoid conflicts
                        method_id = f"{cls['full_name']}::{method['name']}"
                        
                        await session.run("""
                            MERGE (m:Method {method_id: $method_id})
                            ON CREATE SET m.name = $name, 
                                         m.full_name = $full_name,
                                         m.args = $args,
                                         m.params_list = $params_list,
                                         m.params_detailed = $params_detailed,
                                         m.return_type = $return_type,
                                         m.created_at = datetime()
                        """, 
                            name=method['name'], 
                            full_name=method_full_name,
                            method_id=method_id,
                            args=method['args'],
                            params_list=[f"{p['name']}:{p['type']}" for p in method['params']],  # Simple format
                            params_detailed=method.get('params_detailed', []),  # Detailed format
                            return_type=method['return_type']
                        )
                        nodes_created += 1
                        
                        # Connect Class to Method
                        await session.run("""
                            MATCH (c:Class {full_name: $class_full_name})
                            MATCH (m:Method {method_id: $method_id})
                            MERGE (c)-[:HAS_METHOD]->(m)
                        """, 
                            class_full_name=cls['full_name'], 
                            method_id=method_id
                        )
                        relationships_created += 1
                    
                    # 5. Create Attribute nodes - use MERGE to avoid duplicates
                    for attr in cls['attributes']:
                        attr_full_name = f"{cls['full_name']}.{attr['name']}"
                        # Create attribute with unique ID to avoid conflicts
                        attr_id = f"{cls['full_name']}::{attr['name']}"
                        await session.run("""
                            MERGE (a:Attribute {attr_id: $attr_id})
                            ON CREATE SET a.name = $name,
                                         a.full_name = $full_name,
                                         a.type = $type,
                                         a.created_at = datetime()
                        """, 
                            name=attr['name'], 
                            full_name=attr_full_name,
                            attr_id=attr_id,
                            type=attr['type']
                        )
                        nodes_created += 1
                        
                        # Connect Class to Attribute
                        await session.run("""
                            MATCH (c:Class {full_name: $class_full_name})
                            MATCH (a:Attribute {attr_id: $attr_id})
                            MERGE (c)-[:HAS_ATTRIBUTE]->(a)
                        """, 
                            class_full_name=cls['full_name'], 
                            attr_id=attr_id
                        )
                        relationships_created += 1
                
                # 6. Create Function nodes (top-level) - use MERGE to avoid duplicates
                for func in mod['functions']:
                    func_id = f"{mod['file_path']}::{func['name']}"
                    await session.run("""
                        MERGE (f:Function {func_id: $func_id})
                        ON CREATE SET f.name = $name,
                                     f.full_name = $full_name,
                                     f.args = $args,
                                     f.params_list = $params_list,
                                     f.params_detailed = $params_detailed,
                                     f.return_type = $return_type,
                                     f.created_at = datetime()
                    """, 
                        name=func['name'], 
                        full_name=func['full_name'],
                        func_id=func_id,
                        args=func['args'],
                        params_list=func.get('params_list', []),  # Simple format for backwards compatibility
                        params_detailed=func.get('params_detailed', []),  # Detailed format
                        return_type=func['return_type']
                    )
                    nodes_created += 1
                    
                    # Connect File to Function
                    await session.run("""
                        MATCH (file:File {file_id: $file_id})
                        MATCH (func:Function {func_id: $func_id})
                        MERGE (file)-[:DEFINES]->(func)
                    """, file_id=file_id, func_id=func_id)
                    relationships_created += 1
                
                # 7. Create Import relationships
                for import_name in mod['imports']:
                    # Try to find the target file
                    await session.run("""
                        MATCH (source:File {file_id: $source_file_id})
                        OPTIONAL MATCH (target:File) 
                        WHERE target.module_name = $import_name OR target.module_name STARTS WITH $import_name
                        WITH source, target
                        WHERE target IS NOT NULL
                        MERGE (source)-[:IMPORTS]->(target)
                    """, source_file_id=file_id, import_name=import_name)
                    relationships_created += 1
                
                if (i + 1) % 10 == 0:
                    logger.info(f"Processed {i + 1}/{len(modules_data)} files...")
            
            logger.info(f"Created {nodes_created} nodes and {relationships_created} relationships")
    
    async def search_graph(self, query_type: str, **kwargs):
        """Search the Neo4j graph directly"""
        async with self.driver.session() as session:
            if query_type == "files_importing":
                target = kwargs.get('target')
                result = await session.run("""
                    MATCH (source:File)-[:IMPORTS]->(target:File)
                    WHERE target.module_name CONTAINS $target
                    RETURN source.path as file, target.module_name as imports
                """, target=target)
                return [{"file": record["file"], "imports": record["imports"]} async for record in result]
            
            elif query_type == "classes_in_file":
                file_path = kwargs.get('file_path')
                result = await session.run("""
                    MATCH (f:File {path: $file_path})-[:DEFINES]->(c:Class)
                    RETURN c.name as class_name, c.full_name as full_name
                """, file_path=file_path)
                return [{"class_name": record["class_name"], "full_name": record["full_name"]} async for record in result]
            
            elif query_type == "methods_of_class":
                class_name = kwargs.get('class_name')
                result = await session.run("""
                    MATCH (c:Class)-[:HAS_METHOD]->(m:Method)
                    WHERE c.name CONTAINS $class_name OR c.full_name CONTAINS $class_name
                    RETURN m.name as method_name, m.args as args
                """, class_name=class_name)
                return [{"method_name": record["method_name"], "args": record["args"]} async for record in result]


async def main():
    """Example usage"""
    load_dotenv()
    
    neo4j_uri = os.environ.get('NEO4J_URI', 'bolt://localhost:7687')
    neo4j_user = os.environ.get('NEO4J_USER', 'neo4j')
    neo4j_password = os.environ.get('NEO4J_PASSWORD', 'password')
    
    extractor = DirectNeo4jExtractor(neo4j_uri, neo4j_user, neo4j_password)
    
    try:
        await extractor.initialize()
        
        # Analyze repository - direct Neo4j, no LLM processing!
        # repo_url = "https://github.com/pydantic/pydantic-ai.git"
        repo_url = "https://github.com/instadeepai/mlip.git"
        await extractor.analyze_repository(repo_url)
        
        # Direct graph queries
        print("\n=== Direct Neo4j Queries ===")
        
        # Which files import from models?
        results = await extractor.search_graph("files_importing", target="models")
        print(f"\nFiles importing from 'models': {len(results)}")
        for result in results[:3]:
            print(f"- {result['file']} imports {result['imports']}")
        
        # What classes are in a specific file?
        results = await extractor.search_graph("classes_in_file", file_path="mlip/models/mace.py")
        print(f"\nClasses in mace.py: {len(results)}")
        for result in results:
            print(f"- {result['class_name']}")
        
        # What methods does MACE have?
        results = await extractor.search_graph("methods_of_class", class_name="MACE")
        print(f"\nMethods of MACE: {len(results)}")
        for result in results[:5]:
            print(f"- {result['method_name']}({', '.join(result['args'])})")
    
    finally:
        await extractor.close()


if __name__ == "__main__":
    asyncio.run(main())
```


================================================================================
=== FILE: mcp_servers_and_tools\research_server\src\research_mcp.py ===
================================================================================

```python
"""
Research Server
A specialized MCP server designed for comprehensive code research and analysis workflows.
Built to empower AI agents and coding assistants with powerful research capabilities.
Core Capabilities:
- Smart code extraction from web sources with caching
- Comprehensive code search and discovery
- Deep package and module introspection
- Runtime code analysis and exploration
- Local repository analysis and parsing
- Knowledge graph-powered code understanding
This server provides a complete research toolkit for understanding codebases,
discovering implementation patterns, and accelerating development workflows.
"""
from mcp.server.fastmcp import FastMCP, Context
from contextlib import asynccontextmanager
from collections.abc import AsyncIterator
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
from urllib.parse import urlparse, urldefrag
from xml.etree import ElementTree
from dotenv import load_dotenv
from supabase import Client
from pathlib import Path
import requests
import asyncio
import json
import os
import sysconfig as _sc_doc
import site as _site_doc
import re
import sys
import time
import traceback
import logging
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, MemoryAdaptiveDispatcher

# Configure logging to stderr (MCP uses stdout for JSON-RPC)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    stream=sys.stderr
)
logger = logging.getLogger("research_mcp")

# Compute a site-packages hint for docstrings
try:
    _purelib_doc = _sc_doc.get_paths().get("purelib") or ""
    _sitepkgs_doc = []
    try:
        _sitepkgs_doc = _site_doc.getsitepackages() or []
    except Exception:
        _sitepkgs_doc = []
    SITE_PACKAGES_HINT = _purelib_doc or (_sitepkgs_doc[0] if _sitepkgs_doc else "<site-packages>")
except Exception:
    SITE_PACKAGES_HINT = "<site-packages>"

# Add knowledge_graphs and introspection_and_probe folders to path for importing local modules
project_base = Path(__file__).resolve().parent.parent
knowledge_graphs_path = project_base / 'knowledge_graphs'
probe_path = project_base / 'introspection_and_probe'
sys.path.append(str(knowledge_graphs_path))
sys.path.append(str(probe_path))

from research_server_utils import (
    get_supabase_client,
    extract_code_blocks,
    generate_code_example_summary,
    check_extracted_code_exists,
    save_extracted_code_to_supabase,
    get_extracted_code_from_supabase,
    search_code_blocks,
    detect_content_type_and_source,
    extract_readthedocs_code_blocks,
    extract_markdown_code_blocks,
    extract_command_examples,
    extract_smart_context_before,
    extract_smart_context_after,
    extract_generic_html_code_blocks,
    extract_github_html_code_blocks,
    extract_mkdocs_code_blocks,
    extract_jupyter_notebook_cells
)
from parse_repo_into_neo4j import DirectNeo4jExtractor
from quick_introspect_core import run_quick_introspect
from runtime_probe import show_all_keys_or_attrs as rp_show_all, try_get_key as rp_try_get_key, try_get_attr as rp_try_get_attr

# Load environment variables from the project root .env file
# Priority: .env file > shell environment (.bashrc) > code defaults
project_root = Path(__file__).resolve().parent.parent
dotenv_path = project_root / '.env'
load_dotenv(dotenv_path, override=True)

# Research Server Context
@dataclass
class ResearchContext:
    """
    Central context for the Research Server.
    Manages all core resources including web crawling, database connections,
    and knowledge graph components for comprehensive research workflows.
    """
    crawler: AsyncWebCrawler
    supabase_client: Client
    repo_extractor: Optional[Any] = None  # DirectNeo4jExtractor for knowledge graphs

@asynccontextmanager
async def research_lifespan(server: FastMCP) -> AsyncIterator[ResearchContext]:
    """
    Research Server Lifecycle Manager
    Initializes and manages all core components of the Research Server including:
    - Web crawler for content extraction
    - Database connections for code storage
    - Knowledge graph components for analysis
    Args:
        server: The Research Server FastMCP instance
    Yields:
        ResearchContext: Complete research context with all initialized components
    """
    # Browser configuration setup with retry mechanism
    browser_config = BrowserConfig(
        headless=True,
        verbose=False
    )
    
    # Initialize the crawler with retry mechanism
    crawler = None
    max_retries = 3
    for attempt in range(max_retries):
        try:
            crawler = AsyncWebCrawler(config=browser_config)
            await crawler.__aenter__()
            break  # Success, exit retry loop
        except Exception as e:
            if attempt == max_retries - 1:
                # Final attempt failed
                raise Exception(f"Failed to start web crawler after {max_retries} attempts. This may be due to browser resource conflicts. Error: {str(e)}")
            else:
                # Wait before retry
                await asyncio.sleep(1)
                if crawler:
                    try:
                        await crawler.__aexit__(None, None, None)
                    except:
                        pass  # Ignore cleanup errors
                    crawler = None
    # Initialize Supabase client
    supabase_client = get_supabase_client()
    # Initialize Neo4j components if configured and enabled
    repo_extractor = None
    # Check if knowledge graph functionality is enabled
    knowledge_graph_enabled = os.getenv("USE_KNOWLEDGE_GRAPH", "false") == "true"
    if knowledge_graph_enabled:
        neo4j_uri = os.getenv("NEO4J_URI")
        neo4j_user = os.getenv("NEO4J_USER")
        neo4j_password = os.getenv("NEO4J_PASSWORD")
        if neo4j_uri and neo4j_user and neo4j_password:
            try:
                repo_extractor = DirectNeo4jExtractor(neo4j_uri, neo4j_user, neo4j_password)
                await repo_extractor.initialize()
                # print("âœ“ Neo4j knowledge graph extractor initialized")
            except Exception as e:
                logger.error(f"Failed to initialize Neo4j extractor: {e}")
                repo_extractor = None
        else:
            logger.warning("Neo4j credentials not configured - knowledge graph tools will be unavailable")
    else:
        logger.info("Knowledge graph functionality disabled - set USE_KNOWLEDGE_GRAPH=true to enable")
    try:
        yield ResearchContext(
            crawler=crawler,
            supabase_client=supabase_client,
            repo_extractor=repo_extractor
        )
    finally:
        # Clean up components
        if crawler:
            try:
                await crawler.__aexit__(None, None, None)
            except Exception as e:
                logger.error(f"Error closing crawler: {e}")
        if repo_extractor:
            try:
                await repo_extractor.close()
                logger.info("Repository extractor closed")
            except Exception as e:
                logger.error(f"Error closing repository extractor: {e}")

# Research Server - FastMCP Application
mcp = FastMCP(
    "mcp_servers_and_tools/research_server",
    lifespan=research_lifespan,
    host=os.getenv("HOST", "127.0.0.1"),
    port=int(os.getenv("PORT", "8052"))
)


def is_sitemap(url: str) -> bool:
    """
    Check if a URL is a sitemap.
    Args:
        url: URL to check
    Returns:
        True if the URL is a sitemap, False otherwise
    """
    return url.endswith('sitemap.xml') or 'sitemap' in urlparse(url).path

def is_txt(url: str) -> bool:
    """
    Check if a URL is a text file.
    Args:
        url: URL to check
    Returns:
        True if the URL is a text file, False otherwise
    """
    return url.endswith('.txt')

def parse_sitemap(sitemap_url: str) -> List[str]:
    """
    Parse a sitemap and extract URLs.
    Args:
        sitemap_url: URL of the sitemap
    Returns:
        List of URLs found in the sitemap
    """
    resp = requests.get(sitemap_url)
    urls = []
    if resp.status_code == 200:
        try:
            tree = ElementTree.fromstring(resp.content)
            urls = [loc.text for loc in tree.findall('.//{*}loc')]
        except Exception as e:
            logger.error(f"Error parsing sitemap XML: {e}")
    return urls

def smart_chunk_markdown(text: str, chunk_size: int = 5000) -> List[str]:
    """Split text into chunks, respecting code blocks and paragraphs."""
    chunks = []
    start = 0
    text_length = len(text)
    while start < text_length:
        # Calculate end position
        end = start + chunk_size
        # If we're at the end of the text, just take what's left
        if end >= text_length:
            chunks.append(text[start:].strip())
            break
        # Try to find a code block boundary first (```)
        chunk = text[start:end]
        code_block = chunk.rfind('```')
        if code_block != -1 and code_block > chunk_size * 0.3:
            end = start + code_block
        # If no code block, try to break at a paragraph
        elif '\n\n' in chunk:
            # Find the last paragraph break
            last_break = chunk.rfind('\n\n')
            if last_break > chunk_size * 0.3:  # Only break if we're past 30% of chunk_size
                end = start + last_break
        # If no paragraph break, try to break at a sentence
        elif '. ' in chunk:
            # Find the last sentence break
            last_period = chunk.rfind('. ')
            if last_period > chunk_size * 0.3:  # Only break if we're past 30% of chunk_size
                end = start + last_period + 1
        # Extract chunk and clean it up
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        # Move start position for next chunk
        start = end
    return chunks

def github_blob_to_raw(url: str) -> str:
    """
    Convert a GitHub blob URL to the corresponding raw URL if applicable.
    For example:
    https://github.com/user/repo/blob/branch/path/to/file.md
    ->
    https://raw.githubusercontent.com/user/repo/branch/path/to/file.md
    If the URL is not a GitHub blob URL, return it unchanged.
    """
    if not url:
        return url
    # Check if already converted
    if 'raw.githubusercontent.com' in url:
        return url
    m = re.match(r'https://github.com/([^/]+)/([^/]+)/blob/([^#?]+)', url)
    if m:
        user, repo, path = m.groups()
        return f'https://raw.githubusercontent.com/{user}/{repo}/{path}'
    return url

def validate_and_normalize_url(url: str) -> tuple[str, bool]:
    """
    Validate and normalize a URL for processing.
    Handles:
    - view-source: URLs (converts to regular URLs)
    - GitHub blob URLs (converts to raw URLs)
    - Regular URLs (validates and returns as-is)
    Returns:
        Tuple of (normalized_url, is_valid)
    """
    if not url:
        return url, False
    try:
        # Handle view-source: URLs
        if url.startswith('view-source:'):
            # Remove view-source: prefix
            url = url[12:]  # len('view-source:') = 12
        # Basic URL validation
        parsed = urlparse(url)
        if not parsed.scheme or not parsed.netloc:
            return url, False
        # Normalize the URL (GitHub blob to raw, etc.)
        normalized = github_blob_to_raw(url)
        return normalized, True
    except Exception:
        return url, False


@mcp.tool()
async def extract_code_from_url(ctx: Context, url: str = None, urls: list = None) -> str:
    """
    Extract code examples and commands from one or more URLs for immediate use by agents.
    This tool uses a caching strategy: first checks if code has already been extracted from the URL(s),
    and if not, performs extraction and stores the results in Supabase for future use.
    
    Extraction strategy:
    1. Check cache first
    2. Try single page extraction with optimized strategy:
       - HTML extraction only for ReadTheDocs/Sphinx
       - Markdown extraction for other content types
       - Special handling for Jupyter notebooks (JSON extraction)
    3. If no code blocks found, fallback to smart crawl
    
    If a list of URLs is provided, all will be processed and results merged.
    Each code block in the result will include a 'source_url' key indicating its origin.
    
    Args:
        ctx: The MCP server provided context
        url: Single URL to extract code from 
        urls: List of URLs to extract code from
        
    Returns:
        JSON string with extracted code examples, commands, and content summary.
        Key fields to focus on:
        - 'code': The extracted code content
        - 'context_before': Text content before the code block
        - 'context_after': Text content after the code block
        - 'source_url': URL where the code was extracted from
        - 'summary': Code summary (often an empty string unless language model summary is enabled)
    """
    # Set timeout for the entire extraction process (100 seconds)
    EXTRACTION_TIMEOUT = 100
    
    try:
        start_time = time.time()
        
        # Wrap the entire function in a timeout
        async def extract_with_timeout():
            return await _extract_code_from_url_internal(ctx, url, urls)
        
        try:
            result = await asyncio.wait_for(extract_with_timeout(), timeout=EXTRACTION_TIMEOUT)
            elapsed_time = time.time() - start_time
            # print(f"âœ… Extraction completed in {elapsed_time:.2f} seconds")
            return result
        except asyncio.TimeoutError:
            elapsed_time = time.time() - start_time
            logger.warning(f"Extraction timed out after {elapsed_time:.2f} seconds")
            
            # Return a friendly error message instead of crashing
            timeout_message = {
                "success": False,
                "error": f"Extraction timed out after {EXTRACTION_TIMEOUT} seconds",
                "error_type": "timeout",
                "suggestion": "This webpage may not exist, or extraction is taking too long. Do NOT try to extract code from this URL again. Please try:",
                "alternatives": [
                    "Use a different related webpage for code extraction",
                    "Use tavily search to find information about this webpage if the URL exists"
                ],
                "url": url if url else (urls[0] if urls else "unknown"),
                "extracted_code": [],
                "code_blocks_found": 0
            }
            
            if urls:
                # For multiple URLs, return timeout info for all
                timeout_message["urls"] = urls
                timeout_message["per_url_results"] = [
                    {
                        "success": False,
                        "url": u,
                        "error": f"Extraction timed out after {EXTRACTION_TIMEOUT} seconds",
                        "error_type": "timeout",
                        "suggestion": "Try alternative methods or different URLs"
                    } for u in urls
                ]
                timeout_message["summary"] = {
                    "total_unique_urls": len(urls),
                    "successful_extractions": 0,
                    "timeout_urls": len(urls),
                    "total_code_blocks_found": 0
                }
            
            return json.dumps(timeout_message, indent=2)
            
    except Exception as e:
        # print(f"Error in extract_code_from_url: {traceback.format_exc()}")
        return json.dumps({
            "success": False,
            "error": str(e),
            "error_type": "exception"
        }, indent=2)


async def _extract_code_from_url_internal(ctx: Context, url: str = None, urls: list = None) -> str:
    """
    Internal implementation of extract_code_from_url without timeout wrapper.
    This is the original function logic.
    """
    try:
        supabase_client = ctx.request_context.lifespan_context.supabase_client
        async def process_one_url(url):
            """Process a single URL with caching and fallback strategy"""
            url, is_valid = validate_and_normalize_url(url)
            if not is_valid:
                return {
                    "success": False,
                    "url": url,
                    "error": "Invalid URL provided"
                }
            # 1. Check cache first
            if await check_extracted_code_exists(supabase_client, url):
                # print(f"ðŸ“‹ Found cached extracted code for {url}")
                cached_code_blocks = await get_extracted_code_from_supabase(supabase_client, url)
                if cached_code_blocks:
                    for block in cached_code_blocks:
                        block["source_url"] = url
                    return {
                        "success": True,
                        "url": url,
                        "code_blocks_found": len(cached_code_blocks),
                        "extracted_code": cached_code_blocks,
                        "extraction_method": "cached",
                        "cached": True
                    }
            # 2. Try single page extraction
            # print(f"ðŸ”„ Extracting code from {url}...")
            crawler = ctx.request_context.lifespan_context.crawler
            single_page_result = await _extract_code_single_page(crawler, url)
            try:
                result_data = json.loads(single_page_result)
                # If single page crawl succeeded
                if result_data.get("success", False):
                    extracted_code = result_data.get("extracted_code", [])
                    # Add source_url to each code block
                    for block in extracted_code:
                        block["source_url"] = url
                    # Check if we found any code blocks
                    if result_data.get("code_blocks_found", 0) > 0:
                        # Found code blocks - save and return
                        await save_extracted_code_to_supabase(
                            supabase_client, 
                            url, 
                            extracted_code, 
                            result_data.get("code_blocks_found", 0),
                            "single_page"
                        )
                        result_data["cached"] = False
                        return result_data
                    else:
                        # No code blocks found - try smart crawl as fallback
                        # print(f"ðŸ“ No code blocks found with single page, trying smart crawl for {url}...")
                        pass
                else:
                    # Single page crawl failed - try smart crawl
                    # print(f"âŒ Single page crawl failed for {url}, trying smart crawl...")
                    pass
            except (json.JSONDecodeError, KeyError) as e:
                # print(f"Error parsing single page result: {e}")
                pass
            # 3. Fallback to smart crawl (either no code found or crawl failed)
            smart_result = await _extract_code_smart_crawl(crawler, url)
            try:
                smart_data = json.loads(smart_result)
                extracted_code = smart_data.get("extracted_code", [])
                # Add source_url to each code block
                for block in extracted_code:
                    block["source_url"] = url
                # Save to cache
                await save_extracted_code_to_supabase(
                    supabase_client, 
                    url, 
                    extracted_code, 
                    smart_data.get("code_blocks_found", 0),
                    "smart_crawl"
                )
                smart_data["cached"] = False
                return smart_data
            except Exception as e:
                return {
                    "success": False, 
                    "url": url, 
                    "error": f"Both single page and smart crawl failed: {str(e)}"
                }
        # Handle multiple URLs (only if urls is provided and not empty)
        if urls is not None and len(urls) > 0:
            # Special case: if only one URL provided, treat it like single URL for consistent output
            if len(urls) == 1:
                result = await process_one_url(urls[0])
                return json.dumps(result, indent=2)
            
            # Remove duplicates while preserving order
            unique_urls = []
            seen_urls = set()
            duplicate_urls = []
            for u in urls:
                if u in seen_urls:
                    duplicate_urls.append(u)
                else:
                    unique_urls.append(u)
                    seen_urls.add(u)
            
            # After deduplication, check if we ended up with only one URL
            if len(unique_urls) == 1:
                result = await process_one_url(unique_urls[0])
                return json.dumps(result, indent=2)
            
            # Process multiple unique URLs
            all_results = []
            all_code_blocks = []
            cached_count = 0
            extracted_count = 0
            for u in unique_urls:
                result = await process_one_url(u)
                all_results.append(result)
                if result.get("success") and result.get("extracted_code"):
                    all_code_blocks.extend(result["extracted_code"])
                    if result.get("cached", False):
                        cached_count += 1
                    else:
                        extracted_count += 1
            # Create summary
            summary_info = {
                "cached_urls": cached_count,
                "newly_extracted_urls": extracted_count,
                "total_unique_urls": len(unique_urls),
                "total_original_urls": len(urls),
                "duplicate_urls_skipped": len(duplicate_urls)
            }
            if duplicate_urls:
                summary_info["duplicate_urls"] = duplicate_urls
            return json.dumps({
                "success": True,
                "urls": urls,
                "unique_urls": unique_urls,
                "total_code_blocks_found": len(all_code_blocks),
                "all_extracted_code": all_code_blocks,
                "per_url_results": all_results,
                "summary": summary_info
            }, indent=2)
        # Handle single URL
        elif url is not None:
            result = await process_one_url(url)
            return json.dumps(result, indent=2)
        # No URL provided
        else:
            return json.dumps({
                "success": False,
                "error": "No url or urls provided"
            }, indent=2)
    except Exception as e:
        # print(f"Error in _extract_code_from_url_internal: {traceback.format_exc()}")
        return json.dumps({
            "success": False,
            "error": str(e)
        }, indent=2)
        
async def _extract_code_single_page(crawler: AsyncWebCrawler, url: str) -> str:
    """
    Extract code from a single webpage without following any links.
    
    Optimized extraction strategy:
    1. HTML extraction only for ReadTheDocs/Sphinx documentation
    2. Markdown extraction for other content types
    3. Special handling for Jupyter notebooks (JSON extraction with raw URL fallback)
    4. No deduplication to preserve all code blocks
    """
    try:
        original_url = url
        url, is_valid = validate_and_normalize_url(url)
        if not is_valid:
            return json.dumps({
                "success": False,
                "url": original_url,
                "crawl_method": "single_page",
                "error": "Invalid URL provided for single page extraction",
                "extracted_code": []
            }, indent=2)
        # Configure crawler
        run_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS, 
            stream=False,
            verbose=False
        )
        # Fetch the page
        result = await crawler.arun(url=url, config=run_config)
        # Check if crawl was successful and we have content
        if not result.success:
            return json.dumps({
                "success": False,
                "url": original_url,
                "crawl_method": "single_page",
                "error": "Failed to crawl page",
                "error_details": getattr(result, 'error_message', 'Unknown error'),
                "extracted_code": []
            }, indent=2)
        if not hasattr(result, 'html') and not hasattr(result, 'markdown'):
            return json.dumps({
                "success": False,
                "url": original_url,
                "crawl_method": "single_page",
                "error": "No content available (neither HTML nor Markdown)",
                "extracted_code": []
            }, indent=2)
        code_blocks = []
        extraction_method = "none"
        doc_system = "unknown"
        # Strategy: HTML only for ReadTheDocs/Sphinx, otherwise use markdown
        # For Jupyter notebooks, prioritize JSON extraction
        # 1) HTML path only for ReadTheDocs/Sphinx
        if hasattr(result, 'html') and result.html:
            try:
                # Detect documentation system
                content_type, detected_doc_system, has_code = detect_content_type_and_source(
                    result.html, 
                    url
                )
                doc_system = detected_doc_system
                # Only use HTML extraction for ReadTheDocs/Sphinx
                if detected_doc_system in ['readthedocs', 'sphinx']:
                    code_blocks = extract_readthedocs_code_blocks(result.html, min_length=3)
                    extraction_method = f"html_{detected_doc_system}"
                else:
                    code_blocks = []
            except Exception as e:
                import traceback
                traceback.print_exc()
                code_blocks = []
        # 2) Markdown path for everything else
        if not code_blocks and hasattr(result, 'markdown') and result.markdown:
            try:
                md = result.markdown
                extraction_method = "markdown"
                # Check if this is Jupyter notebook content
                is_ipynb_url = url.lower().endswith('.ipynb')
                looks_like_json_notebook = md.strip().startswith('{') and '"cells"' in md[:2000]
                if is_ipynb_url or looks_like_json_notebook:
                    # Try direct JSON extraction from markdown
                    if looks_like_json_notebook:
                        code_blocks = extract_jupyter_notebook_cells(md, min_length=3)
                        doc_system = 'jupyter'
                        extraction_method = 'markdown_jupyter_json'
                    else:
                        # It's an .ipynb URL but markdown is not JSON (GitHub page). Fetch raw and parse
                        raw_url = github_blob_to_raw(url)
                        # Try crawler first (may not return JSON text)
                        raw_result = await crawler.arun(url=raw_url, config=run_config)
                        raw_content = (getattr(raw_result, 'text', None) or getattr(raw_result, 'markdown', None) or '')
                        if not (raw_content and raw_content.strip().startswith('{') and '"cells"' in raw_content[:2000]):
                            # Fallback to direct HTTP request
                            try:
                                resp = requests.get(raw_url, timeout=15)
                                if resp.status_code == 200:
                                    raw_content = resp.text
                                else:
                                    raw_content = ''
                            except Exception as _e:
                                raw_content = ''
                        if raw_content and raw_content.strip().startswith('{') and '"cells"' in raw_content[:2000]:
                            code_blocks = extract_jupyter_notebook_cells(raw_content, min_length=3)
                            doc_system = 'jupyter'
                            extraction_method = 'raw_jupyter_json'
                        else:
                            code_blocks = []
                # If still nothing, extract markdown code blocks + commands
                if not code_blocks:
                    md_blocks = extract_markdown_code_blocks(md, min_length=3)
                    command_blocks = extract_command_examples(md, min_length=3)
                    # Filter overly long command blocks
                    command_blocks = [b for b in command_blocks if len(b.get('code','')) < 5000]
                    # Merge unique command blocks
                    if command_blocks:
                        existing_codes = {block.get('code', '').strip() for block in md_blocks}
                        initial_count = len(md_blocks)
                        for cmd_block in command_blocks:
                            if cmd_block.get('code', '').strip() not in existing_codes:
                                md_blocks.append(cmd_block)
                    code_blocks = md_blocks
                    doc_system = 'unknown'
                    extraction_method = 'markdown_generic'
            except Exception as e:
                import traceback
                traceback.print_exc()
                code_blocks = []
        # Keep all code blocks (no deduplication)
        unique_code_blocks = code_blocks
        # Format extracted code blocks
        all_extracted_code = []
        for i, block in enumerate(unique_code_blocks):
            # Enhance context if missing
            context_before = block.get('context_before', '')
            context_after = block.get('context_after', '')
            if (not context_before or not context_after) and hasattr(result, 'markdown') and result.markdown:
                code_snippet = block.get('code', '')[:50]
                position = result.markdown.find(code_snippet)
                if position > 0:
                    context_before = extract_smart_context_before(
                        result.markdown, position, max_chars=1000
                    )
                    end_position = position + len(block.get('code', ''))
                    context_after = extract_smart_context_after(
                        result.markdown, end_position, max_chars=1000
                    )
            # Build code block info
            code_info = {
                "index": i + 1,
                "url": original_url,
                "type": block.get('type', 'code'),
                "language": block.get('language', 'unknown'),
                "code": block.get('code', ''),
                "context_before": context_before[:1000],
                "context_after": context_after[:1000],
                "extraction_method": extraction_method,
                "doc_system": doc_system,
                "summary": _generate_code_summary({
                    **block,
                    'context_before': context_before,
                    'context_after': context_after
                })
            }
            # Add optional metadata
            if block.get('title'):
                code_info['title'] = block['title']
            if block.get('description'):
                code_info['description'] = block['description']
            all_extracted_code.append(code_info)
        # Return results
        return json.dumps({
            "success": True,
            "url": original_url,
            "processed_url": url,
            "crawl_method": "single_page",
            "extraction_method": extraction_method,
            "doc_system": doc_system,
            "content_length": len(result.html) if hasattr(result, 'html') and result.html else len(result.markdown) if hasattr(result, 'markdown') and result.markdown else 0,
            "code_blocks_found": len(all_extracted_code),
            "extracted_code": all_extracted_code,
            "message": f"Found {len(all_extracted_code)} code blocks" if all_extracted_code else "No code blocks found in page"
        }, indent=2)
    except Exception as e:
        import traceback
        traceback.print_exc()
        return json.dumps({
            "success": False,
            "url": url,
            "crawl_method": "single_page",
            "error": str(e),
            "extracted_code": []
        }, indent=2)

async def _extract_code_smart_crawl(crawler: AsyncWebCrawler, url: str) -> str:
    """Extract code using smart crawl for complex sites."""
    try:
        # Store original URL
        original_url = url
        # Automatically convert GitHub blob URLs to raw URLs for better extraction
        url, is_valid = validate_and_normalize_url(url)
        if not is_valid:
            return json.dumps({
                "success": False,
                "url": original_url,
                "error": "Invalid URL provided for smart crawl",
                "extracted_code": []
            }, indent=2)
        crawl_results = []
        if is_txt(url):
            # For text files, use simple crawl
            crawl_results = await crawl_markdown_file(crawler, url)
        elif is_sitemap(url):
            # For sitemaps, extract URLs and crawl in parallel
            sitemap_urls = parse_sitemap(url)
            crawl_results = await crawl_batch(crawler, sitemap_urls[:5])  # Limit to first 5 URLs
        else:
            # For regular webpages, crawl recursively
            crawl_results = await crawl_recursive_internal_links(crawler, [url], max_depth=2, max_concurrent=5)
        # Extract code from all crawled results
        all_extracted_code = []
        total_content_length = 0
        for result in crawl_results:
            if result.get('success') and result.get('markdown'):
                # Pass URL context to extraction
                result_url = result.get('url', url)
                code_blocks = extract_code_blocks(result['markdown'], min_length=3, url=result_url)
                total_content_length += len(result['markdown'])
                for i, block in enumerate(code_blocks):
                    code_info = {
                        "index": len(all_extracted_code) + 1,
                        "url": original_url,  # Use the original URL
                        "type": block.get('type', 'unknown'),
                        "language": block.get('language', 'unknown'),
                        "code": block['code'],
                        "context_before": block.get('context_before', '')[:1000],
                        "context_after": block.get('context_after', '')[:1000],
                        "summary": _generate_code_summary(block)
                    }
                    all_extracted_code.append(code_info)
        return json.dumps({
            "success": True,
            "url": original_url,
            "processed_url": url,
            "crawl_method": "smart_crawl",
            "content_length": total_content_length,
            "code_blocks_found": len(all_extracted_code),
            "extracted_code": all_extracted_code
        }, indent=2)
    except Exception as e:
        return json.dumps({
            "success": False,
            "url": original_url,
            "error": str(e),
            "extracted_code": []
        }, indent=2)

async def crawl_batch(crawler: AsyncWebCrawler, urls: List[str], max_concurrent: int = 10) -> List[Dict[str, Any]]:
    """
    Batch crawl multiple URLs in parallel.
    Args:
        crawler: AsyncWebCrawler instance
        urls: List of URLs to crawl
        max_concurrent: Maximum number of concurrent browser sessions
    Returns:
        List of dictionaries with URL, markdown content, and success status
    """
    crawl_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS, 
        stream=False,
        verbose=False
    )
    dispatcher = MemoryAdaptiveDispatcher(
        memory_threshold_percent=70.0,
        check_interval=1.0,
        max_session_permit=max_concurrent
    )
    try:
        results = await crawler.arun_many(urls=urls, config=crawl_config, dispatcher=dispatcher)
        return [{
            'url': r.url, 
            'markdown': r.markdown,
            'success': True
        } for r in results if r.success and r.markdown]
    except Exception as e:
        logger.error(f"Error in batch crawl: {e}")
        return []
    finally:
        # Ensure dispatcher is properly cleaned up
        try:
            await dispatcher.close()
        except:
            pass

async def crawl_recursive_internal_links(crawler: AsyncWebCrawler, start_urls: List[str], max_depth: int = 3, max_concurrent: int = 10) -> List[Dict[str, Any]]:
    """
    Recursively crawl internal links from start URLs up to a maximum depth.
    Args:
        crawler: AsyncWebCrawler instance
        start_urls: List of starting URLs
        max_depth: Maximum recursion depth
        max_concurrent: Maximum number of concurrent browser sessions
    Returns:
        List of dictionaries with URL and markdown content
    """
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS, 
        stream=False,
        verbose=False
    )
    dispatcher = MemoryAdaptiveDispatcher(
        memory_threshold_percent=70.0,
        check_interval=1.0,
        max_session_permit=max_concurrent
    )
    visited = set()
    def normalize_url(url):
        return urldefrag(url)[0]
    current_urls = set([normalize_url(u) for u in start_urls])
    results_all = []
    for depth in range(max_depth):
        urls_to_crawl = [normalize_url(url) for url in current_urls if normalize_url(url) not in visited]
        if not urls_to_crawl:
            break
        results = await crawler.arun_many(urls=urls_to_crawl, config=run_config, dispatcher=dispatcher)
        next_level_urls = set()
        for result in results:
            norm_url = normalize_url(result.url)
            visited.add(norm_url)
            if result.success and result.markdown:
                results_all.append({
                    'url': result.url, 
                    'markdown': result.markdown,
                    'success': True
                })
                for link in result.links.get("internal", []):
                    next_url = normalize_url(link["href"])
                    if next_url not in visited:
                        next_level_urls.add(next_url)
        current_urls = next_level_urls
    return results_all

async def crawl_markdown_file(crawler: AsyncWebCrawler, url: str) -> List[Dict[str, Any]]:
    """
    Crawl a .txt or markdown file.
    Args:
        crawler: AsyncWebCrawler instance
        url: URL of the file
    Returns:
        List of dictionaries with URL, markdown content, and success status
    """
    try:
        crawl_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS, 
            stream=False,
            verbose=False
        )
        result = await crawler.arun(url=url, config=crawl_config)
        if result.success:
            # Try to get content, prefer markdown then text
            content = result.markdown or result.text or ""
            if content:
                return [{
                    'url': url, 
                    'markdown': content,
                    'success': True
                }]
            else:
                # print(f"No content found for {url}")
                return [{
                    'url': url,
                    'markdown': '',
                    'success': False,
                    'error': 'No content found'
                }]
        else:
            # print(f"Failed to crawl {url}: {result.error_message}")
            return [{
                'url': url,
                'markdown': '',
                'success': False,
                'error': result.error_message or 'Unknown error'
            }]
    except Exception as e:
        # print(f"Exception while crawling {url}: {str(e)}")
        return [{
            'url': url,
            'markdown': '',
            'success': False,
            'error': str(e)
        }]

def _generate_code_summary(block: Dict[str, Any]) -> str:
    """Generate a summary for a code block using AI Agent."""
    # Check if code summary generation is enabled
    generate_summary = os.getenv("GENERATE_CODE_SUMMARY", "false").lower() == "true"
    if not generate_summary:
        # Return empty string when summary generation is disabled
        return ""
    try:
        code = block.get('code', '')
        context_before = block.get('context_before', '')
        context_after = block.get('context_after', '')
        # Use the same logic as generate_code_example_summary
        return generate_code_example_summary(code, context_before, context_after)
    except Exception as e:
        logger.error(f"Error generating code summary: {e}")
        # Fallback to simple summary
        code = block.get('code', '')
        code_type = block.get('type', 'unknown')
        return f"{code_type.title()}: {code.strip()[:100]}..."


@mcp.tool()
async def retrieve_extracted_code(ctx: Context, query: str, match_count: int = 5) -> str:
    """
    Retrieve extracted code blocks relevant to the query.
    This tool searches the extracted_code table for code blocks relevant to the query and returns
    the matching examples with their summaries and context.
    You can call this tool multiple times with different queries to get more relevant results.
    Args:
        ctx: The MCP server provided context
        query: The search query
        match_count: Maximum number of results to return (default: 5)
    Returns:
        JSON string with the search results
    """
    try:
        # Get the Supabase client from the context
        supabase_client = ctx.request_context.lifespan_context.supabase_client
        # Search for code blocks
        results = await search_code_blocks(
            client=supabase_client,
            query=query,
            match_count=match_count
        )
        # Format the results
        formatted_results = []
        for result in results:
            formatted_result = {
                "source_url": result.get("source_url"),
                "code": result.get("code"),
                "summary": result.get("summary"),
                "context_before": result.get("context_before"),
                "context_after": result.get("context_after"),
                "type": result.get("type"),
                "language": result.get("language"),
                "index": result.get("index"),
                "similarity_score": result.get("similarity_score")
            }
            formatted_results.append(formatted_result)
        return json.dumps({
            "success": True,
            "query": query,
            "search_mode": "vector",
            "results": formatted_results,
            "count": len(formatted_results)
        }, indent=2)
    except Exception as e:
        return json.dumps({
            "success": False,
            "query": query,
            "error": str(e)
        }, indent=2)


@mcp.tool()
async def quick_introspect(
    ctx: Context,
    code_content: str = None,
    class_hint: str = None,
    method_hint: str = None,
    package_path: str = None,
    function_hint: str = None,
    module_hint: str = None,
    repo_hint: str = None,
    max_suggestions: int = 10,
    no_imports: bool = False,
) -> str:
    f"""
    Fast, static-first introspection for fixing import/class/method/function related errors.
    Uses Jedi for static discovery first (no side effects), then runtime import/inspect fallback.
    Returns a human/agent-readable report string with suggested import lines (if no-imports is not set), class methods (if method_hint/class_hint and repo_hint or package_path are provided), and functions (if function_hint/module_hint and repo_hint or package_path are provided).
    Parameter relationships:
    - repo_hint vs package_path: mutually exclusive; pass at most one.
    - module_hint must be used together with function_hint.
    - If class_hint or method_hint is provided, one of repo_hint or package_path is required.
    - If function_hint is provided, one of repo_hint or package_path is required.
    Notes:
    - You can provide fuzzy hints for class_hint, method_hint, function_hint, module_hint (but repo_hint need to be exact) if you cannot provide exact hints.
    - It is recommended to use code_content to provide the code content for import diagnostics.
    - repo_hint is the top-level import module name (may differ from pip distribution name). If repo_hint cannot be imported, provide package_path instead.
    - PATH HANDLING for package_path:
      * Absolute path: used as-is.
      * Relative path: resolved against the active environment's site-packages root {SITE_PACKAGES_HINT}.
        For example, passing "pydantic" will be tried as {SITE_PACKAGES_HINT}/pydantic.
      * If unsure about the absolute path, use your check_package_version tool (if you have this tool) to obtain it.
      * If repo_hint fails to import, try providing an absolute or relative package_path (relative path starts from {SITE_PACKAGES_HINT}).
    - Function vs Method:
      * Use method_hint when the target is a class member (instance/class method). Do not use function_hint for class/instance methods. **Most of the time, you should use method_hint**
      * Use function_hint only for top-level (module-level) functions; optionally add module_hint to narrow
      * Heuristics: analyze the call-site pattern â€” calls like SomeClass.method(...)/obj.method(...) â†’ method_hint; calls like package.module.function(...), module.function(...), function(...) â†’ function_hint
    - method_hint can be provided without class_hint to trigger a repo-wide search (noisy but useful, but often it is more recommended to add fuzzy or exact class_hint to narrow down the search).
    - To silence import diagnostics if you think it is too noisy for your use case, set no_imports=true and reuse this tool to introspect the code again.
    - max_suggestions is the maximum number of suggestions to return. If not provided, it will return all suggestions. You can set it to a smaller number to reduce the noise if needed.
    - Set env QI_DEBUG_ENGINE=1 to see whether Jedi or runtime fallback is used. But normally you don't need to set this.
    """
    try:
        # Resolve relative package_path against active environment's site-packages
        if package_path:
            try:
                import sysconfig, site
                candidate_paths = []
                purelib = sysconfig.get_paths().get("purelib")
                if purelib:
                    candidate_paths.append(purelib)
                try:
                    for p in site.getsitepackages():
                        if p not in candidate_paths:
                            candidate_paths.append(p)
                except Exception:
                    pass
                if not os.path.isabs(package_path):
                    for root in candidate_paths:
                        joined = os.path.join(root, package_path)
                        if os.path.exists(joined):
                            package_path = joined
                            break
            except Exception:
                pass
        report = run_quick_introspect(
            code_content=code_content,
            class_hint=class_hint,
            method_hint=method_hint,
            package_path=package_path,
            function_hint=function_hint,
            module_hint=module_hint,
            repo_hint=repo_hint,
            max_suggestions=max_suggestions,
            no_imports=no_imports,
        )
        import json as _json
        return _json.dumps({
            "success": True,
            "report": report,
        }, indent=2)
    except Exception as e:
        import json as _json
        # Return the core error message in 'report' to keep a single consumption path
        message = str(e)
        return _json.dumps({
            "success": False,
            "report": message,
        }, indent=2)


@mcp.tool()
async def runtime_probe_snippet(ctx: Context, snippet: str) -> str:
    """
    Return a ready-to-paste Python probe snippet for targeted debugging of runtime errors.
    snippet: one of "try_get_key", "try_get_attr".
    - try_get_key: Use at a KeyError site. Replace mapping['k'] with try_get_key(mapping, 'k').
      If the key exists, it prints an OK message (either the earlier error was at a different site and you
      should probe the right line and re-run, or you are now probing a different key than the one that caused the earlier KeyError and have successfully
      debugged it). If missing, it prints available keys and error context to guide a fix.
    - try_get_attr: Use at an AttributeError site. Replace obj.attr with try_get_attr(obj, 'attr').
      If the attribute exists, it prints an OK message (either the earlier error was at a different site and
      you should probe the right line and re-run, or you are now probing a different attribute than the one that caused the earlier AttributeError and have
      successfully debugged it). If missing, it prints public attributes, similarity suggestions and error context to guide a fix.
    The returned report contains a short usage header followed by the snippet code. You MUST Paste the snippet into
    your current script right after your imports and follow the usage note to replace the failing access.
    """
    import json as _json
    SNIPPETS: dict[str, str] = {
        "try_get_key": (
            "import sys, os\n"
            "from pathlib import Path\n"
            "# Resolve project root heuristically and add probe folder to sys.path\n"
            "_here = Path(__file__).resolve() if '__file__' in globals() else Path.cwd()\n"
            "_root = _here\n"
            "for _ in range(5):\n"
            "    if (_root / 'research-server' / 'introspection_and_probe').exists():\n"
            "        sys.path.insert(0, str(_root / 'research-server' / 'introspection_and_probe'))\n"
            "        break\n"
            "    _root = _root.parent\n"
            "from runtime_probe import try_get_key\n\n"
            "# Usage at the KeyError site: replace mapping['k'] with try_get_key(mapping, 'k')\n"
        ),
        "try_get_attr": (
            "import sys, os\n"
            "from pathlib import Path\n"
            "_here = Path(__file__).resolve() if '__file__' in globals() else Path.cwd()\n"
            "_root = _here\n"
            "for _ in range(5):\n"
            "    if (_root / 'research-server' / 'introspection_and_probe').exists():\n"
            "        sys.path.insert(0, str(_root / 'research-server' / 'introspection_and_probe'))\n"
            "        break\n"
            "    _root = _root.parent\n"
            "from runtime_probe import try_get_attr\n\n"
            "# Usage at the AttributeError site: replace obj.attr with try_get_attr(obj, 'attr')\n"
        ),
    }
    key = (snippet or "").strip()
    if key not in SNIPPETS:
        return _json.dumps({
            "success": False,
            "report": "Invalid snippet. Use one of: try_get_key, try_get_attr",
        }, indent=2)
    headers = {
        "try_get_key": (
            "Usage at the KeyError site: replace mapping['k'] with try_get_key(mapping, 'k').\n"
            "**You MUST Paste this snippet right after your imports.**\n"
        ),
        "try_get_attr": (
            "Usage at the AttributeError site: replace obj.attr with try_get_attr(obj, 'attr').\n"
            "**You MUST Paste this snippet right after your imports.**\n"
        ),
    }
    header = headers[key]
    return _json.dumps({
        "success": True,
        "report": header + "\n" + SNIPPETS[key],
    }, indent=2)


@mcp.tool()
async def parse_local_package(ctx: Context, package_name: str = None, package_path: str = None) -> str:
    f"""
    Parse a locally installed Python package into the Neo4j knowledge graph.
    This tool analyzes a locally installed Python package, extracts its code structure,
    and stores it in Neo4j for use in knowledge graph exploration. The tool supports
    flexible package location strategies:
    **Package Location Strategies:**
    1. **Auto-detection**: Provide only `package_name` - tool will use `uv pip show` to locate the package
    2. **Direct path**: Provide only `package_path` - tool will parse the specified directory directly
    3. **Hybrid approach**: Provide both - tool will try `package_path` first, fallback to `package_name` auto-detection
    **Error Handling:**
    - If package_name cannot be found or is not installed, provides detailed error with suggestions
    - If package_path doesn't exist but package_name is provided, tries auto-detection as fallback
    - Returns clear error messages with actionable suggestions
    **Features:**
    - Locates the package in the Python environment or uses provided path
    - Analyzes Python files to extract code structure  
    - Stores classes, methods, functions, and imports in Neo4j
    - Provides detailed statistics about the parsing results
    - Ensures version compatibility with the local environment
    - Checks if the specific version already exists before parsing
    PATH HANDLING for package_path:
    - Absolute path: used as-is.
    - Relative path: resolved against the active environment's site-packages root {SITE_PACKAGES_HINT}.
      For example, passing "pydantic" will be tried as {SITE_PACKAGES_HINT}/pydantic.
    - If unsure about the absolute path, use your check_package_version tool (if you have this tool) to obtain it.

    Args:
        ctx: The MCP server provided context
        package_name: Name of the package to parse (optional if package_path is provided)
        package_path: Path to the package directory (optional if package_name is provided)
    Returns:
        JSON string with parsing results, statistics, and package information
    """
    try:
        # Check if knowledge graph functionality is enabled
        knowledge_graph_enabled = os.getenv("USE_KNOWLEDGE_GRAPH", "false") == "true"
        if not knowledge_graph_enabled:
            return json.dumps({
                "success": False,
                "error": "Knowledge graph functionality is disabled. Set USE_KNOWLEDGE_GRAPH=true in environment."
            }, indent=2)
        # Get the repository extractor from context
        repo_extractor = ctx.request_context.lifespan_context.repo_extractor
        if not repo_extractor:
            return json.dumps({
                "success": False,
                "error": "Repository extractor not available. Check Neo4j configuration in environment variables."
            }, indent=2)
        # Validate input parameters
        if not package_name and not package_path:
            return json.dumps({
                "success": False,
                "error": "Either package_name or package_path must be provided"
            }, indent=2)
        valid_package_path = None
        detected_version = None
        final_package_name = package_name
        def try_auto_detect_package(pkg_name):
            """Helper function to auto-detect package path from package name"""
            try:
                import subprocess
                result = subprocess.run(
                    ["uv", "pip", "show", pkg_name], 
                    capture_output=True, 
                    text=True, 
                    check=True
                )
                # Parse pip show output to get location and version
                site_packages_path = None
                version = None
                for line in result.stdout.split('\n'):
                    if line.startswith('Location:'):
                        site_packages_path = line.split(':', 1)[1].strip()
                    elif line.startswith('Version:'):
                        version = line.split(':', 1)[1].strip()
                # Construct the actual package path by appending package name to site-packages location
                if site_packages_path:
                    auto_detected_path = os.path.join(site_packages_path, pkg_name)
                    if os.path.exists(auto_detected_path):
                        return auto_detected_path, version
            except subprocess.CalledProcessError:
                pass
            return None, None
        # Helper to resolve relative package_path against site-packages
        def _resolve_relative_package_path(path: str) -> Optional[str]:
            try:
                import sysconfig, site
                bases: list[str] = []
                purelib = sysconfig.get_paths().get("purelib")
                if purelib:
                    bases.append(purelib)
                try:
                    for p in site.getsitepackages():
                        if p not in bases:
                            bases.append(p)
                except Exception:
                    pass
                if not os.path.isabs(path):
                    for root in bases:
                        candidate = os.path.join(root, path)
                        if os.path.exists(candidate):
                            return candidate
            except Exception:
                pass
            return None

        # Case 1: Only package_path provided
        if package_path and not package_name:
            resolved_package_path = package_path
            if not os.path.isabs(resolved_package_path):
                maybe = _resolve_relative_package_path(resolved_package_path)
                if maybe:
                    resolved_package_path = maybe
            if os.path.exists(resolved_package_path):
                valid_package_path = resolved_package_path
                # Try to infer package name from path for better error messages
                final_package_name = os.path.basename(valid_package_path.rstrip('/'))
            else:
                return json.dumps({
                    "success": False,
                    "package_path": package_path,
                    "error": f"Package path does not exist: {package_path}. Please confirm the package_path is correct or use package_name instead and retry this tool.",
                    "suggestions": [
                        "1. Verify the package_path is correct and accessible",
                        "2. Use package_name instead to auto-detect the location",
                        "3. Check if the directory exists and contains Python files"
                    ]
                }, indent=2)
        # Case 2: Only package_name provided  
        elif package_name and not package_path:
            valid_package_path, detected_version = try_auto_detect_package(package_name)
            if not valid_package_path:
                return json.dumps({
                    "success": False,
                    "package_name": package_name,
                    "error": (
                        f"Package '{package_name}' may be incorrect or not installed. "
                        f"Please verify the package name is correct, ensure it's installed, "
                        f"or provide package_path instead (absolute path, or relative path starting from {SITE_PACKAGES_HINT}) and retry this tool. "
                        f"If you have the check_package_version tool, you can use it to obtain the absolute package_path."
                    ),
                    "suggestions": [
                        f"1. Verify package name is correct: {package_name}",
                        f"2. Install the package: uv pip install {package_name}",
                        f"3. Provide package_path manually (absolute path or relative path from {SITE_PACKAGES_HINT})",
                        f"4. If you want to use absolute path, use check_package_version tool (if you have this tool) to get the absolute package_path"
                    ]
                }, indent=2)
        # Case 3: Both package_name and package_path provided
        else:  # both package_name and package_path are provided
            # Try package_path first
            resolved_package_path = package_path
            if not os.path.isabs(resolved_package_path):
                maybe = _resolve_relative_package_path(resolved_package_path)
                if maybe:
                    resolved_package_path = maybe
            if os.path.exists(resolved_package_path):
                valid_package_path = resolved_package_path
                # Infer the actual package name from the path (more reliable than user input)
                inferred_package_name = os.path.basename(valid_package_path.rstrip('/'))
                final_package_name = inferred_package_name
                # Try to get version info using the inferred package name
                _, detected_version = try_auto_detect_package(inferred_package_name)
                # If that fails, try with the provided package_name as fallback
                if not detected_version:
                    _, detected_version = try_auto_detect_package(package_name)
            else:
                # Fallback to package_name auto-detection
                valid_package_path, detected_version = try_auto_detect_package(package_name)
                if not valid_package_path:
                    return json.dumps({
                        "success": False,
                        "package_name": package_name,
                        "package_path": package_path,
                        "error": (
                            f"Both provided package_path '{package_path}' and package_name '{package_name}' "
                            f"are incorrect. Please check both parameters and provide correct information to retry."
                        ),
                        "suggestions": [
                            "1. Verify the package_path exists and is accessible",
                            "2. Verify the package_name is correct and installed",
                            "3. You can provide either package_name or package_path (or both)"
                        ]
                    }, indent=2)
        # Try to get version info if we still don't have it and we have a valid path
        if not detected_version and valid_package_path and final_package_name:
            _, detected_version = try_auto_detect_package(final_package_name)
        # Parse the local package using the valid path and detected version
        # print(f"Starting local package analysis for: {final_package_name}")
        # print(f"Using package path: {valid_package_path}")
        # if detected_version:
        #     print(f"Version: {detected_version}")
        # else:
        #     print("Version: Not detected")
        result = await repo_extractor.analyze_local_package(final_package_name, valid_package_path, detected_version)
        if result.get("skipped", False):
            return json.dumps({
                "success": True,
                "package_name": final_package_name,
                "package_path": valid_package_path,
                "version": detected_version,
                "message": result["message"],
                "skipped": True
            }, indent=2)
        # print(f"Local package analysis completed for: {final_package_name}")
        # Use the result from analyze_local_package
        if result.get("success", False):
            stats = {
                "package": final_package_name,
                "files_processed": result.get("files_processed", 0),
                "classes_created": result.get("classes_created", 0),
                "methods_created": result.get("methods_created", 0), 
                "functions_created": result.get("functions_created", 0)
            }
            return json.dumps({
                "success": True,
                "package_name": final_package_name,
                "package_path": valid_package_path,
                "version": detected_version,
                "message": f"Successfully parsed local package '{final_package_name}' into knowledge graph",
                "statistics": stats,
                "ready_for_validation": True,
                "next_steps": [
                    "Package is now available for knowledge graph exploration",
                    f"Use query_knowledge_graph to explore the knowledge graph for {final_package_name}",
                    "The knowledge graph contains classes, methods, and functions from this package"
                ]
            }, indent=2)
        else:
            return json.dumps({
                "success": False,
                "package_name": final_package_name,
                "error": "Failed to parse local package"
            }, indent=2)
    except Exception as e:
        return json.dumps({
            "success": False,
            "package_name": final_package_name if 'final_package_name' in locals() else package_name,
            "error": f"Local package parsing failed: {str(e)}"
        }, indent=2)


@mcp.tool()
async def query_knowledge_graph(ctx: Context, command: str) -> str:
    """
    Query and explore the Neo4j knowledge graph containing repository data.
    This tool provides comprehensive access to the knowledge graph for exploring repositories,
    classes, methods, functions, and their relationships. Perfect for understanding what data
    is available for knowledge graph exploration and debugging validation results.
    **IMPORTANT: Always start with the `repos` command first!**
    Before using any other commands, run `repos` to see what repositories are available
    in your knowledge graph. This will help you understand what data you can explore.
    ## Available Commands:
    **Repository Commands:**
    - `repos` - **START HERE!** List all repositories in the knowledge graph
    - `explore <repo_name>` - Get detailed overview of a specific repository
    **Class Commands:**  
    - `classes` - List all classes across all repositories (limited to 20)
    - `classes <repo_name>` - List classes in a specific repository
    - `class <class_name>` - Get detailed information about a specific class including methods and attributes
    **Method Commands:**
    - `method <method_name>` - Search for methods by name across all classes
    - `method <method_name> <class_name>` - Search for a method within a specific class
    **Custom Query:**
    - `query <cypher_query>` - Execute a custom Cypher query (results limited to 20 records)
    ## Knowledge Graph Schema:
    **Node Types:**
    - Repository: `(r:Repository {name: string})`
    - File: `(f:File {path: string, module_name: string})`
    - Class: `(c:Class {name: string, full_name: string})`
    - Method: `(m:Method {name: string, params_list: [string], params_detailed: [string], return_type: string, args: [string]})`
    - Function: `(func:Function {name: string, params_list: [string], params_detailed: [string], return_type: string, args: [string]})`
    - Attribute: `(a:Attribute {name: string, type: string})`
    **Relationships:**
    - `(r:Repository)-[:CONTAINS]->(f:File)`
    - `(f:File)-[:DEFINES]->(c:Class)`
    - `(c:Class)-[:HAS_METHOD]->(m:Method)`
    - `(c:Class)-[:HAS_ATTRIBUTE]->(a:Attribute)`
    - `(f:File)-[:DEFINES]->(func:Function)`
    ## Example Workflow:
    ```
    1. repos                                    # See what repositories are available
    2. explore pydantic-ai                      # Explore a specific repository
    3. classes pydantic-ai                      # List classes in that repository
    4. class Agent                              # Explore the Agent class
    5. method run_stream                        # Search for run_stream method
    6. method __init__ Agent                    # Find Agent constructor
    7. query "MATCH (c:Class)-[:HAS_METHOD]->(m:Method) WHERE m.name = 'run' RETURN c.name, m.name LIMIT 5"
    ```
    Args:
        ctx: The MCP server provided context
        command: Command string to execute (see available commands above)
    Returns:
        JSON string with query results, statistics, and metadata
    """
    try:
        # Check if knowledge graph functionality is enabled
        knowledge_graph_enabled = os.getenv("USE_KNOWLEDGE_GRAPH", "false") == "true"
        if not knowledge_graph_enabled:
            return json.dumps({
                "success": False,
                "error": "Knowledge graph functionality is disabled. Set USE_KNOWLEDGE_GRAPH=true in environment."
            }, indent=2)
        # Get Neo4j driver from context
        repo_extractor = ctx.request_context.lifespan_context.repo_extractor
        if not repo_extractor or not repo_extractor.driver:
            return json.dumps({
                "success": False,
                "error": "Neo4j connection not available. Check Neo4j configuration in environment variables."
            }, indent=2)
        # Parse command
        command = command.strip()
        if not command:
            return json.dumps({
                "success": False,
                "command": "",
                "error": "Command cannot be empty. Available commands: repos, explore <repo>, classes [repo], class <name>, method <name> [class], query <cypher>"
            }, indent=2)
        parts = command.split()
        cmd = parts[0].lower()
        args = parts[1:] if len(parts) > 1 else []
        async with repo_extractor.driver.session() as session:
            # Route to appropriate handler
            if cmd == "repos":
                return await _handle_repos_command(session, command)
            elif cmd == "explore":
                if not args:
                    return json.dumps({
                        "success": False,
                        "command": command,
                        "error": "Repository name required. Usage: explore <repo_name>"
                    }, indent=2)
                return await _handle_explore_command(session, command, args[0])
            elif cmd == "classes":
                repo_name = args[0] if args else None
                return await _handle_classes_command(session, command, repo_name)
            elif cmd == "class":
                if not args:
                    return json.dumps({
                        "success": False,
                        "command": command,
                        "error": "Class name required. Usage: class <class_name>"
                    }, indent=2)
                return await _handle_class_command(session, command, args[0])
            elif cmd == "method":
                if not args:
                    return json.dumps({
                        "success": False,
                        "command": command,
                        "error": "Method name required. Usage: method <method_name> [class_name]"
                    }, indent=2)
                method_name = args[0]
                class_name = args[1] if len(args) > 1 else None
                return await _handle_method_command(session, command, method_name, class_name)
            elif cmd == "query":
                if not args:
                    return json.dumps({
                        "success": False,
                        "command": command,
                        "error": "Cypher query required. Usage: query <cypher_query>"
                    }, indent=2)
                cypher_query = " ".join(args)
                return await _handle_query_command(session, command, cypher_query)
            else:
                return json.dumps({
                    "success": False,
                    "command": command,
                    "error": f"Unknown command '{cmd}'. Available commands: repos, explore <repo>, classes [repo], class <name>, method <name> [class], query <cypher>"
                }, indent=2)
    except Exception as e:
        return json.dumps({
            "success": False,
            "command": command,
            "error": f"Query execution failed: {str(e)}"
        }, indent=2)

async def _handle_repos_command(session, command: str) -> str:
    """Handle 'repos' command - list all repositories"""
    query = "MATCH (r:Repository) RETURN r.name as name ORDER BY r.name"
    result = await session.run(query)
    repos = []
    async for record in result:
        repos.append(record['name'])
    return json.dumps({
        "success": True,
        "command": command,
        "data": {
            "repositories": repos
        },
        "metadata": {
            "total_results": len(repos),
            "limited": False
        }
    }, indent=2)

async def _handle_explore_command(session, command: str, repo_name: str) -> str:
    """Handle 'explore <repo>' command - get repository overview"""
    # Check if repository exists
    repo_check_query = "MATCH (r:Repository {name: $repo_name}) RETURN r.name as name"
    result = await session.run(repo_check_query, repo_name=repo_name)
    repo_record = await result.single()
    if not repo_record:
        return json.dumps({
            "success": False,
            "command": command,
            "error": f"Repository '{repo_name}' not found in knowledge graph"
        }, indent=2)
    # Get file count
    files_query = """
    MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)
    RETURN count(f) as file_count
    """
    result = await session.run(files_query, repo_name=repo_name)
    file_count = (await result.single())['file_count']
    # Get class count
    classes_query = """
    MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(c:Class)
    RETURN count(DISTINCT c) as class_count
    """
    result = await session.run(classes_query, repo_name=repo_name)
    class_count = (await result.single())['class_count']
    # Get function count
    functions_query = """
    MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(func:Function)
    RETURN count(DISTINCT func) as function_count
    """
    result = await session.run(functions_query, repo_name=repo_name)
    function_count = (await result.single())['function_count']
    # Get method count
    methods_query = """
    MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(c:Class)-[:HAS_METHOD]->(m:Method)
    RETURN count(DISTINCT m) as method_count
    """
    result = await session.run(methods_query, repo_name=repo_name)
    method_count = (await result.single())['method_count']
    return json.dumps({
        "success": True,
        "command": command,
        "data": {
            "repository": repo_name,
            "statistics": {
                "files": file_count,
                "classes": class_count,
                "functions": function_count,
                "methods": method_count
            }
        },
        "metadata": {
            "total_results": 1,
            "limited": False
        }
    }, indent=2)

async def _handle_classes_command(session, command: str, repo_name: str = None) -> str:
    """Handle 'classes [repo]' command - list classes"""
    limit = 20
    if repo_name:
        query = """
        MATCH (r:Repository {name: $repo_name})-[:CONTAINS]->(f:File)-[:DEFINES]->(c:Class)
        RETURN c.name as name, c.full_name as full_name
        ORDER BY c.name
        LIMIT $limit
        """
        result = await session.run(query, repo_name=repo_name, limit=limit)
    else:
        query = """
        MATCH (c:Class)
        RETURN c.name as name, c.full_name as full_name
        ORDER BY c.name
        LIMIT $limit
        """
        result = await session.run(query, limit=limit)
    classes = []
    async for record in result:
        classes.append({
            'name': record['name'],
            'full_name': record['full_name']
        })
    return json.dumps({
        "success": True,
        "command": command,
        "data": {
            "classes": classes,
            "repository_filter": repo_name
        },
        "metadata": {
            "total_results": len(classes),
            "limited": len(classes) >= limit
        }
    }, indent=2)

async def _handle_class_command(session, command: str, class_name: str) -> str:
    """Handle 'class <name>' command - explore specific class"""
    # Find the class
    class_query = """
    MATCH (c:Class)
    WHERE c.name = $class_name OR c.full_name = $class_name
    RETURN c.name as name, c.full_name as full_name
    LIMIT 1
    """
    result = await session.run(class_query, class_name=class_name)
    class_record = await result.single()
    if not class_record:
        return json.dumps({
            "success": False,
            "command": command,
            "error": f"Class '{class_name}' not found in knowledge graph"
        }, indent=2)
    actual_name = class_record['name']
    full_name = class_record['full_name']
    # Get methods
    methods_query = """
    MATCH (c:Class)-[:HAS_METHOD]->(m:Method)
    WHERE c.name = $class_name OR c.full_name = $class_name
    RETURN m.name as name, m.params_list as params_list, m.params_detailed as params_detailed, m.return_type as return_type
    ORDER BY m.name
    """
    result = await session.run(methods_query, class_name=class_name)
    methods = []
    async for record in result:
        # Use detailed params if available, fall back to simple params
        params_to_use = record['params_detailed'] or record['params_list'] or []
        methods.append({
            'name': record['name'],
            'parameters': params_to_use,
            'return_type': record['return_type'] or 'Any'
        })
    # Get attributes
    attributes_query = """
    MATCH (c:Class)-[:HAS_ATTRIBUTE]->(a:Attribute)
    WHERE c.name = $class_name OR c.full_name = $class_name
    RETURN a.name as name, a.type as type
    ORDER BY a.name
    """
    result = await session.run(attributes_query, class_name=class_name)
    attributes = []
    async for record in result:
        attributes.append({
            'name': record['name'],
            'type': record['type'] or 'Any'
        })
    return json.dumps({
        "success": True,
        "command": command,
        "data": {
            "class": {
                "name": actual_name,
                "full_name": full_name,
                "methods": methods,
                "attributes": attributes
            }
        },
        "metadata": {
            "total_results": 1,
            "methods_count": len(methods),
            "attributes_count": len(attributes),
            "limited": False
        }
    }, indent=2)

async def _handle_method_command(session, command: str, method_name: str, class_name: str = None) -> str:
    """Handle 'method <name> [class]' command - search for methods"""
    if class_name:
        query = """
        MATCH (c:Class)-[:HAS_METHOD]->(m:Method)
        WHERE (c.name = $class_name OR c.full_name = $class_name)
          AND m.name = $method_name
        RETURN c.name as class_name, c.full_name as class_full_name,
               m.name as method_name, m.params_list as params_list, 
               m.params_detailed as params_detailed, m.return_type as return_type, m.args as args
        """
        result = await session.run(query, class_name=class_name, method_name=method_name)
    else:
        query = """
        MATCH (c:Class)-[:HAS_METHOD]->(m:Method)
        WHERE m.name = $method_name
        RETURN c.name as class_name, c.full_name as class_full_name,
               m.name as method_name, m.params_list as params_list, 
               m.params_detailed as params_detailed, m.return_type as return_type, m.args as args
        ORDER BY c.name
        LIMIT 20
        """
        result = await session.run(query, method_name=method_name)
    methods = []
    async for record in result:
        # Use detailed params if available, fall back to simple params
        params_to_use = record['params_detailed'] or record['params_list'] or []
        methods.append({
            'class_name': record['class_name'],
            'class_full_name': record['class_full_name'],
            'method_name': record['method_name'],
            'parameters': params_to_use,
            'return_type': record['return_type'] or 'Any',
            'legacy_args': record['args'] or []
        })
    if not methods:
        return json.dumps({
            "success": False,
            "command": command,
            "error": f"Method '{method_name}'" + (f" in class '{class_name}'" if class_name else "") + " not found"
        }, indent=2)
    return json.dumps({
        "success": True,
        "command": command,
        "data": {
            "methods": methods,
            "class_filter": class_name
        },
        "metadata": {
            "total_results": len(methods),
            "limited": len(methods) >= 20 and not class_name
        }
    }, indent=2)

async def _handle_query_command(session, command: str, cypher_query: str) -> str:
    """Handle 'query <cypher>' command - execute custom Cypher query"""
    try:
        # Execute the query with a limit to prevent overwhelming responses
        result = await session.run(cypher_query)
        records = []
        count = 0
        async for record in result:
            records.append(dict(record))
            count += 1
            if count >= 20:  # Limit results to prevent overwhelming responses
                break
        return json.dumps({
            "success": True,
            "command": command,
            "data": {
                "query": cypher_query,
                "results": records
            },
            "metadata": {
                "total_results": len(records),
                "limited": len(records) >= 20
            }
        }, indent=2)
    except Exception as e:
        return json.dumps({
            "success": False,
            "command": command,
            "error": f"Cypher query error: {str(e)}",
            "data": {
                "query": cypher_query
            }
        }, indent=2)


async def main():
    transport = os.getenv("TRANSPORT", "sse")
    if transport == 'sse':
        # Run the MCP server with sse transport
        await mcp.run_sse_async()
    else:
        # Run the MCP server with stdio transport
        await mcp.run_stdio_async()

if __name__ == "__main__":
    asyncio.run(main())
```


================================================================================
=== FILE: mcp_servers_and_tools\research_server\src\research_server_utils.py ===
================================================================================

```python
"""
Utility functions for the research server.
"""
import os
import concurrent.futures
from typing import List, Dict, Any, Optional, Tuple
import json
from supabase import create_client, Client
from urllib.parse import urlparse
import openai
import re
import time
import html
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.common.exceptions import TimeoutException, WebDriverException
import asyncio
# Load OpenAI API key for embeddings
openai.api_key = os.getenv("OPENAI_API_KEY")


def get_supabase_client() -> Client:
    """
    Get a Supabase client with the URL and key from environment variables.
    Returns:
        Supabase client instance
    """
    url = os.getenv("SUPABASE_URL")
    key = os.getenv("SUPABASE_SERVICE_KEY")
    if not url or not key:
        raise ValueError("SUPABASE_URL and SUPABASE_SERVICE_KEY must be set in environment variables")
    return create_client(url, key)


async def create_embeddings_batch(texts: List[str]) -> List[List[float]]:
    """
    Create embeddings for multiple texts in a single API call.
    Args:
        texts: List of texts to create embeddings for
    Returns:
        List of embeddings (each embedding is a list of floats)
    """
    if not texts:
        return []
    max_retries = 3
    retry_delay = 1.0  # Start with 1 second delay
    for retry in range(max_retries):
        try:
            # Offload blocking OpenAI call to a thread
            response = await asyncio.to_thread(
                openai.embeddings.create,
                model=os.getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-3-small"),
                input=texts,
            )
            return [item.embedding for item in response.data]
        except Exception as e:
            if retry < max_retries - 1:
                print(f"Error creating batch embeddings (attempt {retry + 1}/{max_retries}): {e}")
                print(f"Retrying in {retry_delay} seconds...")
                await asyncio.sleep(retry_delay)
                retry_delay = min(retry_delay * 2, 16.0)
            else:
                print(f"Failed to create batch embeddings after {max_retries} attempts: {e}")
                # Try creating embeddings one by one as fallback
                print("Attempting to create embeddings individually...")
                embeddings = []
                successful_count = 0
                for i, text in enumerate(texts):
                    try:
                        individual_response = await asyncio.to_thread(
                            openai.embeddings.create,
                            model=os.getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-3-small"),
                            input=[text],
                        )
                        embeddings.append(individual_response.data[0].embedding)
                        successful_count += 1
                    except Exception as individual_error:
                        print(f"Failed to create embedding for text {i}: {individual_error}")
                        # Add zero embedding as fallback
                        embeddings.append([0.0] * 1536)
                print(f"Successfully created {successful_count}/{len(texts)} embeddings individually")
                return embeddings


async def create_embedding(text: str) -> List[float]:
    """
    Create an embedding for a single text using OpenAI's API.
    Args:
        text: Text to create an embedding for
    Returns:
        List of floats representing the embedding
    """
    try:
        embeddings = await create_embeddings_batch([text])
        return embeddings[0] if embeddings else [0.0] * 1536
    except Exception as e:
        print(f"Error creating embedding: {e}")
        # Return empty embedding if there's an error
        return [0.0] * 1536


def detect_content_type_and_source(html_content: str, url: str) -> Tuple[str, str, bool]:
    """
    Detect the type of content and documentation system from HTML and URL.
    Returns:
        Tuple of (content_type, doc_system, has_code)
        - content_type: 'html', 'jupyter', 'markdown', 'raw_code'
        - doc_system: 'readthedocs', 'sphinx', 'mkdocs', 'github', 'jupyter', 'unknown'
        - has_code: whether the content contains code blocks
    """
    # Check if it's a raw code file based on URL
    if url:
        url_lower = url.lower()
        # Check for raw GitHub URLs or direct code file extensions
        if 'raw.githubusercontent.com' in url_lower or ('github.com' in url_lower and '/raw/' in url_lower):
            code_extensions = ['.py', '.js', '.java', '.cpp', '.c', '.go', '.rs', '.rb', '.php', '.swift', '.kt', '.scala', '.r', '.m']
            for ext in code_extensions:
                if url_lower.endswith(ext):
                    return ('raw_code', 'github', True)
        # Special handling for GitHub Jupyter notebooks (.ipynb files)
        if url_lower.endswith('.ipynb'):
            return ('jupyter', 'github', True)
    # Quick check for Jupyter notebooks (JSON format)
    if html_content.strip().startswith('{') and '"cells"' in html_content[:1000]:
        try:
            import json
            json.loads(html_content)
            return ('jupyter', 'jupyter', True)
        except:
            pass
    # Check for Jupyter notebook HTML rendering
    if ('jp-Notebook' in html_content or 'notebook-container' in html_content or 
        'jupyter' in html_content.lower() or 'ipynb' in html_content.lower()):
        return ('jupyter', 'jupyter', True)
    # Check if it's markdown content (but not HTML with markdown)
    if not '<html' in html_content[:1000] and not '<!DOCTYPE' in html_content[:1000]:
        if html_content.strip().startswith('#') or '```' in html_content[:1000]:
            # Strong indicators of markdown
            has_code = '```' in html_content or '    ' in html_content  # Code blocks or indented code
            return ('markdown', 'unknown', has_code)
    # Parse HTML for detailed detection
    soup = BeautifulSoup(html_content, 'html.parser')
    # Check URL patterns first (most reliable)
    url_lower = url.lower() if url else ''
    # ReadTheDocs detection - PRIORITY CHECK
    if 'readthedocs' in url_lower:
        # Check for code blocks specific to ReadTheDocs
        highlight_divs = soup.find_all('div', class_=re.compile(r'highlight'))
        has_code = len(highlight_divs) > 0
        return ('html', 'readthedocs', has_code)
    # GitHub detection from URL (but not for .ipynb files which are handled above)
    if 'github.com' in url_lower and not url_lower.endswith('.ipynb'):
        has_code = bool(soup.find('pre') or soup.find('code'))
        return ('html', 'github', has_code)
    # Check for documentation system markers in HTML
    # ReadTheDocs/Sphinx detection by HTML structure
    if soup.find('div', class_='wy-nav-content'):  # ReadTheDocs theme
        highlight_divs = soup.find_all('div', class_=re.compile(r'highlight'))
        has_code = len(highlight_divs) > 0
        return ('html', 'readthedocs', has_code)
    if soup.find('div', class_='sphinxsidebar'):  # Classic Sphinx
        has_code = bool(soup.find('div', class_=re.compile(r'highlight')))
        return ('html', 'sphinx', has_code)
    if soup.find('div', class_='rst-content'):  # RestructuredText content
        has_code = bool(soup.find('div', class_=re.compile(r'highlight')))
        return ('html', 'sphinx', has_code)
    # Check for highlight divs (common in ReadTheDocs/Sphinx)
    highlight_divs = soup.find_all('div', class_=re.compile(r'highlight'))
    if len(highlight_divs) > 0:
        # Further check for ReadTheDocs/Sphinx patterns
        if soup.find('div', class_='document') or soup.find('div', role='main'):
            return ('html', 'readthedocs', True)  # Highlight divs mean there's code
        # If many highlight divs, likely ReadTheDocs/Sphinx
        if len(highlight_divs) >= 2:
            return ('html', 'readthedocs', True)
    # MkDocs detection
    if soup.find('div', class_='md-container'):
        has_code = bool(soup.find('pre') or soup.find('code'))
        return ('html', 'mkdocs', has_code)
    if soup.find('nav', class_='md-nav'):
        has_code = bool(soup.find('pre') or soup.find('code'))
        return ('html', 'mkdocs', has_code)
    # GitHub detection by HTML structure
    if soup.find('div', class_='markdown-body'):
        has_code = bool(soup.find('pre') or soup.find('code'))
        return ('html', 'github', has_code)
    if soup.find('article', class_='markdown-body'):
        has_code = bool(soup.find('pre') or soup.find('code'))
        return ('html', 'github', has_code)
    # Check meta tags
    generator = soup.find('meta', attrs={'name': 'generator'})
    if generator:
        content = generator.get('content', '').lower()
        if 'sphinx' in content:
            has_code = bool(soup.find('div', class_=re.compile(r'highlight')))
            return ('html', 'sphinx', has_code)
        if 'mkdocs' in content:
            has_code = bool(soup.find('pre') or soup.find('code'))
            return ('html', 'mkdocs', has_code)
    # Final check for any code indicators
    has_code = False
    code_indicators = [
        soup.find('pre'),
        soup.find('code'),
        soup.find('div', class_=re.compile(r'highlight')),
        soup.find('div', class_='codehilite'),
        soup.find('div', class_='sourceCode'),
        soup.find('div', class_='literal-block'),
        '```' in html_content,
        '<pre>' in html_content,
        '<code>' in html_content
    ]
    has_code = any(code_indicators)
    # Default to HTML with unknown system
    return ('html', 'unknown', has_code)


def detect_language_from_url(url: str) -> str:
    """
    Detect programming language from file extension in URL.
    Args:
        url: URL to analyze
    Returns:
        Language string (e.g., 'python', 'javascript', 'java')
    """
    extension_map = {
        '.py': 'python',
        '.js': 'javascript',
        '.ts': 'typescript',
        '.java': 'java',
        '.cpp': 'cpp',
        '.c': 'c',
        '.cs': 'csharp',
        '.go': 'go',
        '.rs': 'rust',
        '.php': 'php',
        '.rb': 'ruby',
        '.swift': 'swift',
        '.kt': 'kotlin',
        '.scala': 'scala',
        '.r': 'r',
        '.m': 'matlab',
        '.sql': 'sql',
        '.sh': 'bash',
        '.bash': 'bash',
        '.zsh': 'zsh',
        '.yml': 'yaml',
        '.yaml': 'yaml',
        '.json': 'json',
        '.xml': 'xml',
        '.html': 'html',
        '.css': 'css',
        '.md': 'markdown',
        '.txt': 'text'
    }
    for ext, lang in extension_map.items():
        if url.lower().endswith(ext):
            return lang
    return 'text'


def extract_html_code_blocks(html_content: str, doc_system: str) -> List[Dict[str, Any]]:
    """
    Extract code blocks from HTML based on documentation system.
    Args:
        html_content: HTML content to parse
        doc_system: Documentation system type ('sphinx', 'mkdocs', 'github_html', etc.)
    Returns:
        List of code blocks with metadata
    """
    if doc_system == 'sphinx' or doc_system == 'readthedocs':
        return extract_readthedocs_code_blocks(html_content)
    elif doc_system == 'mkdocs':
        return extract_mkdocs_code_blocks(html_content)
    elif doc_system == 'github_html':
        return extract_github_html_code_blocks(html_content)
    else:
        # Fallback to generic HTML code extraction
        return extract_generic_html_code_blocks(html_content)


def extract_mkdocs_code_blocks(html_content: str) -> List[Dict[str, Any]]:
    """
    Extract code blocks from MkDocs generated HTML.
    MkDocs patterns:
    - <pre><code class="language-python">...</code></pre>
    - <div class="highlight"><pre><span></span><code>...</code></pre></div>
    """
    code_blocks = []
    # Pattern 1: Standard MkDocs with language class
    pattern1 = r'<pre><code class="language-(\w+)">(.*?)</code></pre>'
    matches = re.finditer(pattern1, html_content, re.DOTALL)
    for i, match in enumerate(matches):
        language = match.group(1)
        code = html.unescape(match.group(2))
        code = re.sub(r'<[^>]+>', '', code)  # Remove any remaining HTML tags
        if code.strip():
            code_blocks.append({
                'code': code.strip(),
                'language': language,
                'type': 'mkdocs_code_block',
                'context_before': '',
                'context_after': '',
                'full_context': code.strip()
            })
    # Pattern 2: MkDocs with highlight div
    pattern2 = r'<div class="highlight"><pre><span></span><code>(.*?)</code></pre></div>'
    matches = re.finditer(pattern2, html_content, re.DOTALL)
    for i, match in enumerate(matches):
        code = html.unescape(match.group(1))
        code = re.sub(r'<span[^>]*>', '', code)
        code = re.sub(r'</span>', '', code)
        if code.strip():
            code_blocks.append({
                'code': code.strip(),
                'language': 'text',
                'type': 'mkdocs_code_block',
                'context_before': '',
                'context_after': '',
                'full_context': code.strip()
            })
    return code_blocks


def extract_github_html_code_blocks(html_content: str) -> List[Dict[str, Any]]:
    """
    Extract code blocks from GitHub's rendered HTML pages.
    GitHub patterns:
    - <td class="blob-code blob-code-inner">...</td>
    - <div class="highlight highlight-source-python">...</div>
    """
    code_blocks = []
    # Pattern for GitHub blob view
    pattern = r'<td class="blob-code blob-code-inner[^"]*">(.*?)</td>'
    matches = re.finditer(pattern, html_content, re.DOTALL)
    lines = []
    for match in matches:
        line = html.unescape(match.group(1))
        line = re.sub(r'<[^>]+>', '', line)
        lines.append(line)
    if lines:
        code = '\n'.join(lines)
        code_blocks.append({
            'code': code,
            'language': 'text',
            'type': 'github_blob',
            'context_before': '',
            'context_after': '',
            'full_context': code
        })
    return code_blocks


def extract_generic_html_code_blocks(html_content: str) -> List[Dict[str, Any]]:
    """
    Generic HTML code block extraction as fallback.
    Looks for:
    - <pre>...</pre>
    - <code>...</code>
    """
    code_blocks = []
    # Extract from <pre> tags
    pre_pattern = r'<pre[^>]*>(.*?)</pre>'
    matches = re.finditer(pre_pattern, html_content, re.DOTALL | re.IGNORECASE)
    for match in matches:
        code = html.unescape(match.group(1))
        code = re.sub(r'<[^>]+>', '', code)
        if code.strip() and len(code.strip()) > 10:
            code_blocks.append({
                'code': code.strip(),
                'language': 'text',
                'type': 'generic_pre',
                'context_before': '',
                'context_after': '',
                'full_context': code.strip()
            })
    return code_blocks


def extract_readthedocs_code_blocks(html_content: str, min_length: int = 3) -> List[Dict[str, Any]]:
    """
    Extract code blocks from ReadTheDocs/Sphinx documentation HTML without duplicates.
    ReadTheDocs typically uses nested structure:
    - <div class="highlight-{language} notranslate">
        <div class="highlight">
            <pre>...</pre>
        </div>
    </div>
    We extract each unique code block once, avoiding duplicates.
    Args:
        html_content: HTML content to parse
        min_length: Minimum length of code to consider (default: 10)
    Returns:
        List of unique code blocks with metadata
    """
    soup = BeautifulSoup(html_content, 'html.parser')
    code_blocks = []
    # Find all <pre> tags and track which ones we've processed
    all_pres = soup.find_all('pre')
    # Process each pre tag once, using content hash to avoid duplicates
    seen_codes = set()  # Track code content to avoid duplicates
    for i, pre in enumerate(all_pres):
        code = pre.get_text().strip()
        # Debug: show first few chars of each pre
        preview = code[:50] + "..." if len(code) > 50 else code
        # Skip if too short or already seen
        if not code:
            continue
        if len(code) < min_length:
            continue
        if code in seen_codes:
            continue
        seen_codes.add(code)
        # Determine language by looking at parent divs
        language = 'text'
        parent = pre.parent
        while parent:
            if parent.name == 'div' and parent.get('class'):
                classes = parent.get('class', [])
                for cls in classes:
                    if isinstance(cls, str) and cls.startswith('highlight-') and cls != 'highlight':
                        language = cls.replace('highlight-', '')
                        break
                if language != 'text':
                    break
            parent = parent.parent
        if language == 'text':
            pass
        # Find the topmost highlight container for context
        highlight_container = pre
        temp = pre.parent
        container_depth = 0
        while temp:
            if temp.name == 'div' and temp.get('class'):
                if any('highlight' in str(c) for c in temp.get('class', [])):
                    highlight_container = temp
                    container_depth += 1
                    temp = temp.parent
                else:
                    break
            else:
                break
        # Get context from surrounding elements
        context_before = ""
        context_after = ""
        # Get previous sibling text (often contains description)
        prev = highlight_container.find_previous_sibling()
        if prev:
            # Get text, but limit length
            prev_text = prev.get_text().strip()
            if prev_text:
                context_before = prev_text[:500]
        # Get next sibling text
        next_elem = highlight_container.find_next_sibling()
        if next_elem:
            next_text = next_elem.get_text().strip()
            if next_text:
                context_after = next_text[:500]
        code_blocks.append({
            'code': code,
            'language': language,
            'type': 'code',
            'context_before': context_before,
            'context_after': context_after
        })
    # Also check for literal-block elements (another Sphinx pattern)
    literal_blocks = soup.find_all(class_='literal-block')
    for i, block in enumerate(literal_blocks):
        code = block.get_text().strip()
        preview = code[:50] + "..." if len(code) > 50 else code
        # Only add if not already seen and substantial
        if not code:
            continue
        if len(code) < min_length:
            continue
        if code in seen_codes:
            continue
        seen_codes.add(code)
        # Get context
        context_before = ""
        context_after = ""
        prev_elem = block.find_previous_sibling()
        if prev_elem:
            context_before = prev_elem.get_text().strip()[:500]
        next_elem = block.find_next_sibling()
        if next_elem:
            context_after = next_elem.get_text().strip()[:500]
        code_blocks.append({
            'code': code,
            'language': 'text',
            'type': 'literal-block',
            'context_before': context_before,
            'context_after': context_after
        })
    return code_blocks


def extract_code_blocks(content: str, min_length: int = 3, url: str = None) -> List[Dict[str, Any]]:
    """
    Universal code block extractor that handles multiple content formats.
    Automatically detects content type and applies the most appropriate extraction method:
    - Jupyter notebooks (JSON format, even from GitHub URLs)
    - HTML documentation (ReadTheDocs/Sphinx, MkDocs, GitHub, etc.)
    - Markdown files (fenced blocks, command examples, code boxes)
    - Raw code files (from GitHub raw URLs)
    - Plain text with code patterns
    Uses specialized extractors when available (e.g., ReadTheDocs) with automatic
    fallback to general extractors. Removes duplicates and adds sequential indexing.
    Args:
        content: The content to extract code blocks from
        min_length: Minimum length of code blocks to extract (default: 3 character)
        url: Optional URL to help identify content type
    Returns:
        List of dictionaries containing code blocks with:
        - 'code': The extracted code content
        - 'language': Detected programming language
        - 'type': Source type (e.g., 'raw_file', 'notebook_cell')
        - 'index': Sequential number of the block
    """
    code_blocks = []
    # Detect content type and source
    content_type, doc_system, has_code = detect_content_type_and_source(content, url)
    # print(f"ðŸ” Detected content type: {content_type}, doc system: {doc_system}, has code: {has_code}")
    # Handle based on content type
    if content_type == 'raw_code':
        # This is a raw code file (e.g., from GitHub raw URL)
        language = detect_language_from_url(url) if url else 'text'
        code_blocks.append({
            'code': content,
            'language': language,
            'type': 'raw_file',
            'context_before': f'Source: {url}' if url else '',
            'context_after': '',
            'full_context': content
        })
    elif content_type == 'jupyter':
        # Handle Jupyter notebook
        if doc_system == 'github':
            # Check if this is raw JSON content (from raw GitHub URL)
            if content.strip().startswith('{') and '"cells"' in content[:1000]:
                # print("ðŸ” Using Jupyter notebook JSON extractor for raw GitHub URL...")
                notebook_blocks = extract_jupyter_notebook_cells(content, min_length)
                code_blocks.extend(notebook_blocks)
            else:
                # GitHub-rendered Jupyter notebook HTML - fallback to markdown extraction
                # print("ðŸ” Using markdown extraction for GitHub Jupyter HTML...")
                markdown_blocks = extract_markdown_code_blocks(content, min_length)
                code_blocks.extend(markdown_blocks)
        else:
            # Standard Jupyter notebook JSON format
            notebook_blocks = extract_jupyter_notebook_cells(content, min_length)
            code_blocks.extend(notebook_blocks)
    elif content_type == 'html' and has_code:
        # Priority extraction based on doc_system
        if doc_system == 'sphinx':  # This includes ReadTheDocs
            # print("ðŸ” Using Sphinx/ReadTheDocs specific extractor...")
            specific_blocks = extract_readthedocs_code_blocks(content, min_length)
            if specific_blocks:
                code_blocks.extend(specific_blocks)
                # print(f"âœ… Sphinx/ReadTheDocs extractor found {len(specific_blocks)} blocks")
            else:
                # Fallback to general HTML extraction
                # print("âš ï¸ Sphinx extractor found no blocks, falling back to general HTML...")
                html_blocks = extract_html_code_blocks(content, doc_system)
                code_blocks.extend(html_blocks)
        else:
            # General HTML extraction for other doc systems
            # print("ðŸ” Using general HTML extractor...")
            html_blocks = extract_html_code_blocks(content, doc_system)
            code_blocks.extend(html_blocks)
        # Final fallback: if still no blocks, try markdown extraction on HTML
        if not code_blocks:
            # print("ðŸ” No HTML code blocks found, trying markdown extraction as last resort...")
            markdown_blocks = extract_markdown_code_blocks(content, min_length)
            code_blocks.extend(markdown_blocks)
    elif content_type == 'markdown':
        # Extract markdown code blocks
        markdown_blocks = extract_markdown_code_blocks(content, min_length)
        code_blocks.extend(markdown_blocks)
        # Also extract command examples and code boxes
        command_blocks = extract_command_examples(content, min_length)
        filtered_command_blocks = [block for block in command_blocks if len(block['code']) < 5000]
        code_blocks.extend(filtered_command_blocks)
        codebox_blocks = extract_codebox_content(content, min_length)
        code_blocks.extend(codebox_blocks)
    else:
        # For plain text or unknown content, try all extraction methods
        # print("ðŸ” Unknown content type, trying all extraction methods...")
        # Try markdown extraction first
        markdown_blocks = extract_markdown_code_blocks(content, min_length)
        code_blocks.extend(markdown_blocks)
        # Try command extraction
        command_blocks = extract_command_examples(content, min_length)
        filtered_command_blocks = [block for block in command_blocks if len(block['code']) < 5000]
        code_blocks.extend(filtered_command_blocks)
        # Try codebox extraction
        codebox_blocks = extract_codebox_content(content, min_length)
        code_blocks.extend(codebox_blocks)
    # Add index to each block (no deduplication to preserve all code blocks)
    for i, block in enumerate(code_blocks):
        block['index'] = i + 1
    # print(f"ðŸŽ¯ Total code blocks extracted: {len(code_blocks)}")
    return code_blocks


def extract_jupyter_notebook_cells(markdown_content: str, min_length: int = 3) -> List[Dict[str, Any]]:
    """Extract code cells from Jupyter notebook JSON format"""
    import json
    code_blocks = []
    try:
        # Parse the JSON content
        notebook = json.loads(markdown_content)
        if 'cells' not in notebook:
            # print("No cells found in notebook")
            return code_blocks
        # Process each cell
        for i, cell in enumerate(notebook['cells']):
            cell_type = cell.get('cell_type', '')
            if cell_type == 'code':
                # Extract source code
                source = cell.get('source', [])
                if isinstance(source, list):
                    code_content = ''.join(source)
                else:
                    code_content = str(source)
                # Skip empty or very short code cells
                if len(code_content.strip()) < min_length:
                    continue
                # Get cell metadata
                metadata = cell.get('metadata', {})
                outputs = cell.get('outputs', [])
                # Extract context from surrounding cells
                context_before = ""
                context_after = ""
                # Get previous cell content (if any)
                if i > 0:
                    prev_cell = notebook['cells'][i-1]
                    prev_source = prev_cell.get('source', [])
                    if isinstance(prev_source, list):
                        context_before = ''.join(prev_source)[:500]
                    else:
                        context_before = str(prev_source)[:500]
                # Get next cell content (if any)
                if i < len(notebook['cells']) - 1:
                    next_cell = notebook['cells'][i+1]
                    next_source = next_cell.get('source', [])
                    if isinstance(next_source, list):
                        context_after = ''.join(next_source)[:500]
                    else:
                        context_after = str(next_source)[:500]
                code_blocks.append({
                    'code': code_content,
                    'language': 'python',
                    'type': 'jupyter_code_cell',
                    'cell_index': i,
                    'metadata': metadata,
                    'outputs': outputs,
                    'context_before': context_before,
                    'context_after': context_after,
                    'full_context': f"Cell {i}:\n{context_before}\n\n{code_content}\n\n{context_after}"
                })
            elif cell_type == 'markdown':
                # Extract markdown content for context
                source = cell.get('source', [])
                if isinstance(source, list):
                    markdown_content = ''.join(source)
                else:
                    markdown_content = str(source)
                # Look for code blocks within markdown cells
                if '```' in markdown_content:
                    markdown_blocks = extract_markdown_code_blocks(markdown_content, min_length)
                    for block in markdown_blocks:
                        block['cell_index'] = i
                        block['type'] = 'jupyter_markdown_code'
                        code_blocks.append(block)
    except json.JSONDecodeError as e:
        # print(f"Failed to parse Jupyter notebook JSON: {e}")
        pass
    except Exception as e:
        # print(f"Error processing Jupyter notebook: {e}")
        pass
    return code_blocks


def extract_smart_context_before(markdown_content: str, code_start_pos: int, max_chars: int = 1000) -> str:
    """
    Intelligently extract the most relevant context before a code block.
    This function looks for the most meaningful content preceding a code block:
    1. First tries to find the previous paragraph or heading
    2. If no paragraph found, looks for the previous section
    3. Falls back to a reasonable character limit if nothing else works
    Args:
        markdown_content: The full markdown content
        code_start_pos: Position where the code block starts
        max_chars: Maximum characters to extract (fallback limit)
    Returns:
        The most relevant context before the code block
    """
    if code_start_pos <= 0:
        return ""
    # Look backwards from the code block position
    search_start = max(0, code_start_pos - max_chars)
    search_content = markdown_content[search_start:code_start_pos]
    # Split into lines for analysis
    lines = search_content.split('\n')
    # Strategy 1: Find the most recent paragraph or heading
    relevant_lines = []
    for line in reversed(lines):
        line = line.strip()
        # Skip empty lines at the end
        if not line and not relevant_lines:
            continue
        # Stop if we hit a major section break
        if (line.startswith('#') or 
            line.startswith('---') or 
            line.startswith('===') or
            len(line) > 200):  # Very long lines are likely not relevant context
            break
        # Add this line to relevant context
        relevant_lines.insert(0, line)
        # Stop if we have enough context (2-3 lines is usually sufficient)
        if len(relevant_lines) >= 3:
            break
    # Join the relevant lines
    context = '\n'.join(relevant_lines).strip()
    # If we found meaningful context, return it
    if context and len(context) > 10:
        return context
    # Strategy 2: Fallback to character-based extraction with better boundaries
    # Look for natural paragraph breaks (double newlines)
    fallback_start = max(0, code_start_pos - max_chars)
    fallback_content = markdown_content[fallback_start:code_start_pos]
    # Find the last paragraph break
    last_break = fallback_content.rfind('\n\n')
    if last_break != -1:
        return fallback_content[last_break + 2:].strip()
    # Strategy 3: Final fallback - just take the last reasonable amount
    return fallback_content.strip()


def extract_smart_context_after(markdown_content: str, code_end_pos: int, max_chars: int = 1000) -> str:
    """
    Intelligently extract the most relevant context after a code block.
    This function looks for the most meaningful content following a code block:
    1. First tries to find the next paragraph or explanation
    2. If no paragraph found, looks for the next section
    3. Falls back to a reasonable character limit if nothing else works
    Args:
        markdown_content: The full markdown content
        code_end_pos: Position where the code block ends
        max_chars: Maximum characters to extract (fallback limit)
    Returns:
        The most relevant context after the code block
    """
    if code_end_pos >= len(markdown_content):
        return ""
    # Look forwards from the code block position
    search_end = min(len(markdown_content), code_end_pos + max_chars)
    search_content = markdown_content[code_end_pos:search_end]
    # Split into lines for analysis
    lines = search_content.split('\n')
    # Strategy 1: Find the next paragraph or heading
    relevant_lines = []
    for line in lines:
        line = line.strip()
        # Skip empty lines at the beginning
        if not line and not relevant_lines:
            continue
        # Stop if we hit a major section break
        if (line.startswith('#') or 
            line.startswith('---') or 
            line.startswith('===') or
            len(line) > 200):  # Very long lines are likely not relevant context
            break
        # Add this line to relevant context
        relevant_lines.append(line)
        # Stop if we have enough context (2-3 lines is usually sufficient)
        if len(relevant_lines) >= 3:
            break
    # Join the relevant lines
    context = '\n'.join(relevant_lines).strip()
    # If we found meaningful context, return it
    if context and len(context) > 10:
        return context
    # Strategy 2: Fallback to character-based extraction with better boundaries
    # Look for natural paragraph breaks (double newlines)
    fallback_end = min(len(markdown_content), code_end_pos + max_chars)
    fallback_content = markdown_content[code_end_pos:fallback_end]
    # Find the next paragraph break
    next_break = fallback_content.find('\n\n')
    if next_break != -1:
        return fallback_content[:next_break].strip()
    # Strategy 3: Final fallback - just take the next reasonable amount
    return fallback_content.strip()


def extract_markdown_code_blocks(markdown_content: str, min_length: int = 3) -> List[Dict[str, Any]]:
    """Extract markdown code blocks (```) with intelligent context extraction"""
    code_blocks = []
    # Skip if content starts with triple backticks (edge case for files wrapped in backticks)
    content = markdown_content.strip()
    start_offset = 0
    if content.startswith('```'):
        # Skip the first triple backticks
        start_offset = 3
        # print("Skipping initial triple backticks")
    # Find all occurrences of triple backticks
    backtick_positions = []
    pos = start_offset
    while True:
        pos = markdown_content.find('```', pos)
        if pos == -1:
            break
        backtick_positions.append(pos)
        pos += 3
    # Process pairs of backticks
    i = 0
    while i < len(backtick_positions) - 1:
        start_pos = backtick_positions[i]
        end_pos = backtick_positions[i + 1]
        # Extract the content between backticks
        code_section = markdown_content[start_pos+3:end_pos]
        # Check if there's a language specifier on the first line
        lines = code_section.split('\n', 1)
        if len(lines) > 1:
            # Check if first line is a language specifier (no spaces, common language names)
            first_line = lines[0].strip()
            if first_line and not ' ' in first_line and len(first_line) < 20:
                language = first_line
                code_content = lines[1].strip() if len(lines) > 1 else ""
            else:
                language = ""
                code_content = code_section.strip()
        else:
            language = ""
            code_content = code_section.strip()
        # Only skip if code block is extremely short (likely empty or just whitespace)
        if len(code_content.strip()) < min_length:
            i += 2  # Move to next pair
            continue
        # Use intelligent context extraction instead of fixed character count
        context_before = extract_smart_context_before(markdown_content, start_pos)
        context_after = extract_smart_context_after(markdown_content, end_pos + 3)
        code_blocks.append({
            'code': code_content,
            'language': language,
            'type': 'markdown_code_block',
            'context_before': context_before,
            'context_after': context_after,
            'full_context': f"{context_before}\n\n{code_content}\n\n{context_after}"
        })
        # Move to next pair (skip the closing backtick we just processed)
        i += 2
    return code_blocks


def extract_command_examples(markdown_content: str, min_length: int = 3) -> List[Dict[str, Any]]:
    """Extract command line examples"""
    command_blocks = []
    # Match command line patterns with better boundary detection
    # Look for lines starting with '>' followed by command and output
    pattern = r'>\s*(\w+.*?)\n((?:[^>].*\n?)*?)(?=\n>|\n\n|\n[A-Z][a-z]|\n#|\n##|\n###|\n####|\n#####|\n######|$)'
    matches = re.finditer(pattern, markdown_content, re.DOTALL)
    for match in matches:
        command = match.group(1).strip()
        output = match.group(2).strip()
        # Additional filtering: ensure output doesn't contain new command patterns
        # and doesn't extend too far into the document
        lines = output.split('\n')
        filtered_lines = []
        for line in lines:
            # Stop if we hit another command pattern or section header
            if (line.strip().startswith('>') or 
                line.strip().startswith('#') or
                re.match(r'^[A-Z][a-z].*:', line.strip()) or
                len(line.strip()) > 200):  # Very long lines are likely not command output
                break
            filtered_lines.append(line)
        output = '\n'.join(filtered_lines).strip()
        if len(output.strip()) >= min_length:
            # Use intelligent context extraction instead of fixed character count
            context_before = extract_smart_context_before(markdown_content, match.start())
            context_after = extract_smart_context_after(markdown_content, match.end())
            command_blocks.append({
                'code': f"{command}\n{output}",
                'language': 'shell',
                'type': 'command_example',
                'command': command,
                'output': output,
                'context_before': context_before,
                'context_after': context_after,
                'full_context': f"{context_before}\n\n{command}\n{output}\n\n{context_after}"
            })
    return command_blocks


def extract_codebox_content(markdown_content: str, min_length: int = 3) -> List[Dict[str, Any]]:
    """Extract content from code boxes and copyable blocks"""
    codebox_blocks = []
    # Common patterns for code boxes
    patterns = [
        r'Copy\s*\n(.*?)(?=\n\n|\n[A-Z]|\n#|\n##|\n###|\n####|\n#####|\n######|$)',
        r'```\s*copy\s*\n(.*?)\n```',
        r'<code[^>]*>(.*?)</code>',
        r'<pre[^>]*>(.*?)</pre>'
    ]
    for pattern in patterns:
        matches = re.finditer(pattern, markdown_content, re.DOTALL | re.IGNORECASE)
        for match in matches:
            code_content = match.group(1).strip()
            if len(code_content.strip()) >= min_length:
                # Use intelligent context extraction instead of fixed character count
                context_before = extract_smart_context_before(markdown_content, match.start())
                context_after = extract_smart_context_after(markdown_content, match.end())
                codebox_blocks.append({
                    'code': code_content,
                    'language': 'text',
                    'type': 'codebox',
                    'context_before': context_before,
                    'context_after': context_after,
                    'full_context': f"{context_before}\n\n{code_content}\n\n{context_after}"
                })
    return codebox_blocks


def generate_code_example_summary(code: str, context_before: str, context_after: str) -> str:
    """
    Generate a summary for a code example using its surrounding context.
    Args:
        code: The code example
        context_before: Context before the code
        context_after: Context after the code
    Returns:
        A summary of what the code example demonstrates
    """
    model_choice = os.getenv("OPENAI_SUMMARY_MODEL", "gpt-4.1-nano")
    # Create the prompt
    prompt = f"""<context_before>
{context_before[-1000:] if len(context_before) > 1000 else context_before}
</context_before>
<code_example>
{code[:3000] if len(code) > 3000 else code}
</code_example>
<context_after>
{context_after[:1000] if len(context_after) > 1000 else context_after}
</context_after>
Based on the code example and its surrounding context, provide a concise summary (2-3 sentences) that describes what this code example demonstrates and its purpose. Focus on the practical application and key concepts illustrated.
"""
#     prompt = f"""<context_before>
# {context_before}
# </context_before>
# <code_example>
# {code}
# </code_example>
# <context_after>
# {context_after}
# </context_after>
# Based on the code example and its surrounding context, provide a concise summary (2-3 sentences) that describes what this code example demonstrates and its purpose. Focus on the practical application and key concepts illustrated.
# """
    try:
        response = openai.chat.completions.create(
            model=model_choice,
            messages=[
                {"role": "system", "content": "You are a helpful assistant that provides concise code example summaries."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=100
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"Error generating code example summary: {e}")
        return "Code example for demonstration purposes."


async def search_code_blocks(
    client: Client, 
    query: str, 
    match_count: int = 10, 
    filter_metadata: Optional[Dict[str, Any]] = None
) -> List[Dict[str, Any]]:
    """
    Search for relevant code blocks using semantic similarity.
    Args:
        client: Supabase client instance
        query: Search query
        match_count: Number of matches to return
        filter_metadata: Optional metadata filters (e.g., {"language": "python"})
    Returns:
        List of relevant code blocks with similarity scores
    """
    try:
        # Create embedding for the query
        query_embedding = await create_embedding(query)
        
        # Build the search query
        def _run_rpc():
            return client.rpc(
                'match_code_blocks',
                {
                    'query_embedding': query_embedding,
                    'match_count': match_count,
                    'filter_metadata': filter_metadata or {}
                }
            ).execute()
        result = await asyncio.to_thread(_run_rpc)
        if result.data:
            # Convert the results back to code block format
            code_blocks = []
            for record in result.data:
                code_block = {
                    "code": record["code_text"],
                    "summary": record["summary"],
                    "context_before": record["context_before"],
                    "context_after": record["context_after"],
                    "type": record["code_type"],
                    "language": record["language"],
                    "index": record["index"],
                    "source_url": record["url"],
                    "similarity_score": record.get("similarity", 0.0)
                }
                code_blocks.append(code_block)
            return code_blocks
        else:
            return []
    except Exception as e:
        print(f"Error searching code blocks: {e}")
        return []


async def check_extracted_code_exists(client: Client, url: str) -> bool:
    """
    Check if code has already been extracted from a given URL.
    Args:
        client: Supabase client instance
        url: URL to check
    Returns:
        True if code exists for this URL, False otherwise
    """
    try:
        def _run():
            return client.table("extracted_code").select("url").eq("url", url).limit(1).execute()
        result = await asyncio.to_thread(_run)
        return len(result.data) > 0
    except Exception as e:
        print(f"Error checking extracted code existence: {e}")
        return False


def _format_embedding_for_vector_column(embedding: List[float]) -> str:
    """
    Convert a Python list of floats to PostgreSQL vector format.
    
    Args:
        embedding: List of float values
        
    Returns:
        String in PostgreSQL vector format: [1.0,2.0,3.0]
    """
    return '[' + ','.join(map(str, embedding)) + ']'


async def save_extracted_code_to_supabase(
    client: Client,
    url: str,
    extracted_code: List[Dict[str, Any]],
    total_code_blocks_found: int,
    extraction_method: str = "single_page"
) -> bool:
    """
    Save extracted code blocks to Supabase with batch operations.
    Args:
        client: Supabase client instance
        url: Source URL of the code
        extracted_code: List of extracted code blocks
        total_code_blocks_found: Total number of code blocks found
        extraction_method: Method used for extraction
    Returns:
        Boolean indicating success
    """
    try:
        if not extracted_code:
            # print(f"No code blocks to save for {url}")
            return True
        # Prepare embedding texts for all code blocks
        embedding_texts = []
        for code_block in extracted_code:
            code_text = code_block.get('code', '')
            summary = code_block.get('summary', '')
            context_before = code_block.get('context_before', '')
            context_after = code_block.get('context_after', '')
            # Combine relevant text for embedding
            embedding_text = f"Code: {code_text}\nSummary: {summary}\nContext: {context_before} {context_after}"
            embedding_texts.append(embedding_text)
        # print(f"Creating embeddings for {len(embedding_texts)} code blocks...")
        # Generate embeddings in batch
        embeddings = await create_embeddings_batch(embedding_texts)
        # Prepare batch data for insertion
        batch_data = []
        for i, code_block in enumerate(extracted_code):
            # Generate summary if enabled
            summary = ""
            if os.getenv("GENERATE_CODE_SUMMARY", "false").lower() == "true":
                summary = generate_code_example_summary(
                    code_block.get('code', ''),
                    code_block.get('context_before', ''),
                    code_block.get('context_after', '')
                )
            # Convert embedding to proper PostgreSQL vector format
            embedding = embeddings[i] if i < len(embeddings) else [0.0] * 1536
            data = {
                "url": url,
                "code_text": code_block.get('code', ''),
                "summary": summary or code_block.get('summary', ''),
                "context_before": code_block.get('context_before', ''),
                "context_after": code_block.get('context_after', ''),
                "code_type": code_block.get('type', ''),
                "language": code_block.get('language', ''),
                "index": code_block.get('index', 0),
                "extraction_method": extraction_method,
                "embedding": _format_embedding_for_vector_column(embedding)
            }
            batch_data.append(data)
        # Batch insert to Supabase with conflict resolution
        # print(f"Inserting {len(batch_data)} code blocks to database...")
        def _insert_batch():
            return client.table("extracted_code").upsert(batch_data, on_conflict="url,index").execute()
        result = await asyncio.to_thread(_insert_batch)
        if result.data:
            # print(f"âœ… Successfully saved {len(result.data)}/{total_code_blocks_found} code blocks for {url}")
            return True
        else:
            # print(f"âŒ Failed to save code blocks for {url}")
            return False
    except Exception as e:
        print(f"Error saving extracted code to Supabase: {e}")
        # Fallback to individual insertion if batch fails
        try:
            print("Attempting fallback to individual insertion...")
            success_count = 0
            for i, code_block in enumerate(extracted_code):
                try:
                    # Generate embedding for this specific block
                    embedding_text = f"Code: {code_block.get('code', '')}\nContext: {code_block.get('context_before', '')} {code_block.get('context_after', '')}"
                    embedding = await create_embedding(embedding_text)
                    data = {
                        "url": url,
                        "code_text": code_block.get('code', ''),
                        "summary": code_block.get('summary', ''),
                        "context_before": code_block.get('context_before', ''),
                        "context_after": code_block.get('context_after', ''),
                        "code_type": code_block.get('type', ''),
                        "language": code_block.get('language', ''),
                        "index": code_block.get('index', 0),
                        "extraction_method": extraction_method,
                        "embedding": _format_embedding_for_vector_column(embedding)
                    }
                    def _insert_one():
                        return client.table("extracted_code").upsert(data, on_conflict="url,index").execute()
                    result = await asyncio.to_thread(_insert_one)
                    if result.data:
                        success_count += 1
                except Exception as individual_error:
                    print(f"Failed to insert code block {i}: {individual_error}")
                    continue
            print(f"âœ… Fallback saved {success_count}/{len(extracted_code)} code blocks")
            return success_count > 0
        except Exception as fallback_error:
            print(f"Fallback insertion also failed: {fallback_error}")
            return False


async def get_extracted_code_from_supabase(client: Client, url: str) -> Optional[List[Dict[str, Any]]]:
    """
    Retrieve extracted code from Supabase for a given URL.
    Args:
        client: Supabase client instance
        url: URL to retrieve code for
    Returns:
        List of code blocks, or None if not found
    """
    try:
        def _run():
            return client.table("extracted_code").select("*").eq("url", url).order("index").execute()
        result = await asyncio.to_thread(_run)
        if result.data and len(result.data) > 0:
            # Convert each record back to the original code block format
            code_blocks = []
            for record in result.data:
                code_block = {
                    "code": record["code_text"],
                    "summary": record["summary"],
                    "context_before": record["context_before"],
                    "context_after": record["context_after"],
                    "type": record["code_type"],
                    "language": record["language"],
                    "index": record["index"],
                    "source_url": record["url"]
                }
                code_blocks.append(code_block)
            return code_blocks
        else:
            return None
    except Exception as e:
        print(f"Error retrieving extracted code from Supabase: {e}")
        return None


async def clear_extracted_code_table(client: Client) -> bool:
    """
    Clear all records from the extracted_code table.
    Args:
        client: Supabase client instance
    Returns:
        True if successful, False otherwise
    """
    try:
        # First, get the count of records before deletion
        def _count():
            return client.table("extracted_code").select("id", count="exact").execute()
        count_result = await asyncio.to_thread(_count)
        count = count_result.count if count_result.count is not None else 0
        # Delete all records
        def _delete():
            return client.table("extracted_code").delete().neq("id", 0).execute()
        result = await asyncio.to_thread(_delete)
        print(f"   âœ… Cleared {count} records from extracted_code table")
        return True
    except Exception as e:
        print(f"   âŒ Error clearing extracted_code table: {e}")
        return False

```


================================================================================
=== FILE: mcp_servers_and_tools\workspace_server\Dockerfile ===
================================================================================

```
# Start with a Node.js base image
FROM node:18-alpine AS builder
# Create a directory for the app
WORKDIR /app
# Copy package.json and package-lock.json for installing dependencies
COPY package.json package-lock.json ./
# Install dependencies
RUN npm install --ignore-scripts
# Copy the rest of the application source code
COPY . .
# Build the project
RUN npm run build

# Use the same Node.js base image for the final container
FROM node:18-alpine
# Set the working directory
WORKDIR /app
# Copy the build output and necessary files from the builder stage
COPY --from=builder /app/build /app/build
COPY --from=builder /app/package.json /app/package.json
COPY --from=builder /app/package-lock.json /app/package-lock.json
COPY --from=builder /app/node_modules /app/node_modules

# Install Python and required tools
RUN apk add --no-cache python3 py3-pip curl bash

# Download and install uv using the official installer
ADD https://astral.sh/uv/install.sh /uv-installer.sh
RUN sh /uv-installer.sh && rm /uv-installer.sh

# Ensure the installed binary is on the PATH
ENV PATH="/root/.local/bin:$PATH"

# Create required directories
RUN mkdir -p /app/generated_code
RUN mkdir -p /app/saved_files
RUN mkdir -p /app/.venvs/ai
RUN mkdir -p /app/forbidden

# Create a virtual environment
RUN uv venv /app/.venvs/ai

# Set the environment variables
ENV CODE_STORAGE_DIR=/app/generated_code
ENV SAVED_FILES_DIR=/app/saved_files
ENV ENV_TYPE=venv-uv
ENV UV_VENV_PATH=/app/.venvs/ai
ENV PROJECT_ROOT=/app
ENV FORBIDDEN_PATH=/app/forbidden
ENV PATH="/app/.venvs/ai/bin:$PATH"

# Specify the command to run the Workspace Server
ENTRYPOINT ["node", "build/index.js"]

```


================================================================================
=== FILE: mcp_servers_and_tools\workspace_server\LICENSE ===
================================================================================

```
MIT License

Copyright (c) 2025 bazinga012

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

```


================================================================================
=== FILE: mcp_servers_and_tools\workspace_server\package.json ===
================================================================================

```json
{
  "name": "workspace-server",
  "version": "0.1.0",
  "description": "Workspace server for code execution and file management",
  "private": true,
  "type": "module",
  "bin": {
    "workspace-server": "./build/index.js"
  },
  "files": [
    "build"
  ],
  "scripts": {
    "build": "tsc && chmod +x build/index.js",
    "prepare": "npm run build",
    "watch": "tsc --watch",
    "inspector": "npx @modelcontextprotocol/inspector build/index.js"
  },
  "dependencies": {
    "@modelcontextprotocol/sdk": "0.6.0",
    "mcp-framework": "^0.1.12"
  },
  "devDependencies": {
    "@types/node": "^20.11.24",
    "typescript": "^5.8.3"
  }
}

```


================================================================================
=== FILE: mcp_servers_and_tools\workspace_server\README.md ===
================================================================================

```markdown
# Workspace Server

A specialized MCP server designed for comprehensive workspace management and code execution workflows. Provides powerful tools for executing code, managing files, and handling dependencies in various Python environments.

## Features

- Execute Python code from LLM prompts with automatic file management
- Read and save files with flexible path handling and security restrictions
- Install and manage Python dependencies using multiple package managers
- Check package installations and versions with intelligent name variation handling
- Execute shell commands and scripts with automatic shebang insertion
- Support for multiple Python environments (Conda, virtualenv, UV virtualenv)
- Configurable code storage and file directories
- Built-in security features to protect sensitive directories

## Prerequisites

- Node.js installed
- One of the following:
  - Conda installed with desired Conda environment created
  - Python virtualenv
  - UV virtualenv

## Setup

1. Navigate to the workspace_server directory:

```bash
cd mcp_servers_and_tools/workspace_server
```

2. Install the Node.js dependencies:

```bash
npm install
```

3. Build the project:

```bash
npm run build
```

## Configuration

To configure the Workspace Server, add the following to your MCP servers configuration file:

### Using Node.js

```json
{
  "mcpServers": {
    "workspace-server": {
      "command": "node",
      "args": [
        "/path/to/mcp_servers_and_tools/workspace_server/build/index.js" 
      ],
      "env": {
        "CODE_STORAGE_DIR": "/path/to/code/storage",
        "SAVED_FILES_DIR": "/path/to/saved/files",
        "PROJECT_ROOT": "/path/to/your/project/root",
        "FORBIDDEN_PATH": "/path/to/forbidden/directory",
        "ENV_TYPE": "conda",
        "CONDA_ENV_NAME": "your-conda-env"
      }
    }
  }
}
```

### Using Docker

Build the Docker image:
```bash
docker build -t workspace-server .
```

Run with default settings:
```json
{
  "mcpServers": {
    "workspace-server": {
      "command": "docker",
      "args": ["run", "-i", "--rm", "workspace-server"]
    }
  }
}
```

Or override environment variables:
```bash
docker run -i --rm \
  -e PROJECT_ROOT=/app \
  -e FORBIDDEN_PATH=/app/forbidden \
  -e CODE_STORAGE_DIR=/app/generated_code \
  workspace-server
```

### Environment Variables

#### Required Variables
- `CODE_STORAGE_DIR`: Directory where the generated code will be stored
- `SAVED_FILES_DIR`: Directory where saved files will be stored
- `PROJECT_ROOT`: Absolute path to the project root directory (used for security boundaries)
- `FORBIDDEN_PATH`: Absolute path to a directory that agents are forbidden from accessing (e.g., benchmark directory)

#### Optional Variables
- `ENV_TYPE`: Type of Python environment (`conda`, `venv`, or `venv-uv`)
- `CONDA_ENV_NAME`: Name of the Conda environment (required if `ENV_TYPE=conda`)
- `VENV_PATH`: Path to the virtualenv (required if `ENV_TYPE=venv`)
- `UV_VENV_PATH`: Path to the UV virtualenv (required if `ENV_TYPE=venv-uv`)

## Available Tools

1. **`execute_code`**: Execute Python code in the configured environment. Code is saved to a temporary file and executed.
2. **`read_file`**: Read the content of any text file (Python code, log files, output files, etc.). Use absolute paths for best results.
3. **`install_dependencies`**: Install Python dependencies using the appropriate package manager (pip, uv, or conda).
4. **`check_installed_packages`**: List all installed packages in the current Python environment.
5. **`check_package_version`**: Check specific package versions, installation paths, and module locations. Automatically handles package name variations (hyphens, underscores, dots).
6. **`save_file`**: Save a file to the SAVED_FILES_DIR with the specified filename.
7. **`execute_shell_command`**: Execute a shell command with configurable working directory.
8. **`create_and_execute_script`**: Create and execute a shell script. Automatically adds shebang if not present.

## Security Features

- **Project Root Restriction**: All file operations are restricted to the `PROJECT_ROOT` directory to prevent access to files outside the project scope.
- **Forbidden Path Protection**: Agents are explicitly forbidden from accessing the `FORBIDDEN_PATH` directory (e.g., benchmark directory) to prevent accidental modifications to sensitive data.
- **Path Validation**: Comprehensive path checking to ensure operations are performed in safe locations.
- **Command Security**: Dangerous shell commands (like `rm -rf`, `sudo`, etc.) are blocked for security.

```


================================================================================
=== FILE: mcp_servers_and_tools\workspace_server\smithery.yaml ===
================================================================================

```yaml
# Smithery configuration file: https://smithery.ai/docs/config#smitheryyaml

startCommand:
  type: stdio
  configSchema:
    # JSON Schema defining the configuration options for the MCP.
    type: object
    required:
      - codeStorageDir
      - savedFilesDir
      - projectRoot
      - forbiddenPath
    properties:
      codeStorageDir:
        type: string
        description: Directory where generated code files will be stored.
      savedFilesDir:
        type: string
        description: Directory where saved files will be stored.
      projectRoot:
        type: string
        description: Absolute path to the project root directory for security restrictions.
      forbiddenPath:
        type: string
        description: Absolute path to the forbidden directory (e.g., benchmark) that should not be accessed.
      envType:
        type: string
        enum: ["conda", "venv", "venv-uv"]
        description: Type of Python environment to use.
        default: "conda"
      condaEnvName:
        type: string
        description: Name of the Conda environment for code execution (required if envType is conda).
      venvPath:
        type: string
        description: Path to the virtualenv (required if envType is venv).
      uvVenvPath:
        type: string
        description: Path to the UV virtualenv (required if envType is venv-uv).
  commandFunction:
    # A function that produces the CLI command to start the MCP on stdio.
    |-
    (config) => {
      const env = {
        CODE_STORAGE_DIR: config.codeStorageDir,
        SAVED_FILES_DIR: config.savedFilesDir,
        PROJECT_ROOT: config.projectRoot,
        FORBIDDEN_PATH: config.forbiddenPath,
        ENV_TYPE: config.envType || "conda"
      };
      
      if (config.condaEnvName) env.CONDA_ENV_NAME = config.condaEnvName;
      if (config.venvPath) env.VENV_PATH = config.venvPath;
      if (config.uvVenvPath) env.UV_VENV_PATH = config.uvVenvPath;
      
      return {command:'node',args:['build/index.js'],env:env};
    }

```


================================================================================
=== FILE: mcp_servers_and_tools\workspace_server\test_workspace_security.py ===
================================================================================

```python
#!/usr/bin/env python3
"""
Security test for workspace server MCP tools.
Tests that all tools are properly restricted to PROJECT_ROOT and forbidden from accessing FORBIDDEN_PATH.
"""

import asyncio
import json
import os
import sys
from pathlib import Path

try:
    from mcp import ClientSession, StdioServerParameters
    from mcp.client.stdio import stdio_client
except ImportError:
    print("âŒ Need to install MCP client library")
    print("Please run: pip install mcp")
    sys.exit(1)

def get_project_root():
    """Get project root by looking for .git directory"""
    current_path = Path(__file__).resolve()
    while not (current_path / ".git").exists() and current_path.parent != current_path:
        current_path = current_path.parent
    if (current_path / ".git").exists():
        return current_path
    else:
        raise FileNotFoundError("Could not find project root (no .git directory found)")

def get_workspace_server_params():
    project_root = get_project_root()
    workspace_server_path = project_root / "mcp_servers_and_tools/workspace_server" / "build" / "index.js"
    temp_code_dir = project_root / "deep_solver_benchmark" / "temp_code"
    saved_files_dir = project_root / "deep_solver_benchmark" / "saved_code"
    venv_path = project_root / ".venv"
    
    # Create directories if they don't exist
    temp_code_dir.mkdir(parents=True, exist_ok=True)
    saved_files_dir.mkdir(parents=True, exist_ok=True)
    
    # Prefer user-space Node 18+ via nvm if available
    nvm_node = Path.home() / ".nvm" / "versions" / "node" / "v18.20.8" / "bin" / "node"
    node_cmd = str(nvm_node) if nvm_node.exists() else "node"
    
    return StdioServerParameters(
        command=node_cmd,
        args=[str(workspace_server_path)],
        cwd=str(project_root),
        env={
            **os.environ,  # Pass all current environment variables
            "CODE_STORAGE_DIR": str(temp_code_dir),
            "SAVED_FILES_DIR": str(saved_files_dir),
            "ENV_TYPE": "venv",
            "VENV_PATH": str(venv_path),
            "PROJECT_ROOT": str(project_root),
            "FORBIDDEN_PATH": str(project_root / "benchmark_tasks_and_results"),
            "MCP_QUIET": "1",
            "NODE_ENV": "production"
        }
    )

# Test configuration
project_root = get_project_root()
PROJECT_ROOT = str(project_root)
FORBIDDEN_PATH = str(project_root / "benchmark_tasks_and_results")
HOME_DIR = os.path.expanduser("~")  # User's home directory

print(f"ðŸ”’ Security Test Configuration:")
print(f"   PROJECT_ROOT: {PROJECT_ROOT}")
print(f"   FORBIDDEN_PATH: {FORBIDDEN_PATH}")
print(f"   HOME_DIR: {HOME_DIR}")
print()

async def test_tool_security():
    """Test security restrictions for all workspace server tools."""
    
    # Start the workspace server
    server_params = get_workspace_server_params()
    
    try:
        async with stdio_client(server_params) as (read, write):
            async with ClientSession(read, write) as session:
                print("âœ… Successfully connected to workspace-server")
                
                # List available tools
                tools_result = await session.list_tools()
                available_tools = {tool.name: tool for tool in tools_result.tools}
                print(f"ðŸ“‹ Available tools: {list(available_tools.keys())}")
                print()
                
                # Test cases for each tool (only tools that actually exist in workspace server)
                test_cases = [
                    {
                        "tool": "execute_code",
                        "safe_params": {"code": "print('Hello, World!')", "filename": "test_safe.py"},
                        "unsafe_home_params": {"code": "print('Hello, World!')", "filename": f"{HOME_DIR}/test_home.py"},
                        "unsafe_forbidden_params": {"code": "print('Hello, World!')", "filename": f"{FORBIDDEN_PATH}/test_forbidden.py"},
                        "description": "Execute code tool"
                    },
                    {
                        "tool": "read_file",
                        "safe_params": {"file_path": f"{PROJECT_ROOT}/deep_solver_benchmark/deep_solver/output_types.py"},
                        "unsafe_home_params": {"file_path": f"{HOME_DIR}/.bashrc"},
                        "unsafe_forbidden_params": {"file_path": f"{FORBIDDEN_PATH}/test.txt"},
                        "description": "Read file tool"
                    },
                    {
                        "tool": "save_file", 
                        "safe_params": {"filename": "test_safe.py", "content": "# Safe test file"},
                        "unsafe_home_params": {"filename": f"{HOME_DIR}/test_home.py", "content": "# Test in home"},
                        "unsafe_forbidden_params": {"filename": f"{FORBIDDEN_PATH}/test_forbidden.py", "content": "# Test in forbidden"},
                        "description": "Save file tool"
                    },
                    {
                        "tool": "execute_shell_command",
                        "safe_params": {"command": "cd mcp_servers_and_tools && ls", "working_dir": PROJECT_ROOT},
                        "unsafe_home_params": {"command": "ls", "working_dir": HOME_DIR},
                        "unsafe_forbidden_params": {"command": "ls", "working_dir": FORBIDDEN_PATH},
                        "description": "Execute shell command tool"
                    },
                    {
                        "tool": "create_and_execute_script",
                        "safe_params": {"script_content": "echo 'Safe script'", "filename": "safe_script.sh"},
                        "unsafe_home_params": {"script_content": "echo 'Unsafe script'", "filename": f"{HOME_DIR}/unsafe_script.sh"},
                        "unsafe_forbidden_params": {"script_content": "echo 'Forbidden script'", "filename": f"{FORBIDDEN_PATH}/forbidden_script.sh"},
                        "description": "Create and execute script tool"
                    }
                ]
                
                # Run security tests
                for test_case in test_cases:
                    tool_name = test_case["tool"]
                    if tool_name not in available_tools:
                        print(f"âš ï¸  Tool '{tool_name}' not available, skipping...")
                        continue
                    
                    print(f"ðŸ” Testing {tool_name} ({test_case['description']})")
                    print("=" * 60)
                    
                    # Test 1: Safe operation (should succeed)
                    print(f"âœ… Test 1: Safe operation in PROJECT_ROOT")
                    print(f"   Parameters: {test_case['safe_params']}")
                    try:
                        result = await session.call_tool(tool_name, test_case["safe_params"])
                        print(f"   Raw Result: {result.content[0].text}")
                    except Exception as e:
                        print(f"   Exception: {e}")
                    print()
                    
                    # Test 2: Unsafe operation - HOME directory (should be blocked)
                    print(f"ðŸš« Test 2: Unsafe operation - HOME directory access")
                    print(f"   Parameters: {test_case['unsafe_home_params']}")
                    try:
                        result = await session.call_tool(tool_name, test_case["unsafe_home_params"])
                        print(f"   Raw Result: {result.content[0].text}")
                    except Exception as e:
                        print(f"   Exception: {e}")
                    print()
                    
                    # Test 3: Unsafe operation - FORBIDDEN_PATH (should be blocked)
                    print(f"ðŸš« Test 3: Unsafe operation - FORBIDDEN_PATH access")
                    print(f"   Parameters: {test_case['unsafe_forbidden_params']}")
                    try:
                        result = await session.call_tool(tool_name, test_case["unsafe_forbidden_params"])
                        print(f"   Raw Result: {result.content[0].text}")
                    except Exception as e:
                        print(f"   Exception: {e}")
                    print()
                    
                    print("-" * 60)
                    print()
                
                print("ðŸŽ¯ Security test completed!")
                print("=" * 60)
                print("SUMMARY:")
                print("- All tool calls and their raw results are displayed above")
                print("- You can now analyze the results to verify security restrictions")
                print("- Safe operations in PROJECT_ROOT should succeed")
                print("- Unsafe operations (HOME/FORBIDDEN_PATH) should be blocked")
                
    except Exception as e:
        print(f"âŒ Security test failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(test_tool_security())

```


================================================================================
=== FILE: mcp_servers_and_tools\workspace_server\test_workspace_server.py ===
================================================================================

```python
#!/usr/bin/env python3
"""
Test workspace-server MCP server functionality.

Tested Tools:
1. check_installed_packages - List all installed packages
2. install_dependencies - Install Python dependencies
3. check_package_version - Check specific package versions
4. execute_code - Execute Python code
5. read_file - Read file content
6. save_file - Save file content
7. execute_shell_command - Execute shell commands
8. create_and_execute_script - Create and execute scripts
"""

import asyncio
import sys
import json
import os
from pathlib import Path

try:
    from mcp import ClientSession, StdioServerParameters
    from mcp.client.stdio import stdio_client
except ImportError:
    print("âŒ Need to install MCP client library")
    print("Please run: pip install mcp")
    sys.exit(1)

def get_project_root():
    """Get project root by looking for .git directory"""
    current_path = Path(__file__).resolve()
    while not (current_path / ".git").exists() and current_path.parent != current_path:
        current_path = current_path.parent
    if (current_path / ".git").exists():
        return current_path
    else:
        raise FileNotFoundError("Could not find project root (no .git directory found)")

def get_workspace_server_params():
    project_root = get_project_root()
    workspace_server_path = project_root / "mcp_servers_and_tools/workspace_server" / "build" / "index.js"
    temp_code_dir = project_root / "deep_solver_benchmark" / "temp_code"
    saved_files_dir = project_root / "deep_solver_benchmark" / "saved_code"
    venv_path = project_root / ".venv"
    
    # Create directories if they don't exist
    temp_code_dir.mkdir(parents=True, exist_ok=True)
    saved_files_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"get_workspace_server_params:")
    print(f"  project_root: {project_root}")
    print(f"  workspace_server_path: {workspace_server_path}")
    print(f"  temp_code_dir: {temp_code_dir}")
    print(f"  saved_files_dir: {saved_files_dir}")
    print(f"  venv_path: {venv_path}")
    
    # Prefer user-space Node 18+ via nvm if available
    nvm_node = Path.home() / ".nvm" / "versions" / "node" / "v18.20.8" / "bin" / "node"
    node_cmd = str(nvm_node) if nvm_node.exists() else "node"
    
    return StdioServerParameters(
        command=node_cmd,
        args=[str(workspace_server_path)],
        cwd=str(project_root),
        env={
            **os.environ,  # Pass all current environment variables
            "CODE_STORAGE_DIR": str(temp_code_dir),
            "SAVED_FILES_DIR": str(saved_files_dir),
            "ENV_TYPE": "venv",
            "VENV_PATH": str(venv_path),
            "PROJECT_ROOT": str(project_root),  # Explicitly set project root
            "FORBIDDEN_PATH": str(project_root / "benchmark_tasks_and_results"),  # Set forbidden path
            "MCP_QUIET": "1",
            "NODE_ENV": "production"
        }
    )

async def test_check_installed_packages(session: ClientSession):
    print("\n=== Testing check_installed_packages ===")
    try:
        result = await session.call_tool("check_installed_packages", {})
        print("âœ“ check_installed_packages result:")
        response_data = json.loads(result.content[0].text)
        print(f"  Status: {response_data.get('status')}")
        print(f"  Environment Type: {response_data.get('env_type')}")
        print(f"  Package Manager: {response_data.get('package_manager')}")
        print(f"  Total Packages: {response_data.get('total_packages')}")
        
        # Show first 10 packages as sample
        packages = response_data.get('installed_packages', [])
        if packages:
            print(f"  Sample packages (first 10):")
            for i, pkg in enumerate(packages[:10]):
                print(f"    {i+1}. {pkg.get('name')} {pkg.get('version')}")
            if len(packages) > 10:
                print(f"    ... and {len(packages) - 10} more packages")
        else:
            print("  No packages found")
            
    except Exception as e:
        print(f"âŒ check_installed_packages error: {e}")

async def test_install_dependencies(session: ClientSession, packages):
    print(f"\n=== Testing install_dependencies ===")
    print(f"Installing packages: {packages}")
    try:
        result = await session.call_tool("install_dependencies", {"packages": packages})
        print("âœ“ install_dependencies result:")
        response_data = json.loads(result.content[0].text)
        print(f"  Status: {response_data.get('status')}")
        if response_data.get('output'):
            print(f"  Output: {response_data.get('output')}...")
        if response_data.get('warnings'):
            print(f"  Warnings: {response_data.get('warnings')}...")
        if response_data.get('error'):
            print(f"  Error: {response_data.get('error')}")
            
    except Exception as e:
        print(f"âŒ install_dependencies error: {e}")

async def test_check_package_version(session: ClientSession, package_names):
    print(f"\n=== Testing check_package_version ===")
    print(f"Checking packages: {package_names}")
    try:
        result = await session.call_tool("check_package_version", {"packages": package_names})
        print("âœ“ check_package_version result:")
        response_data = json.loads(result.content[0].text)
        print(f"  Status: {response_data.get('status')}")
        print(f"  Environment Type: {response_data.get('env_type')}")
        print(f"  Package Manager: {response_data.get('package_manager')}")
        
        package_details = response_data.get('package_details', [])
        for pkg in package_details:
            print(f"  Package: {pkg.get('package_name')}")
            print(f"    Version: {pkg.get('version')}")
            print(f"    Path: {pkg.get('package_path')}")
            print(f"    Location: {pkg.get('location')}")
            if pkg.get('error'):
                print(f"    Error: {pkg.get('error')}")
            print()
            
    except Exception as e:
        print(f"âŒ check_package_version error: {e}")

async def test_execute_code(session: ClientSession, code, filename=None):
    print(f"\n=== Testing execute_code ===")
    print(f"Code to execute: {code[:100]}...")
    if filename:
        print(f"Filename: {filename}")
    
    try:
        args = {"code": code}
        if filename:
            args["filename"] = filename
            
        result = await session.call_tool("execute_code", args)
        print("âœ“ execute_code result:")
        response_data = json.loads(result.content[0].text)
        print(f"  Status: {response_data.get('status')}")
        print(f"  File Path: {response_data.get('file_path')}")
        if response_data.get('output'):
            print(f"  Output: {response_data.get('output')}")
        if response_data.get('error'):
            print(f"  Error: {response_data.get('error')}")
        
        return result
            
    except Exception as e:
        print(f"âŒ execute_code error: {e}")
        return None

async def test_read_file(session: ClientSession, file_path):
    print(f"\n=== Testing read_file ===")
    print(f"Reading file: {file_path}")
    try:
        result = await session.call_tool("read_file", {"file_path": file_path})
        print("âœ“ read_file result:")
        response_data = json.loads(result.content[0].text)
        print(f"  Status: {response_data.get('status')}")
        if response_data.get('content'):
            content = response_data.get('content')
            print(f"  Content length: {len(content)} chars")
            print(f"  Content preview: {content[:200]}...")
        if response_data.get('error'):
            print(f"  Error: {response_data.get('error')}")
            
    except Exception as e:
        print(f"âŒ read_file error: {e}")

async def test_save_file(session: ClientSession, content, filename):
    print(f"\n=== Testing save_file ===")
    print(f"Saving file: {filename}")
    print(f"Content: {content[:100]}...")
    try:
        result = await session.call_tool("save_file", {"content": content, "filename": filename})
        print("âœ“ save_file result:")
        response_data = json.loads(result.content[0].text)
        print(f"  Status: {response_data.get('status')}")
        print(f"  File Path: {response_data.get('file_path')}")
        if response_data.get('error'):
            print(f"  Error: {response_data.get('error')}")
            
    except Exception as e:
        print(f"âŒ save_file error: {e}")

async def test_execute_shell_command(session: ClientSession, command, working_dir=None):
    print(f"\n=== Testing execute_shell_command ===")
    print(f"Command: {command}")
    if working_dir:
        print(f"Working directory: {working_dir}")
    try:
        args = {"command": command}
        if working_dir:
            args["working_dir"] = working_dir
            
        result = await session.call_tool("execute_shell_command", args)
        print("âœ“ execute_shell_command result:")
        response_data = json.loads(result.content[0].text)
        print(f"  Status: {response_data.get('status')}")
        if response_data.get('stdout'):
            print(f"  Output: {response_data.get('stdout')}")
        if response_data.get('stderr'):
            print(f"  Stderr: {response_data.get('stderr')}")
        if response_data.get('error'):
            print(f"  Error: {response_data.get('error')}")
            
    except Exception as e:
        print(f"âŒ execute_shell_command error: {e}")

async def test_create_and_execute_script(session: ClientSession, script_content, filename):
    print(f"\n=== Testing create_and_execute_script ===")
    print(f"Creating script: {filename}")
    print(f"Script content: {script_content[:100]}...")
    try:
        result = await session.call_tool("create_and_execute_script", {"script_content": script_content, "filename": filename})
        print("âœ“ create_and_execute_script result:")
        response_data = json.loads(result.content[0].text)
        print(f"  Status: {response_data.get('status')}")
        print(f"  Script Path: {response_data.get('script_path')}")
        if response_data.get('stdout'):
            print(f"  Output: {response_data.get('stdout')}")
        if response_data.get('stderr'):
            print(f"  Stderr: {response_data.get('stderr')}")
        if response_data.get('error'):
            print(f"  Error: {response_data.get('error')}")
            
    except Exception as e:
        print(f"âŒ create_and_execute_script error: {e}")

async def main():
    print("ðŸš€ Starting workspace-server functionality test")
    
    server_params = get_workspace_server_params()
    
    try:
        async with stdio_client(server_params) as (read, write):
            async with ClientSession(read, write) as session:
                print("âœ“ Successfully connected to workspace-server")
                
                # Test 1: Check installed packages
                await test_check_installed_packages(session)
                
                # Test 2: Install ase package
                await test_install_dependencies(session, ["scipy"])
                
                # Test 3: Check ase package version
                await test_check_package_version(session, ["scipy"])
                
                # Test 4: Execute hello world code
                hello_world_code = """print("Hello, World!")
print("This is a test from workspace-server!")
import sys
print(f"Python version: {sys.version}")
print(f"Python executable: {sys.executable}")"""
                
                result = await test_execute_code(session, hello_world_code, "hello_world_test.py")
                
                # Test 5: File operations
                print("\n" + "="*60)
                print("ðŸ“ Testing File Operations")
                print("="*60)
                
                # Test read_file - read README.md
                await test_read_file(session, "mcp_servers_and_tools/workspace_server/README.md")
                
                # Test save_file
                test_content = """# Test file created by workspace-server
This is a test file to verify save_file functionality.
Created at: $(date)
Content includes:
- Multiple lines
- Numbers: 1234567890
- Simple text content for testing"""
                await test_save_file(session, test_content, "test_save_file.txt")
                
                # Test 6: Shell operations
                print("\n" + "="*60)
                print("ðŸš Testing Shell Operations")
                print("="*60)
                
                # Test execute_shell_command - find file, the default working directory is the project root
                await test_execute_shell_command(session, "find . -name 'index.ts' -type f")
                
                # Test execute_shell_command - list directory
                await test_execute_shell_command(session, "ls -la src/", "mcp_servers_and_tools/workspace_server")
                
                # Test execute_shell_command with working directory
                await test_execute_shell_command(session, "pwd && ls -la", "mcp_servers_and_tools")
                
                # Test create_and_execute_script
                script_content = '''#!/bin/bash
echo "=== Test Script Execution ==="
echo "Current directory: $(pwd)"
echo "Current user: $(whoami)"
echo "System info: $(uname -a)"
echo "Python version: $(python3 --version)"
echo "Node version: $(node --version)"
echo "=== Script completed ==="'''
                await test_create_and_execute_script(session, script_content, "test_system_info.sh")
                
                print("\nðŸŽ‰ All tests completed!")
                
    except Exception as e:
        print(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nâš ï¸  Test interrupted by user")
    except Exception as e:
        print(f"\nâŒ Unexpected error: {e}")
        sys.exit(1)

```


================================================================================
=== FILE: mcp_servers_and_tools\workspace_server\tsconfig.json ===
================================================================================

```json
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "NodeNext",
    "moduleResolution": "NodeNext",
    "outDir": "./build",
    "rootDir": "./src",
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true
  },
  "include": ["src/**/*"],
  "exclude": ["node_modules", "build"]
}
```


================================================================================
=== FILE: mcp_servers_and_tools\workspace_server\src\index.ts ===
================================================================================

```typescript
#!/usr/bin/env node

import { Server } from "@modelcontextprotocol/sdk/server/index.js";
import { StdioServerTransport } from "@modelcontextprotocol/sdk/server/stdio.js";
import {
    CallToolRequestSchema,
    ListToolsRequestSchema,
} from "@modelcontextprotocol/sdk/types.js";
import { randomBytes } from 'crypto';
import { fileURLToPath } from 'url';
import { dirname, join, resolve, relative, isAbsolute, basename } from 'path';
import { mkdir, writeFile, appendFile, readFile, access, unlink } from 'fs/promises';
import { exec, ExecOptions } from 'child_process';
import { promisify } from 'util';
import { platform } from 'os';

// ES module compatible __filename and __dirname
const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);
const projectRoot = resolve(__dirname, '..', '..');

function resolveStorageDir(dir: string): string {
    if (!dir) return '';
    return isAbsolute(dir) ? dir : resolve(projectRoot, dir);
}

// Define environment config interface for type safety
interface EnvironmentConfig {
    type: 'conda' | 'venv' | 'venv-uv';
    conda_name?: string;
    venv_path?: string;
    uv_venv_path?: string;
}

const CODE_STORAGE_DIR = resolveStorageDir(process.env.CODE_STORAGE_DIR || '');
const SAVED_FILES_DIR = resolveStorageDir(process.env.SAVED_FILES_DIR || '');
// Default environment settings
let ENV_CONFIG: EnvironmentConfig = {
    // Default environment (conda, venv, or venv-uv)
    type: (process.env.ENV_TYPE || 'conda') as 'conda' | 'venv' | 'venv-uv',
    // Name of the conda environment
    conda_name: process.env.CONDA_ENV_NAME,
    // Path to virtualenv
    venv_path: process.env.VENV_PATH,
    // Path to uv virtualenv
    uv_venv_path: process.env.UV_VENV_PATH
};

if (!CODE_STORAGE_DIR) {
    throw new Error('Missing required environment variable: CODE_STORAGE_DIR');
}

if (!SAVED_FILES_DIR) {
    throw new Error('Missing required environment variable: SAVED_FILES_DIR');
}

// Validate environment settings based on the selected type
if (ENV_CONFIG.type === 'conda' && !ENV_CONFIG.conda_name) {
    throw new Error('Missing required environment variable: CONDA_ENV_NAME (required for conda environment)');
} else if (ENV_CONFIG.type === 'venv' && !ENV_CONFIG.venv_path) {
    throw new Error('Missing required environment variable: VENV_PATH (required for virtualenv)');
} else if (ENV_CONFIG.type === 'venv-uv' && !ENV_CONFIG.uv_venv_path) {
    throw new Error('Missing required environment variable: UV_VENV_PATH (required for uv virtualenv)');
}

const execAsync = promisify(exec);

/**
 * Get platform-specific command for environment activation and execution
 */
function getPlatformSpecificCommand(pythonCommand: string): { command: string, options: ExecOptions } {
    const isWindows = platform() === 'win32';
    let command = '';
    let options: ExecOptions = {};
    
    switch (ENV_CONFIG.type) {
        case 'conda':
            if (!ENV_CONFIG.conda_name) {
                throw new Error("conda_name is required for conda environment");
            }
            if (isWindows) {
                command = `conda run -n ${ENV_CONFIG.conda_name} ${pythonCommand}`;
                options = { shell: 'cmd.exe' };
            } else {
                command = `source $(conda info --base)/etc/profile.d/conda.sh && conda activate ${ENV_CONFIG.conda_name} && ${pythonCommand}`;
                options = { shell: '/bin/bash' };
            }
            break;
            
        case 'venv':
            if (!ENV_CONFIG.venv_path) {
                throw new Error("venv_path is required for virtualenv");
            }
            if (isWindows) {
                command = `${join(ENV_CONFIG.venv_path, 'Scripts', 'activate')} && ${pythonCommand}`;
                options = { shell: 'cmd.exe' };
            } else {
                command = `source ${join(ENV_CONFIG.venv_path, 'bin', 'activate')} && ${pythonCommand}`;
                options = { shell: '/bin/bash' };
            }
            break;
            
        case 'venv-uv':
            if (!ENV_CONFIG.uv_venv_path) {
                throw new Error("uv_venv_path is required for uv virtualenv");
            }
            if (isWindows) {
                command = `${join(ENV_CONFIG.uv_venv_path, 'Scripts', 'activate')} && ${pythonCommand}`;
                options = { shell: 'cmd.exe' };
            } else {
                command = `source ${join(ENV_CONFIG.uv_venv_path, 'bin', 'activate')} && ${pythonCommand}`;
                options = { shell: '/bin/bash' };
            }
            break;
            
        default:
            throw new Error(`Unsupported environment type: ${ENV_CONFIG.type}`);
    }
    
    return { command, options };
}

/**
 * Execute Python code and return the result
 */
async function executeCode(code: string, filePath: string) {
    try {
        // Handle filename - resolve relative paths from CODE_STORAGE_DIR
        let absFilePath: string;
        if (isAbsolute(filePath)) {
            // For absolute paths, use as-is
            absFilePath = filePath;
        } else {
            // For relative paths, resolve from CODE_STORAGE_DIR
            absFilePath = join(CODE_STORAGE_DIR, filePath);
        }
        
        // Check if path is forbidden and provide accurate error message
        const errorReason = getPathForbiddenReason(absFilePath);
        if (errorReason) {
            return {
                type: 'text',
                text: JSON.stringify({
                    status: 'error',
                    error: errorReason,
                    file_path: absFilePath
                }),
                isError: true
            };
        }
        
        // Write code to file
        await writeFile(absFilePath, code, 'utf-8');

        // Get platform-specific command with unbuffered output
        const pythonCmd = platform() === 'win32' ? `python -u "${absFilePath}"` : `python3 -u "${absFilePath}"`;
        const { command, options } = getPlatformSpecificCommand(pythonCmd);

        // Execute code
        const { stdout, stderr } = await execAsync(command, {
            cwd: CODE_STORAGE_DIR,
            env: { ...process.env, PYTHONUNBUFFERED: '1' },
            ...options
        });

        // Return all output
        const responseOutput = [stderr, stdout].filter(Boolean).join('\n');
        
        const response = {
            status: stderr ? 'error' : 'success',
            output: responseOutput,
            file_path: absFilePath
        };

        return {
            type: 'text',
            text: JSON.stringify(response),
            isError: !!stderr
        };
    } catch (error) {
        const response = {
            status: 'error',
            error: error instanceof Error ? error.message : String(error),
            file_path: filePath
        };

        return {
            type: 'text',
            text: JSON.stringify(response),
            isError: true
        };
    }
}

/**
 * Read the content of a code file
 */
async function readCodeFile(filePath: string) {
    try {
        // Normalize the path first
        const normalizedPath = isAbsolute(filePath) ? filePath : join(PROJECT_ROOT, filePath);
        
        // Check if path is forbidden and provide accurate error message
        const errorReason = getPathForbiddenReason(normalizedPath);
        if (errorReason) {
            return {
                type: 'text',
                text: JSON.stringify({
                    status: 'error',
                    error: errorReason,
                    file_path: filePath
                }),
                isError: true
            };
        }
        // Ensure file exists
        await access(normalizedPath);
        
        // Read file content
        const content = await readFile(normalizedPath, 'utf-8');
        
        return {
            type: 'text',
            text: JSON.stringify({
                status: 'success',
                content: content,
                file_path: filePath
            }),
            isError: false
        };
    } catch (error) {
        return {
            type: 'text',
            text: JSON.stringify({
                status: 'error',
                error: error instanceof Error ? error.message : String(error),
                file_path: filePath
            }),
            isError: true
        };
    }
}

// Add package manager detection utilities at the top level
async function detectPackageManager(): Promise<'pip' | 'uv' | 'conda'> {
    try {
        // Check if uv is available
        await execAsync('uv --version', { timeout: 2000 });
        return 'uv';
    } catch {
        try {
            // Check if conda is available and we're in a conda env
            if (process.env.CONDA_DEFAULT_ENV) {
                await execAsync('conda --version', { timeout: 2000 });
                return 'conda';
            }
        } catch {
            // Fall back to pip
        }
    }
    return 'pip';
}

async function getPackageManagerCommands(packageManager: 'pip' | 'uv' | 'conda', packages?: string[]) {
    const commands = {
        pip: {
            list: [process.platform === 'win32' ? 'python' : 'python3', '-m', 'pip', 'list', '--format=freeze'],
            show: (pkg: string) => [process.platform === 'win32' ? 'python' : 'python3', '-m', 'pip', 'show', pkg],
            install: (pkgs: string[]) => [process.platform === 'win32' ? 'python' : 'python3', '-m', 'pip', 'install', ...pkgs]
        },
        uv: {
            list: ['uv', 'pip', 'list', '--format=freeze'],
            show: (pkg: string) => ['uv', 'pip', 'show', pkg],
            install: (pkgs: string[]) => ['uv', 'pip', 'install', ...pkgs]
        },
        conda: {
            list: ['conda', 'list', '--export'],
            show: (pkg: string) => ['conda', 'list', pkg],
            install: (pkgs: string[]) => ['conda', 'install', '-y', ...pkgs]
        }
    };
    
    return commands[packageManager];
}

/**
 * Install dependencies using the appropriate package manager
 */
async function installDependencies(packages: string[]) {
    try {
        if (!packages || packages.length === 0) {
            return {
                type: 'text',
                text: JSON.stringify({
                    status: 'error',
                    error: 'No packages specified'
                }),
                isError: true
            };
        }

        // Auto-detect package manager or use environment-specified one
        let packageManager: 'pip' | 'uv' | 'conda';
        
        // First, respect explicit environment configuration
        if (ENV_CONFIG.type === 'conda') {
            packageManager = 'conda';
        } else if (ENV_CONFIG.type === 'venv-uv') {
            packageManager = 'uv';
        } else if (ENV_CONFIG.type === 'venv') {
            // For venv, detect if uv is available and prefer it
            packageManager = await detectPackageManager();
        } else {
            // Auto-detect for other cases
            packageManager = await detectPackageManager();
        }

        const commands = await getPackageManagerCommands(packageManager, packages);
        
        // Create a temporary Python script to get all installed packages
        const tempId = randomBytes(4).toString('hex');
        const installScriptPath = join(CODE_STORAGE_DIR, `install_packages_${tempId}.py`);
        
                 // Build the appropriate command based on detected package manager
         let installCommand = '';
         if (packageManager === 'conda' && ENV_CONFIG.conda_name) {
             const condaInstallCmd = ['conda', 'install', '-y', '-n', ENV_CONFIG.conda_name, ...packages];
             installCommand = JSON.stringify(condaInstallCmd);
         } else {
             const installCmd = commands.install(packages);
             installCommand = JSON.stringify(installCmd);
         }
        
        const installScript = `
import subprocess
import sys
import json

def install_packages():
    """Install packages in the current environment."""
    try:
        result = subprocess.run(${installCommand}, 
                              capture_output=True, text=True, check=True)
        return {
            "status": "success",
            "output": result.stdout,
            "warnings": result.stderr
        }
    except Exception as e:
        return {"status": "error", "error": str(e)}

# Install packages
install_result = install_packages()

# Return the result
print(json.dumps(install_result))
`;

        await writeFile(installScriptPath, installScript, 'utf-8');

        // Execute the install script with unbuffered output
        const pythonCmd = platform() === 'win32' ? `python -u "${installScriptPath}"` : `python3 -u "${installScriptPath}"`;
        const { command: installCommandExec, options: installOptions } = getPlatformSpecificCommand(pythonCmd);

        const { stdout, stderr } = await execAsync(installCommandExec, {
            cwd: CODE_STORAGE_DIR,
            env: { ...process.env, PYTHONUNBUFFERED: '1' },
            ...installOptions
        });

        if (stderr) {
            return {
                type: 'text',
                text: JSON.stringify({
                    status: 'error',
                    env_type: ENV_CONFIG.type,
                    package_manager: packageManager,
                    error: stderr
                }),
                isError: true
            };
        }

        // Parse the response from the Python script
        let parsed;
        try {
            parsed = JSON.parse(stdout.trim());
        } catch (e) {
            return {
                type: 'text',
                text: JSON.stringify({
                    status: 'error',
                    env_type: ENV_CONFIG.type,
                    package_manager: packageManager,
                    error: `Failed to parse output: ${stdout}`
                }),
                isError: true
            };
        }

        // Clean up the temporary script
        try {
            await unlink(installScriptPath);
        } catch (e) {
            // Ignore cleanup errors
        }

        // Check if the installation failed
        const isError = parsed.status === 'error';

        return {
            type: 'text',
            text: JSON.stringify(parsed),
            isError: isError
        };
    } catch (error) {
        const response = {
            status: 'error',
            env_type: ENV_CONFIG.type,
            error: error instanceof Error ? error.message : String(error)
        };

        return {
            type: 'text',
            text: JSON.stringify(response),
            isError: true
        };
    }
}

/**
 * Check if packages are installed in the current environment
 */
async function checkPackageInstallation(packages: string[]) {
    try {
        // Auto-detect package manager or use environment-specified one
        let packageManager: 'pip' | 'uv' | 'conda';
        
        // First, respect explicit environment configuration
        if (ENV_CONFIG.type === 'conda') {
            packageManager = 'conda';
        } else if (ENV_CONFIG.type === 'venv-uv') {
            packageManager = 'uv';
        } else if (ENV_CONFIG.type === 'venv') {
            // For venv, detect if uv is available and prefer it
            packageManager = await detectPackageManager();
        } else {
            // Auto-detect for other cases
            packageManager = await detectPackageManager();
        }

        const commands = await getPackageManagerCommands(packageManager);
        
        // Create a temporary Python script to get all installed packages
        const tempId = randomBytes(4).toString('hex');
        const checkScriptPath = join(CODE_STORAGE_DIR, `list_packages_${tempId}.py`);
        
        // Build the appropriate command based on detected package manager
        let listCommand = '';
        if (packageManager === 'conda' && ENV_CONFIG.conda_name) {
            listCommand = `['conda', 'list', '-n', '${ENV_CONFIG.conda_name}', '--export']`;
        } else {
            listCommand = JSON.stringify(commands.list);
        }
        
        const checkScript = `
import subprocess
import sys
import json

def get_all_installed_packages():
    """Get all installed packages in the current environment."""
    try:
        result = subprocess.run(${listCommand}, 
                              capture_output=True, text=True, check=True)
        packages = []
        for line in result.stdout.strip().split('\\n'):
            if line and ('==' in line or '=' in line):
                # Handle both pip freeze format (==) and conda format (=)
                if '==' in line:
                    name, version = line.split('==', 1)
                else:
                    parts = line.split('=')
                    name, version = parts[0], parts[1] if len(parts) > 1 else 'unknown'
                packages.append({
                    "name": name.strip(),
                    "version": version.strip()
                })
        return packages
    except Exception as e:
        return {"error": str(e)}

# Get all installed packages
all_packages = get_all_installed_packages()

# Return the result
if isinstance(all_packages, list):
    result = {
        "status": "success",
        "total_packages": len(all_packages),
        "packages": all_packages,
        "package_manager": "${packageManager}"
    }
else:
    result = {
        "status": "error",
        "error": all_packages.get("error", "Unknown error"),
        "package_manager": "${packageManager}"
    }

print(json.dumps(result))
`;

        await writeFile(checkScriptPath, checkScript, 'utf-8');

        // Execute the check script with unbuffered output
        const pythonCmd = platform() === 'win32' ? `python -u "${checkScriptPath}"` : `python3 -u "${checkScriptPath}"`;
        const { command, options } = getPlatformSpecificCommand(pythonCmd);

        const { stdout, stderr } = await execAsync(command, {
            cwd: CODE_STORAGE_DIR,
            env: { ...process.env, PYTHONUNBUFFERED: '1' },
            ...options
        });

        if (stderr) {
            return {
                type: 'text',
                text: JSON.stringify({
                    status: 'error',
                    env_type: ENV_CONFIG.type,
                    package_manager: packageManager,
                    error: stderr
                }),
                isError: true
            };
        }

        // Parse the response from the Python script
        let parsed;
        try {
            parsed = JSON.parse(stdout.trim());
        } catch (e) {
            return {
                type: 'text',
                text: JSON.stringify({
                    status: 'error',
                    env_type: ENV_CONFIG.type,
                    package_manager: packageManager,
                    error: `Failed to parse output: ${stdout}`
                }),
                isError: true
            };
        }

        // Build the final response
        const response = {
            status: parsed.status,
            env_type: ENV_CONFIG.type,
            package_manager: parsed.package_manager || packageManager,
            total_packages: parsed.total_packages || 0,
            installed_packages: parsed.packages || [],
            error: parsed.error
        };

        // Clean up the temporary script
        try {
            await unlink(checkScriptPath);
        } catch (e) {
            // Ignore cleanup errors
        }

        return {
            type: 'text',
            text: JSON.stringify(response),
            isError: false
        };
    } catch (error) {
        const response = {
            status: 'error',
            env_type: ENV_CONFIG.type,
            error: error instanceof Error ? error.message : String(error)
        };

        return {
            type: 'text',
            text: JSON.stringify(response),
            isError: true
        };
    }
}

/**
 * Save a file to the SAVED_FILES_DIR with the specified filename
 */
async function saveFile(content: string, filename: string) {
    try {
        
        
        // Handle filename - resolve relative paths from SAVED_FILES_DIR
        let actualFilePath: string;
        if (isAbsolute(filename)) {
            // For absolute paths, use as-is
            actualFilePath = filename;
        } else {
            // For relative paths, resolve from SAVED_FILES_DIR
            actualFilePath = join(SAVED_FILES_DIR, filename);
        }
        
        // Check if path is forbidden and provide accurate error message
        const errorReason = getPathForbiddenReason(actualFilePath);
        if (errorReason) {
            return {
                type: 'text',
                text: JSON.stringify({
                    status: 'error',
                    error: errorReason,
                    filename: filename
                }),
                isError: true
            };
        }
        
        // Ensure filename has .py extension if it's a relative path
        if (!isAbsolute(filename) && !actualFilePath.endsWith('.py')) {
            actualFilePath = `${actualFilePath}.py`;
        }
        
        // Write content to file
        await writeFile(actualFilePath, content, 'utf-8');
        
        return {
            type: 'text',
            text: JSON.stringify({
                status: 'success',
                message: 'File saved successfully',
                file_path: actualFilePath,
                filename: filename
            }),
            isError: false
        };
    } catch (error) {
        return {
            type: 'text',
            text: JSON.stringify({
                status: 'error',
                error: error instanceof Error ? error.message : String(error)
            }),
            isError: true
        };
    }
}

/**
 * Execute a shell command and return the result
 */
async function executeShellCommand(command: string, workingDir?: string) {
    try {
        // Security check for working directory
        let resolvedWorkingDir = workingDir;
        if (workingDir) {
            // Handle both absolute and relative paths
            const normalizedWorkingDir = isAbsolute(workingDir) ? workingDir : join(PROJECT_ROOT, workingDir);
            const errorReason = getPathForbiddenReason(normalizedWorkingDir);
            if (errorReason) {
                return {
                    type: 'text',
                    text: JSON.stringify({
                        status: 'error',
                        error: errorReason
                    }),
                    isError: true
                };
            }
            resolvedWorkingDir = normalizedWorkingDir;
        }
        
        // Enhanced security checks for shell commands using comprehensive forbidden path detection
        
        // Note: Working directory check is already done above (lines 661-676)
        
        // 1. Extract and check potential file paths from the command
        const potentialPaths = extractPathsFromCommand(command);
        for (const path of potentialPaths) {
            const errorReason = getPathForbiddenReason(path);
            if (errorReason) {
                return {
                    type: 'text',
                    text: JSON.stringify({
                        status: 'error',
                        error: errorReason
                    }),
                    isError: true
                };
            }
        }
        
        // 2. Check for dangerous commands that could access forbidden paths
        const forbiddenDirName = basename(FORBIDDEN_PATH);
        const dangerousPatterns = [
            new RegExp(`find\\s+.*${forbiddenDirName}`, 'i'),
            new RegExp(`grep\\s+.*${forbiddenDirName}`, 'i'),
            new RegExp(`cat\\s+.*${forbiddenDirName}`, 'i'),
            new RegExp(`cp\\s+.*${forbiddenDirName}`, 'i'),
            new RegExp(`mv\\s+.*${forbiddenDirName}`, 'i'),
            new RegExp(`rm\\s+.*${forbiddenDirName}`, 'i'),
            new RegExp(`ls\\s+.*${forbiddenDirName}`, 'i'),
            new RegExp(`tar\\s+.*${forbiddenDirName}`, 'i'),
            new RegExp(`zip\\s+.*${forbiddenDirName}`, 'i'),
            new RegExp(`unzip\\s+.*${forbiddenDirName}`, 'i'),
        ];
        
        for (const pattern of dangerousPatterns) {
            if (pattern.test(command)) {
                return {
                    type: 'text',
                    text: JSON.stringify({
                        status: 'error',
                        error: `Command contains explicit reference to forbidden directory: ${command}`
                    }),
                    isError: true
                };
            }
        }
        
        // 3. Check for glob patterns that could access forbidden content
        if (command.includes('*')) {
            // Check if the command uses dangerous relative paths that could access forbidden content
            // Only block "find ." or "find ./" (without specific subdirectory)
            if (command.match(/find\s+\.\s/) || command.match(/find\s+\.$/) || 
                command.match(/find\s+\.\/\s/) || command.match(/find\s+\.\/$/)) {
                // Commands like "find ." or "find ./" could search in forbidden directories
                return {
                    type: 'text',
                    text: JSON.stringify({
                        status: 'error',
                        error: `Glob pattern with relative path could access forbidden content. Please specify a specific directory: ${command}`
                    }),
                    isError: true
                };
            }
            
            // Also check for unrestricted glob patterns without any path restrictions
            // But allow commands that specify a specific directory (like "find research_agent -name '*.py'")
            if (!command.includes('/') && !command.includes('\\') && !command.match(/find\s+\w+/)) {
                return {
                    type: 'text',
                    text: JSON.stringify({
                        status: 'error',
                        error: `Unrestricted glob pattern could access forbidden content. Please specify a directory: ${command}`
                    }),
                    isError: true
                };
            }
        }
        
        // 4. Forbid dangerous commands
        const dangerousCommands = [
            'rm -rf', 'sudo', 'su', 'chmod 777', 'chown root',
            'dd if=', 'mkfs', 'fdisk', 'mount', 'umount',
            'systemctl', 'service', 'init', 'telinit',
            'curl', 'wget', 'nc', 'netcat', 'ssh', 'scp', 'rsync'
        ];
        
        for (const dangerousCmd of dangerousCommands) {
            if (command.includes(dangerousCmd)) {
                return {
                    type: 'text',
                    text: JSON.stringify({
                        status: 'error',
                        error: `Dangerous commands are forbidden: ${dangerousCmd}`
                    }),
                    isError: true
                };
            }
        }
        
        
        // Execute command with output-based security monitoring
        const { stdout, stderr } = await execAsync(command, {
            cwd: resolvedWorkingDir || PROJECT_ROOT,
            env: { ...process.env },
            timeout: 300000 // 5 minutes timeout
        });
        
        // Check if output contains forbidden directory content
        const forbiddenPath = FORBIDDEN_PATH;
        
        // More comprehensive check for forbidden directory content
        const forbiddenPatterns = [
            `"${forbiddenDirName}"`,           // "benchmark"
            `/${forbiddenDirName}/`,           // /benchmark/
            `${forbiddenDirName}/`,            // benchmark/
            forbiddenPath,                     // Full absolute path
            relative(PROJECT_ROOT, forbiddenPath), // Relative path from project root
        ];
        
        const outputToCheck = stdout + stderr;
        for (const pattern of forbiddenPatterns) {
            if (pattern && outputToCheck.includes(pattern)) {
                return {
                    type: 'text',
                    text: JSON.stringify({
                        status: 'error',
                        error: `Command output contains content from forbidden directory: ${pattern}`
                    }),
                };
            }
        }

        const response = {
            status: stderr ? 'warning' : 'success',
            stdout: stdout,
            stderr: stderr
        };

        return {
            type: 'text',
            text: JSON.stringify(response),
            isError: false
        };
    } catch (error) {
        const response = {
            status: 'error',
            error: 'Command execution failed'
        };

        return {
            type: 'text',
            text: JSON.stringify(response),
            isError: true
        };
    }
}

/**
 * Create and execute a shell script
 */
async function createAndExecuteScript(scriptContent: string, filename?: string, interpreter?: string) {
    try {
        // Generate filename if not provided
        if (!filename) {
            filename = `script_${randomBytes(4).toString('hex')}.sh`;
        }
        
        // Handle filename - resolve relative paths from CODE_STORAGE_DIR
        let filePath: string;
        if (isAbsolute(filename)) {
            // For absolute paths, use as-is
            filePath = filename;
        } else {
            // For relative paths, resolve from CODE_STORAGE_DIR
            filePath = join(CODE_STORAGE_DIR, filename);
        }
        
        // Check if path is forbidden and provide accurate error message
        const errorReason = getPathForbiddenReason(filePath);
        if (errorReason) {
            return {
                type: 'text',
                text: JSON.stringify({
                    status: 'error',
                    error: errorReason,
                    filename: filename
                }),
                isError: true
            };
        }

        // Add shebang if not present and interpreter is specified
        let finalContent = scriptContent;
        if (interpreter && !scriptContent.startsWith('#!')) {
            finalContent = `#!/usr/bin/env ${interpreter}\n\n${scriptContent}`;
        } else if (!scriptContent.startsWith('#!')) {
            // Default to bash if no shebang
            finalContent = `#!/usr/bin/env bash\n\n${scriptContent}`;
        }
        
        // Write script to file
        await writeFile(filePath, finalContent, 'utf-8');
        
        // Make script executable (Unix-like systems)
        if (platform() !== 'win32') {
            await execAsync(`chmod +x "${filePath}"`);
        }

        // Execute the script
        const { stdout, stderr } = await execAsync(`"${filePath}"`, {
            cwd: CODE_STORAGE_DIR,
            env: { ...process.env },
            timeout: 300000 // 5 minutes timeout
        });

        const response = {
            status: stderr ? 'warning' : 'success',
            stdout: stdout,
            stderr: stderr,
            script_path: filePath,
            script_content: finalContent
        };

        return {
            type: 'text',
            text: JSON.stringify(response),
            isError: false
        };
    } catch (error) {
        const response = {
            status: 'error',
            error: error instanceof Error ? error.message : String(error),
            script_path: filename ? join(CODE_STORAGE_DIR, filename) : 'unknown',
            script_content: scriptContent
        };

        return {
            type: 'text',
            text: JSON.stringify(response),
            isError: true
        };
    }
}

/**
 * Create an MCP server to handle code execution and dependency management
 */
const server = new Server(
    {
        name: "workspace-server",
        version: "0.1.0",
    },
    {
        capabilities: {
            tools: {},
        },
    }
);

/**
 * Handler for listing available tools.
 */
server.setRequestHandler(ListToolsRequestSchema, async () => {
    return {
        tools: [
            {
                name: "execute_code",
                description: `Execute Python code in the ${ENV_CONFIG.type} environment. Code is saved to ${CODE_STORAGE_DIR} and executed. Use this tool to run Python scripts and get their output.`,
                inputSchema: {
                    type: "object",
                    properties: {
                        code: {
                            type: "string",
                            description: `Python code to execute`
                        },
                        filename: {
                            type: "string",
                            description: `Optional: Name of the file to save the code (default: generated UUID)`
                        }
                    },
                    required: ["code"]
                }
            },
            {
                name: "read_file",
                description: `Read the content of any text file (Python code, log files, output files, etc.). Use this to examine local package code files, output files from external programs, log files, or any text-based files to extract specific information. PATH HANDLING: Prefer absolute paths. If using relative paths, they are resolved from ${PROJECT_ROOT}`,
                inputSchema: {
                    type: "object",
                    properties: {
                        file_path: {
                            type: "string",
                            description: `Path to the file to read (supports any text file format: .py, .log, .txt, .out, .xyz, etc.). Using absolute paths (preferred) or relative paths from ${PROJECT_ROOT}`
                        }
                    },
                    required: ["file_path"]
                }
            },
            {
                name: "install_dependencies",
                description: `Install missing Python dependencies in the ${ENV_CONFIG.type} environment. Use this tool to install packages that are required for your code to run. Example: {"packages": ["package1", "package2", "package3"]}`,
                inputSchema: {
                    type: "object",
                    properties: {
                        packages: {
                            type: "array",
                            items: {
                                type: "string"
                            },
                            description: `Array of package names to install. Each package should be a string. Example: ["package1", "package2", "package3"]`
                        }
                    },
                    required: ["packages"]
                }
            },
            {
                name: "check_installed_packages",
                description: `List all installed packages in the current Python environment. Use this tool to check what packages are already available before attempting to install new ones.`,
                inputSchema: {
                    type: "object",
                    properties: {}
                }
            },
            {
                name: "check_package_version",
                description: `Check if specific packages are installed and get their version, package path, and module location information. Use this tool to verify specific package installations. Example: {"packages": ["package1", "package2", "package3"]}`,
                inputSchema: {
                    type: "object",
                    properties: {
                        packages: {
                            type: "array",
                            items: {
                                type: "string"
                            },
                            description: `Array of package names to check. Each package should be a string. Example: ["package1", "package2", "package3"]`
                        }
                    },
                    required: ["packages"]
                }
            },
            {
                name: "save_file",
                description: `Save a file to ${SAVED_FILES_DIR} with the specified filename`,
                inputSchema: {
                    type: "object",
                    properties: {
                        content: {
                            type: "string",
                            description: `Content of the file`
                        },
                        filename: {
                            type: "string",
                            description: `Filename of the file`
                        }
                    },
                    required: ["content", "filename"]
                }
            },
            {
                name: "execute_shell_command",
                description: `Execute a shell command and return the result. Default working directory is ${PROJECT_ROOT}. PATH HANDLING: Prefer absolute paths for working_dir. If using relative paths for working_dir, they are resolved from ${PROJECT_ROOT}`,
                inputSchema: {
                    type: "object",
                    properties: {
                        command: {
                            type: "string",
                            description: `Shell command to execute`
                        },
                        working_dir: {
                            type: "string",
                            description: `Working directory for the command (absolute path or relative path from ${PROJECT_ROOT}). Default is ${PROJECT_ROOT}`
                        }
                    },
                    required: ["command"]
                }
            },
            {
                name: "create_and_execute_script",
                description: `Create and execute a shell script. Scripts are created in ${CODE_STORAGE_DIR}`,
                inputSchema: {
                    type: "object",
                    properties: {
                        script_content: {
                            type: "string",
                            description: `Content of the script`
                        },
                        filename: {
                            type: "string",
                            description: `Optional: Name of the script file`
                        },
                        interpreter: {
                            type: "string",
                            description: `Optional: Interpreter for the script`
                        }
                    },
                    required: ["script_content"]
                }
            }
        ]
    };
});

interface ExecuteCodeArgs {
    code?: string;
    filename?: string;
}

interface ReadCodeFileArgs {
    file_path?: string;
}

interface InstallDependenciesArgs {
    packages?: string[];
}

interface CheckInstalledPackagesArgs {
    // No parameters needed - returns all installed packages
}

interface SaveFileArgs {
    content?: string;
    filename?: string;
}

interface ExecuteShellCommandArgs {
    command?: string;
    working_dir?: string;
}

interface CreateAndExecuteScriptArgs {
    script_content?: string;
    filename?: string;
    interpreter?: string;
}

interface CheckPackageVersionArgs {
    packages?: string[];
}

/**
 * Handler for tool execution.
 */
server.setRequestHandler(CallToolRequestSchema, async (request) => {
    switch (request.params.name) {
        case "execute_code": {
            const args = request.params.arguments as ExecuteCodeArgs;
            if (!args?.code) {
                return {
                    content: [{
                        type: "text",
                        text: JSON.stringify({
                            "status": "error",
                            "error": "Code is required",
                            "suggestion": "Please provide Python code to execute"
                        }),
                        isError: true
                    }]
                };
            }

            try {
                
                const filename = args.filename ? `${args.filename.replace(/\.py$/, '')}_${randomBytes(4).toString('hex')}.py` : `code_${randomBytes(4).toString('hex')}.py`;
                const result = await executeCode(args.code, filename);

                return {
                    content: [{
                        type: "text",
                        text: result.text,
                        isError: result.isError
                    }]
                };
            } catch (error) {
                return {
                    content: [{
                        type: "text",
                        text: JSON.stringify({
                            "status": "error",
                            "error": error instanceof Error ? error.message : String(error),
                            "code_length": args.code.length,
                            "suggestion": "Check if the Python code is valid and try again"
                        }),
                        isError: true
                    }]
                };
            }
        }
        
        case "read_file": {
            const args = request.params.arguments as ReadCodeFileArgs;
            if (!args?.file_path) {
                return {
                    content: [{
                        type: "text",
                        text: JSON.stringify({
                            "status": "error",
                            "error": "File path is required",
                            "suggestion": "Please provide a valid file path to read"
                        }),
                        isError: true
                    }]
                };
            }

            try {
                const result = await readCodeFile(args.file_path);
                return {
                    content: [{
                        type: "text",
                        text: result.text,
                        isError: result.isError
                    }]
                };
            } catch (error) {
                return {
                    content: [{
                        type: "text",
                        text: JSON.stringify({
                            "status": "error",
                            "error": error instanceof Error ? error.message : String(error),
                            "file_path": args.file_path,
                            "suggestion": "Check if the file path is correct and accessible"
                        }),
                        isError: true
                    }]
                };
            }
        }
        
        case "install_dependencies": {
            const args = request.params.arguments as InstallDependenciesArgs;
            let packages = args?.packages;
            
            // Handle case where packages is passed as a JSON string instead of array
            if (typeof packages === 'string') {
                const packagesStr = packages; // Store the original string value
                try {
                    packages = JSON.parse(packagesStr);
                    // If JSON parse succeeds but result is still a string, wrap it in array
                    if (typeof packages === 'string') {
                        packages = [packages];
                    }
                } catch (e) {
                    // Not valid JSON, treat as a single package name
                    packages = [packagesStr];
                }
            }
            
            if (!packages || !Array.isArray(packages)) {
                return {
                    content: [{
                        type: "text",
                        text: JSON.stringify({
                            "status": "error",
                            "error": "Valid packages array is required",
                            "suggestion": "Please provide packages as an array, e.g., {\"packages\": [\"package1\", \"package2\"]}"
                        }),
                        isError: true
                    }]
                };
            }
            
            // Filter out empty strings and normalize package names
            packages = packages
                .filter(pkg => pkg && typeof pkg === 'string' && pkg.trim().length > 0)
                .map(pkg => pkg.trim());
            
            if (packages.length === 0) {
                return {
                    content: [{
                        type: "text",
                        text: JSON.stringify({
                            "status": "error",
                            "error": "No valid package names provided",
                            "suggestion": "Please provide at least one valid package name"
                        }),
                        isError: true
                    }]
                };
            }

            try {
                const result = await installDependencies(packages);
                return {
                    content: [{
                        type: "text",
                        text: result.text,
                        isError: result.isError
                    }]
                };
            } catch (error) {
                return {
                    content: [{
                        type: "text",
                        text: JSON.stringify({
                            "status": "error",
                            "error": error instanceof Error ? error.message : String(error),
                            "packages_requested": packages,
                            "suggestion": "Check if the package names are correct and try again"
                        }),
                        isError: true
                    }]
                };
            }
        }
        
        case "check_installed_packages": {
            const result = await checkPackageInstallation([]);

            return {
                content: [{
                    type: "text",
                    text: result.text,
                    isError: result.isError
                }]
            };
        }
        
        case "check_package_version": {
            const args = request.params.arguments as CheckPackageVersionArgs;
            let packages = args?.packages;
            
            // Handle case where packages is passed as a JSON string instead of array
            if (typeof packages === 'string') {
                const packagesStr = packages; // Store the original string value
                try {
                    packages = JSON.parse(packagesStr);
                    // If JSON parse succeeds but result is still a string, wrap it in array
                    if (typeof packages === 'string') {
                        packages = [packages];
                    }
                } catch (e) {
                    // Not valid JSON, treat as a single package name
                    packages = [packagesStr];
                }
            }
            
            if (!packages || !Array.isArray(packages)) {
                return {
                    content: [{
                        type: "text",
                        text: JSON.stringify({
                            "status": "error",
                            "error": "Valid packages array is required",
                            "suggestion": "Please provide packages as an array, e.g., {\"packages\": [\"package1\", \"package2\"]}"
                        }),
                        isError: true
                    }]
                };
            }
            
            // Filter out empty strings and normalize package names
            packages = packages
                .filter(pkg => pkg && typeof pkg === 'string' && pkg.trim().length > 0)
                .map(pkg => pkg.trim());
            
            if (packages.length === 0) {
                return {
                    content: [{
                        type: "text",
                        text: JSON.stringify({
                            "status": "error",
                            "error": "No valid package names provided",
                            "suggestion": "Please provide at least one valid package name"
                        }),
                        isError: true
                    }]
                };
            }

            try {
                // Auto-detect package manager or use environment-specified one
                let packageManager: 'pip' | 'uv' | 'conda';
                
                // First, respect explicit environment configuration
                if (ENV_CONFIG.type === 'conda') {
                    packageManager = 'conda';
                } else if (ENV_CONFIG.type === 'venv-uv') {
                    packageManager = 'uv';
                } else if (ENV_CONFIG.type === 'venv') {
                    // For venv, detect if uv is available and prefer it
                    packageManager = await detectPackageManager();
                } else {
                    // Auto-detect for other cases
                    packageManager = await detectPackageManager();
                }

                const commands = await getPackageManagerCommands(packageManager);
                
                const results = [];
                for (const package_name of packages) {
                    let version = "unknown";
                    let package_path = "unknown";
                    let location = "unknown";
                    let error = null;

                    // Step 1: Try to get version using appropriate package manager show command
                    try {
                        let showCommand = [];
                        if (packageManager === 'conda' && ENV_CONFIG.conda_name) {
                            showCommand = ['conda', 'list', '-n', ENV_CONFIG.conda_name, package_name];
                        } else {
                            showCommand = commands.show(package_name);
                        }
                        
                        const showCmdStr = showCommand.join(' ');
                        const { command: showCmd, options: showOptions } = getPlatformSpecificCommand(showCmdStr);
                        const { stdout: showOut, stderr: showErr } = await execAsync(showCmd, {
                            cwd: CODE_STORAGE_DIR,
                            env: { ...process.env, PYTHONUNBUFFERED: '1' },
                            ...showOptions
                        });
                        
                        const lines = showOut.split("\n");
                        for (const line of lines) {
                            if (line.startsWith("Version:")) {
                                version = line.split(":")[1].trim();
                            }
                            if (line.startsWith("Location:")) {
                                const location = line.split(":")[1].trim();
                                // Try different possible package paths
                                const possiblePaths = [
                                    `${location}/${package_name}`,
                                    `${location}/${package_name.replace(/-/g, '_')}`,
                                    `${location}/${package_name.replace(/-/g, '/')}`,
                                ];
                                
                                // For packages ending with -py, try removing the suffix
                                if (package_name.endsWith('-py')) {
                                    possiblePaths.push(`${location}/${package_name.slice(0, -3)}`);
                                }
                                
                                // For packages with multiple hyphens, try first part only
                                const hyphenCount = (package_name.match(/-/g) || []).length;
                                if (package_name.includes('-') && hyphenCount > 1) {
                                    const firstPart = package_name.split('-')[0];
                                    possiblePaths.push(`${location}/${firstPart}`);
                                }
                                
                                // Find the first existing path
                                let found_package_path = "unknown";
                                for (const path of possiblePaths) {
                                    try {
                                        const fs = require('fs');
                                        if (fs.existsSync(path)) {
                                            found_package_path = path;
                                            break;
                                        }
                                    } catch (e) {
                                        // Continue to next path
                                    }
                                }
                                
                                // Set package_path to the found path (or fallback to original logic)
                                if (found_package_path !== "unknown") {
                                    package_path = found_package_path;
                                } else {
                                    package_path = `${location}/${package_name}`;
                                }
                            }
                        }
                    } catch (e) {
                        error = `${packageManager} show failed: ${e}`;
                    }

                                        // Step 2: Try multiple import strategies to find the correct module
                    try {
                        // Generate all possible variations of the package name
                        const variations = new Set<string>();
                        
                        // Original name
                        variations.add(package_name);
                        
                        // Replace hyphens with underscores
                        if (package_name.includes('-')) {
                            variations.add(package_name.replace(/-/g, '_'));
                        }
                        
                        // Replace hyphens with dots
                        if (package_name.includes('-')) {
                            variations.add(package_name.replace(/-/g, '.'));
                        }
                        
                        // Replace underscores with hyphens
                        if (package_name.includes('_')) {
                            variations.add(package_name.replace(/_/g, '-'));
                        }
                        
                        // Replace dots with hyphens
                        if (package_name.includes('.')) {
                            variations.add(package_name.replace(/\./g, '-'));
                        }
                        
                        // Replace dots with underscores
                        if (package_name.includes('.')) {
                            variations.add(package_name.replace(/\./g, '_'));
                        }
                        
                        // Try each variation until one succeeds
                        let success = false;
                        for (const variation of variations) {
                            try {
                                const pyCmd = `python -c "import ${variation}; print(${variation}.__file__); print(getattr(${variation}, '__version__', 'no __version__'))"`;
                                const { command: pyCommand, options: pyOptions } = getPlatformSpecificCommand(pyCmd);
                                const { stdout: pyOut, stderr: pyErr } = await execAsync(pyCommand, {
                                    cwd: CODE_STORAGE_DIR,
                                    env: { ...process.env, PYTHONUNBUFFERED: '1' },
                                    ...pyOptions
                                });
                                
                                if (pyOut.trim()) {
                                    const lines = pyOut.trim().split("\n");
                                    // Check if we got a valid file path (not None)
                                    if (lines.length >= 1 && lines[0] !== 'None') {
                                        location = lines[0];
                                        if (package_path === "unknown" && location) {
                                            package_path = location.substring(0, location.lastIndexOf("/"));
                                        }
                                    } else if (lines.length >= 1 && lines[0] === 'None') {
                                        // Handle namespace packages where __file__ is None
                                        // Try to get the path from __path__
                                        try {
                                            const pathCmd = `python -c "import ${variation}; print(str(${variation}.__path__))"`;
                                            const { command: pathCommand, options: pathOptions } = getPlatformSpecificCommand(pathCmd);
                                            const { stdout: pathOut, stderr: pathErr } = await execAsync(pathCommand, {
                                                cwd: CODE_STORAGE_DIR,
                                                env: { ...process.env, PYTHONUNBUFFERED: '1' },
                                                ...pathOptions
                                            });
                                            
                                            if (pathOut.trim() && pathOut.trim() !== 'None') {
                                                // Extract path from _NamespacePath format
                                                const pathMatch = pathOut.trim().match(/_NamespacePath\(\[['"]([^'"]+)['"]\]\)/);
                                                if (pathMatch) {
                                                    location = pathMatch[1];
                                                    if (package_path === "unknown" && location) {
                                                        package_path = location.substring(0, location.lastIndexOf("/"));
                                                    }
                                                }
                                            }
                                        } catch (e) {
                                            // Ignore path extraction errors
                                        }
                                    }
                                    if (lines.length >= 2 && lines[1] !== "no __version__") {
                                        version = lines[1];
                                    }
                                    success = true;
                                    break; // Found a working variation
                                }
                            } catch (e) {
                                // Continue to next variation
                                continue;
                            }
                        }
                        
                        // Final fallback: try importlib with original name
                        if (!success) {
                            const pyCmd = `python -c "import importlib; pkg = importlib.import_module('${package_name}'); print(pkg.__file__); print(getattr(pkg, '__version__', 'no __version__'))"`;
                            const { command: pyCommand, options: pyOptions } = getPlatformSpecificCommand(pyCmd);
                            const { stdout: pyOut, stderr: pyErr } = await execAsync(pyCommand, {
                                cwd: CODE_STORAGE_DIR,
                                env: { ...process.env, PYTHONUNBUFFERED: '1' },
                                ...pyOptions
                            });
                            
                            if (pyOut.trim()) {
                                const lines = pyOut.trim().split("\n");
                                // Check if we got a valid file path (not None)
                                if (lines.length >= 1 && lines[0] !== 'None') {
                                    location = lines[0];
                                    if (package_path === "unknown" && location) {
                                        package_path = location.substring(0, location.lastIndexOf("/"));
                                    }
                                } else if (lines.length >= 1 && lines[0] === 'None') {
                                    // Handle namespace packages where __file__ is None
                                    // Try to get the path from __path__
                                    try {
                                        const pathCmd = `python -c "import importlib; pkg = importlib.import_module('${package_name}'); print(str(pkg.__path__))"`;
                                        const { command: pathCommand, options: pathOptions } = getPlatformSpecificCommand(pathCmd);
                                        const { stdout: pathOut, stderr: pathErr } = await execAsync(pathCommand, {
                                            cwd: CODE_STORAGE_DIR,
                                            env: { ...process.env, PYTHONUNBUFFERED: '1' },
                                            ...pathOptions
                                        });
                                        
                                        if (pathOut.trim() && pathOut.trim() !== 'None') {
                                            // Extract path from _NamespacePath format
                                            const pathMatch = pathOut.trim().match(/_NamespacePath\(\[['"]([^'"]+)['"]\]\)/);
                                            if (pathMatch) {
                                                location = pathMatch[1];
                                                if (package_path === "unknown" && location) {
                                                    package_path = location.substring(0, location.lastIndexOf("/"));
                                                }
                                            }
                                        }
                                    } catch (e) {
                                        // Ignore path extraction errors
                                    }
                                }
                                if (lines.length >= 2 && lines[1] !== "no __version__") {
                                    version = lines[1];
                                }
                                success = true;
                            }
                        }
                        
                    } catch (e) {
                        if (!error) error = `python import failed: ${e}`;
                    }

                    // Final fix: Use location from import if it's more accurate than package_path
                    if (location && location.endsWith('__init__.py')) {
                        const dir_from_location = location.substring(0, location.lastIndexOf('/__init__.py'));
                        if (dir_from_location) {
                            package_path = dir_from_location;
                        }
                    }
                    
                    results.push({
                        package_name,
                        version,
                        package_path,
                        location,
                        error,
                        summary: `Package ${package_name} version ${version} at ${package_path}`
                    });
                }

                return {
                    content: [{
                        type: "text",
                        text: JSON.stringify({
                            status: 'success',
                            env_type: ENV_CONFIG.type,
                            package_manager: packageManager,
                            venv_path: ENV_CONFIG.venv_path,
                            package_details: results
                        }),
                        isError: false
                    }]
                };
            } catch (error) {
                return {
                    content: [{
                        type: "text",
                        text: JSON.stringify({
                            status: 'error',
                            error: error instanceof Error ? error.message : String(error),
                            env_type: ENV_CONFIG.type,
                            venv_path: ENV_CONFIG.venv_path
                        }),
                        isError: true
                    }]
                };
            }
        }
        
        case "save_file": {
            const args = request.params.arguments as SaveFileArgs;
            if (!args?.content) {
                return {
                    content: [{
                        type: "text",
                        text: JSON.stringify({
                            "status": "error",
                            "error": "Content is required",
                            "suggestion": "Please provide file content to save"
                        }),
                        isError: true
                    }]
                };
            }
            if (!args?.filename) {
                return {
                    content: [{
                        type: "text",
                        text: JSON.stringify({
                            "status": "error",
                            "error": "Filename is required",
                            "suggestion": "Please provide a filename for the file"
                        }),
                        isError: true
                    }]
                };
            }

            try {
                const result = await saveFile(args.content, args.filename);
                return {
                    content: [result]
                };
            } catch (error) {
                return {
                    content: [{
                        type: "text",
                        text: JSON.stringify({
                            "status": "error",
                            "error": error instanceof Error ? error.message : String(error),
                            "filename": args.filename,
                            "content_length": args.content.length,
                            "suggestion": "Check if the filename is valid and the content is accessible"
                        }),
                        isError: true
                    }]
                };
            }
        }
        
        case "execute_shell_command": {
            const args = request.params.arguments as ExecuteShellCommandArgs;
            if (!args?.command) {
                return {
                    content: [{
                        type: "text",
                        text: JSON.stringify({
                            "status": "error",
                            "error": "Command is required",
                            "suggestion": "Please provide a shell command to execute"
                        }),
                        isError: true
                    }]
                };
            }

            try {
                const result = await executeShellCommand(args.command, args.working_dir);
                return {
                    content: [{
                        type: "text",
                        text: result.text,
                        isError: result.isError
                    }]
                };
            } catch (error) {
                return {
                    content: [{
                        type: "text",
                        text: JSON.stringify({
                            "status": "error",
                            "error": error instanceof Error ? error.message : String(error),
                            "command": args.command,
                            "working_dir": args.working_dir || CODE_STORAGE_DIR,
                            "suggestion": "Check if the command is valid and try again"
                        }),
                        isError: true
                    }]
                };
            }
        }
        
        case "create_and_execute_script": {
            const args = request.params.arguments as CreateAndExecuteScriptArgs;
            if (!args?.script_content) {
                return {
                    content: [{
                        type: "text",
                        text: JSON.stringify({
                            "status": "error",
                            "error": "Script content is required",
                            "suggestion": "Please provide script content to create and execute"
                        }),
                        isError: true
                    }]
                };
            }

            try {
                const result = await createAndExecuteScript(args.script_content, args.filename, args.interpreter);
                return {
                    content: [{
                        type: "text",
                        text: result.text,
                        isError: result.isError
                    }]
                };
            } catch (error) {
                return {
                    content: [{
                        type: "text",
                        text: JSON.stringify({
                            "status": "error",
                            "error": error instanceof Error ? error.message : String(error),
                            "script_length": args.script_content.length,
                            "filename": args.filename || "auto-generated",
                            "interpreter": args.interpreter || "auto-detected",
                            "suggestion": "Check if the script content is valid and try again"
                        }),
                        isError: true
                    }]
                };
            }
        }
        
        default:
            return {
                content: [{
                    type: "text",
                    text: JSON.stringify({
                        "status": "error",
                        "error": `Unknown tool: ${request.params.name}`,
                        "available_tools": [
                            "execute_code",
                            "read_file", 
                            "install_dependencies",
                            "check_installed_packages",
                            "check_package_version",
                            "save_file",
                            "execute_shell_command",
                            "create_and_execute_script"
                        ],
                        "suggestion": "Please use one of the available tools listed above"
                    }),
                    isError: true
                }]
            };
    }
});

/**
 * Start the server using stdio transport.
 */
async function main() {
    // Ensure storage directories exist
    try {
        await mkdir(CODE_STORAGE_DIR, { recursive: true });
        await mkdir(SAVED_FILES_DIR, { recursive: true });
    } catch (error) {
        console.error('Error creating directories:', error);
    }
    
    const transport = new StdioServerTransport();
    await server.connect(transport);
}

// Define the project boundaries and forbidden paths - read from environment variables only
if (!process.env.PROJECT_ROOT) {
    throw new Error('Missing required environment variable: PROJECT_ROOT');
}
if (!process.env.FORBIDDEN_PATH) {
    throw new Error('Missing required environment variable: FORBIDDEN_PATH');
}

const PROJECT_ROOT: string = resolve(process.env.PROJECT_ROOT);
const FORBIDDEN_PATH: string = resolve(process.env.FORBIDDEN_PATH);

// Utility to extract potential file paths from shell commands
function extractPathsFromCommand(command: string): string[] {
    const paths: string[] = [];
    
    // Simple regex to handle quoted strings and basic word splitting
    const tokenRegex = /"[^"]*"|'[^']*'|\S+/g;
    const words = command.match(tokenRegex) || [];
    
    for (let word of words) {
        // Remove quotes if present
        if ((word.startsWith('"') && word.endsWith('"')) || 
            (word.startsWith("'") && word.endsWith("'"))) {
            word = word.slice(1, -1);
        }
        
        // Skip command names and flags
        if (word.startsWith('-') || word.startsWith('--') || 
            ['ls', 'cat', 'find', 'grep', 'head', 'tail', 'wc', 'sort', 'uniq', 'xargs'].includes(word)) {
            continue;
        }
        
        // Check if word looks like a path (contains / or starts with . or ~ or $)
        // But skip shell redirection syntax
        if ((word.includes('/') || word.startsWith('.') || word.startsWith('~') || word.startsWith('$')) &&
            !/^(\d+>|>|>>|&>)/.test(word)) {  // Skip redirection syntax like "2>", ">", ">>", "&>"
            paths.push(word);
        }
    }
    
    // Also check for forbidden directory name specifically
    const forbiddenDirName = basename(FORBIDDEN_PATH);
    if (command.includes(forbiddenDirName)) {
        paths.push(forbiddenDirName);
    }
    
    // Note: Glob pattern checking is handled separately in the main security checks
    
    return paths;
}

// Utility to check if a path is forbidden (inside forbidden directory or outside project scope)
function isPathForbidden(targetPath: string): boolean {
    const absTarget = resolve(targetPath);
    
    // Check if path is inside the forbidden directory
    const forbiddenRel = relative(FORBIDDEN_PATH, absTarget);
    if (forbiddenRel === '' || !forbiddenRel.startsWith('..')) {
        return true; // Inside forbidden directory
    }
    
    // Check if path is outside the project scope
    const projectRel = relative(PROJECT_ROOT, absTarget);
    if (projectRel.startsWith('..')) {
        return true; // Outside project scope
    }
    
    return false; // Path is allowed
}

function getPathForbiddenReason(targetPath: string): string | null {
    // Expand environment variables and home directory before path resolution
    let expandedPath = targetPath;
    if (targetPath.startsWith('$')) {
        // Handle $VAR and ${VAR} syntax
        const envVar = targetPath.startsWith('${') ? 
            targetPath.slice(2, -1) : 
            targetPath.slice(1);
        const envValue = process.env[envVar];
        if (envValue) {
            expandedPath = envValue;
        }
    } else if (targetPath.startsWith('~')) {
        // Handle ~ and ~user syntax
        expandedPath = targetPath.replace(/^~/, process.env.HOME || '');
    }
    
    const absTarget = resolve(expandedPath);
    const absForbiddenPath = resolve(FORBIDDEN_PATH);
    
    // Check if path is inside the forbidden directory FIRST
    try {
        const forbiddenRel = relative(absForbiddenPath, absTarget);
        if (forbiddenRel === '' || !forbiddenRel.startsWith('..')) {
            return 'Cannot access forbidden directory. Please check your path.';
        }
    } catch (e) {
        // If relative() fails, the paths are on different drives (Windows) or other issues
        // Continue to other checks
    }
    
    // Check if path is outside the project scope
    try {
        const projectRel = relative(PROJECT_ROOT, absTarget);
        if (projectRel.startsWith('..')) {
            return 'Cannot access files outside project root. Please check your path.';
        }
    } catch (e) {
        // If relative() fails, the paths are on different drives (Windows) or other issues
        return 'Cannot access files outside project root. Please check your path.';
    }
    
    return null; // Path is allowed
}

main().catch((error) => {
    console.error("Server error:", error);
    process.exit(1);
});
```


================================================================================
=== FILE: utils\check_neo4j.py ===
================================================================================

```python
#!/usr/bin/env python3
"""
Check if Neo4j database is completely cleaned

This script connects to Neo4j and outputs the count of all major node types.
"""

import os
import sys
import asyncio
from pathlib import Path
from neo4j import GraphDatabase

def get_project_root():
    """Get the project root directory by looking for .git directory"""
    current_path = Path(__file__).resolve()
    # Navigate up to find project root (directory containing .git)
    while not (current_path / ".git").exists() and current_path.parent != current_path:
        current_path = current_path.parent
    if (current_path / ".git").exists():
        return current_path
    else:
        raise FileNotFoundError("Could not find project root (no .git directory found)")

def read_neo4j_env_vars():
    """Read Neo4j environment variables from environment or .env file"""
    # First try to get from environment variables
    neo4j_uri = os.environ.get('NEO4J_URI')
    neo4j_user = os.environ.get('NEO4J_USER')
    neo4j_password = os.environ.get('NEO4J_PASSWORD')
    
    # If not found in environment, try .env file
    if not all([neo4j_uri, neo4j_user, neo4j_password]):
        project_root = get_project_root()
        env_path = project_root / "mcp_servers_and_tools/research_server" / ".env"
        
        if env_path.exists():
            with open(env_path, "r") as f:
                for line in f:
                    line = line.strip()
                    if line.startswith("NEO4J_URI=") and not neo4j_uri:
                        neo4j_uri = line.split("=", 1)[1].strip()
                    elif line.startswith("NEO4J_USER=") and not neo4j_user:
                        neo4j_user = line.split("=", 1)[1].strip()
                    elif line.startswith("NEO4J_PASSWORD=") and not neo4j_password:
                        neo4j_password = line.split("=", 1)[1].strip()
    
    return neo4j_uri, neo4j_user, neo4j_password

async def check_neo4j_clean():
    """Check Neo4j database cleaning status"""
    
    # Get Neo4j connection details from environment or .env file
    neo4j_uri, neo4j_user, neo4j_password = read_neo4j_env_vars()
    
    if not all([neo4j_uri, neo4j_user, neo4j_password]):
        print("âŒ Neo4j environment variables not found. Please set NEO4J_URI, NEO4J_USER, and NEO4J_PASSWORD in your environment or .env file.")
        return
    
    try:
        # Connect to Neo4j
        driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))
        
        print(f"ðŸ”Œ Connecting to Neo4j: {neo4j_uri}")
        
        with driver.session() as session:
            print("âœ… Successfully connected to Neo4j")
            
            # First, get all available labels in the database
            result = session.run("CALL db.labels() YIELD label RETURN label")
            available_labels = [record["label"] for record in result]
            
            print(f"\nðŸ“Š Database contains {len(available_labels)} node types:")
            for label in sorted(available_labels):
                print(f"   â€¢ {label}")
            
            # Check different node types (only if they exist)
            node_types = ["Repository", "File", "Class", "Method", "Function", "Attribute"]
            
            print(f"\nðŸ“ˆ Node counts by type:")
            print("â”€" * 40)
            
            total_nodes = 0
            for node_type in node_types:
                if node_type in available_labels:
                    result = session.run(f"MATCH (n:{node_type}) RETURN count(n) as count")
                    count = result.single()["count"]
                    total_nodes += count
                    print(f"  {node_type:12} : {count:>6}")
                else:
                    print(f"  {node_type:12} : {0:>6} (not found)")
            
            print("â”€" * 40)
            print(f"  {'Total':12} : {total_nodes:>6}")
            
            # Show repository details if any exist
            if "Repository" in available_labels:
                print(f"\nðŸ“š Repository Details:")
                print("â”€" * 40)
                result = session.run("""
                    MATCH (r:Repository) 
                    RETURN r.name as name
                    ORDER BY r.name
                """)
                repos = list(result)
                
                if repos:
                    for i, repo in enumerate(repos, 1):
                        name = repo["name"] or "Unknown"
                        print(f"  {i}. {name}")
                    print()
                else:
                    print("  No repositories found")
            
            # Additional check for any other nodes
            result = session.run("MATCH (n) RETURN count(n) as count")
            actual_total = result.single()["count"]
            
            if actual_total != total_nodes:
                print(f"\nâš ï¸  Note: Total nodes in database ({actual_total}) differs from sum above ({total_nodes})")
                print("   This may indicate nodes with other labels or unlabeled nodes.")
                print("   ðŸ’¡ Tip: These might be memory-related nodes from conversational system.")
                print("      â€¢ Check: python utils/clean_neo4j_memory.py --mode stats")
                print("      â€¢ Preview deletion: python utils/clean_neo4j_memory.py --mode memory")
                print("      â€¢ Actually clean: python utils/clean_neo4j_memory.py --mode memory --execute")
            
        print("\nâœ… Check completed")
        
    except Exception as e:
        print(f"âŒ Error: {e}")
    finally:
        if 'driver' in locals():
            driver.close()

def main():
    """Main function"""
    print("ðŸ—„ï¸  Neo4j Database Status Check")
    print("=" * 50)
    asyncio.run(check_neo4j_clean())

if __name__ == "__main__":
    main() 
```


================================================================================
=== FILE: utils\check_supabase_connection.py ===
================================================================================

```python
"""
Check Supabase connection and table status
"""

import os
import psycopg2
from psycopg2 import sql

print("="*80)
print("SUPABASE CONNECTION CHECK")
print("="*80)

connection_string = os.getenv('SUPABASE_DATABASE_URL')

if not connection_string:
    print("âŒ SUPABASE_DATABASE_URL not set")
    exit(1)

try:
    print("\n1ï¸âƒ£  Connecting to Supabase...")
    conn = psycopg2.connect(connection_string)
    cursor = conn.cursor()
    print("âœ… Connected successfully")

    # Check if conversational_memories table exists (check both public and vecs schemas)
    print("\n2ï¸âƒ£  Checking for 'conversational_memories' table...")

    # Check in vecs schema (mem0 with Supabase uses vecs extension)
    cursor.execute("""
        SELECT table_schema, table_name
        FROM information_schema.tables
        WHERE table_name = 'conversational_memories'
    """)
    results = cursor.fetchall()

    if results:
        for schema, table_name in results:
            print(f"âœ… Table '{table_name}' exists in schema '{schema}'")

            # Count records
            cursor.execute(f"SELECT COUNT(*) FROM {schema}.{table_name}")
            count = cursor.fetchone()[0]
            print(f"   Records in table: {count}")

            # Show sample
            if count > 0:
                cursor.execute(f"SELECT * FROM {schema}.{table_name} LIMIT 3")
                columns = [desc[0] for desc in cursor.description]
                print(f"   Columns: {columns}")
                print("\n   Sample records:")
                for row in cursor.fetchall():
                    print(f"   - {row[:3]}...")  # Show first 3 fields
    else:
        print("âŒ Table 'conversational_memories' does NOT exist in any schema")
        print("\n   Mem0 should create this table automatically.")
        print("   If it doesn't exist, mem0 might be failing silently.")

    # Check for mem0-related tables
    print("\n3ï¸âƒ£  Checking for other mem0 tables...")
    cursor.execute("""
        SELECT table_schema, table_name
        FROM information_schema.tables
        WHERE table_name LIKE '%mem%'
        AND table_schema NOT IN ('pg_catalog', 'information_schema')
        ORDER BY table_schema, table_name
    """)
    mem_tables = cursor.fetchall()

    if mem_tables:
        print("   Found mem0-related tables:")
        for schema, table_name in mem_tables:
            cursor.execute(f"SELECT COUNT(*) FROM {schema}.{table_name}")
            count = cursor.fetchone()[0]
            print(f"   - {schema}.{table_name}: {count} records")
    else:
        print("   âš ï¸  No mem0-related tables found")

    # List all tables by schema
    print("\n4ï¸âƒ£  All tables by schema:")
    cursor.execute("""
        SELECT table_schema, table_name
        FROM information_schema.tables
        WHERE table_schema IN ('public', 'vecs')
        ORDER BY table_schema, table_name
    """)
    all_tables = cursor.fetchall()

    current_schema = None
    for schema, table_name in all_tables[:30]:  # Show first 30
        if schema != current_schema:
            print(f"\n   Schema: {schema}")
            current_schema = schema
        print(f"   - {table_name}")

    if len(all_tables) > 30:
        print(f"\n   ... and {len(all_tables) - 30} more tables")

    cursor.close()
    conn.close()

except Exception as e:
    print(f"âŒ Error: {e}")
    import traceback
    traceback.print_exc()

print("\n" + "="*80)

```


================================================================================
=== FILE: utils\clean_neo4j_memory.py ===
================================================================================

```python
"""
Clean Neo4j Memory Data

Safely cleans memory data from conversational system without affecting
research-server code structure data.

Usage:
    python clean_neo4j_memory.py --mode [memory|all|user]

Modes:
    memory: Clean only memory nodes (nodes with user_id property) - SAFE
    all:    Clean ALL data including research-server code structure - DANGEROUS
    user:   Clean only specific user's memory data
"""

import os
import argparse
from neo4j import GraphDatabase


def get_neo4j_connection():
    """Get Neo4j connection details"""
    url = os.getenv('NEO4J_URI', 'bolt://localhost:7687')
    username = os.getenv('NEO4J_USER', 'neo4j')
    password = os.getenv('NEO4J_PASSWORD', 'password')
    return url, username, password


def clean_memory_only(driver, dry_run=True):
    """
    Clean only memory nodes (nodes with user_id property).
    Safe - won't affect research-server code structure.
    """
    print("\n" + "="*80)
    print("CLEANING MEMORY DATA ONLY")
    print("="*80)

    with driver.session() as session:
        # Count memory nodes
        result = session.run("""
            MATCH (n)
            WHERE n.user_id IS NOT NULL
            RETURN count(n) as count
        """)
        memory_count = result.single()['count']

        # Count memory relationships
        result = session.run("""
            MATCH (a)-[r]-(b)
            WHERE a.user_id IS NOT NULL OR b.user_id IS NOT NULL
            RETURN count(r) as count
        """)
        rel_count = result.single()['count']

        print(f"\nðŸ“Š Memory data to delete:")
        print(f"   - Nodes: {memory_count}")
        print(f"   - Relationships: {rel_count}")

        if dry_run:
            print("\nâš ï¸  DRY RUN - No data will be deleted")
            print("   Run with --execute to actually delete")

            # Show sample
            result = session.run("""
                MATCH (n)
                WHERE n.user_id IS NOT NULL
                RETURN labels(n) as labels, n.user_id as user_id, n.name as name
                LIMIT 10
            """)
            print("\n   Sample nodes to be deleted:")
            for record in result:
                print(f"   - [{record['labels'][0]}] {record['name']} (user: {record['user_id']})")

            return

        # Actually delete
        print("\nðŸ—‘ï¸  Deleting memory data...")

        # Delete relationships first
        session.run("""
            MATCH (a)-[r]-(b)
            WHERE a.user_id IS NOT NULL OR b.user_id IS NOT NULL
            DELETE r
        """)
        print(f"   âœ… Deleted {rel_count} relationships")

        # Delete nodes
        session.run("""
            MATCH (n)
            WHERE n.user_id IS NOT NULL
            DELETE n
        """)
        print(f"   âœ… Deleted {memory_count} nodes")

        print("\nâœ… Memory data cleaned successfully!")
        print("   Research-server code structure data is intact.")


def clean_specific_user(driver, user_id, dry_run=True):
    """Clean only specific user's memory data"""
    print("\n" + "="*80)
    print(f"CLEANING USER: {user_id}")
    print("="*80)

    with driver.session() as session:
        # Count user nodes
        result = session.run("""
            MATCH (n)
            WHERE n.user_id = $user_id
            RETURN count(n) as count
        """, user_id=user_id)
        node_count = result.single()['count']

        # Count relationships
        result = session.run("""
            MATCH (a)-[r]-(b)
            WHERE a.user_id = $user_id OR b.user_id = $user_id
            RETURN count(r) as count
        """, user_id=user_id)
        rel_count = result.single()['count']

        print(f"\nðŸ“Š Data to delete for user '{user_id}':")
        print(f"   - Nodes: {node_count}")
        print(f"   - Relationships: {rel_count}")

        if node_count == 0:
            print(f"\nâš ï¸  No data found for user: {user_id}")
            return

        if dry_run:
            print("\nâš ï¸  DRY RUN - No data will be deleted")
            print("   Run with --execute to actually delete")

            # Show sample
            result = session.run("""
                MATCH (n)
                WHERE n.user_id = $user_id
                RETURN labels(n) as labels, n.name as name, n.mentions as mentions
                ORDER BY n.mentions DESC
                LIMIT 10
            """, user_id=user_id)
            print("\n   Sample nodes to be deleted:")
            for record in result:
                mentions = record.get('mentions', 'N/A')
                print(f"   - [{record['labels'][0]}] {record['name']} (mentioned {mentions}x)")

            return

        # Actually delete
        print(f"\nðŸ—‘ï¸  Deleting data for user '{user_id}'...")

        # Delete relationships
        session.run("""
            MATCH (a)-[r]-(b)
            WHERE a.user_id = $user_id OR b.user_id = $user_id
            DELETE r
        """, user_id=user_id)
        print(f"   âœ… Deleted {rel_count} relationships")

        # Delete nodes
        session.run("""
            MATCH (n)
            WHERE n.user_id = $user_id
            DELETE n
        """, user_id=user_id)
        print(f"   âœ… Deleted {node_count} nodes")

        print(f"\nâœ… User '{user_id}' data cleaned successfully!")


def clean_all(driver, dry_run=True):
    """
    Clean ALL Neo4j data - DANGEROUS!
    This will delete research-server code structure too!
    """
    print("\n" + "="*80)
    print("âš ï¸  DANGER: CLEANING ALL DATA")
    print("="*80)

    with driver.session() as session:
        # Count everything
        result = session.run("MATCH (n) RETURN count(n) as count")
        node_count = result.single()['count']

        result = session.run("MATCH ()-[r]->() RETURN count(r) as count")
        rel_count = result.single()['count']

        print(f"\nðŸ“Š ALL data to delete:")
        print(f"   - Nodes: {node_count}")
        print(f"   - Relationships: {rel_count}")
        print("\nâš ï¸  WARNING: This includes research-server code structure data!")

        if dry_run:
            print("\nâš ï¸  DRY RUN - No data will be deleted")
            print("   Run with --execute to actually delete")

            # Show node types
            result = session.run("""
                MATCH (n)
                RETURN DISTINCT labels(n) as labels, count(*) as count
                ORDER BY count DESC
            """)
            print("\n   Node types:")
            for record in result:
                print(f"   - {record['labels'][0]}: {record['count']} nodes")

            return

        # Require confirmation
        print("\nâš ï¸  Are you ABSOLUTELY SURE you want to delete ALL data?")
        print("   This will delete research-server code structure!")
        confirm = input("   Type 'DELETE ALL' to confirm: ")

        if confirm != "DELETE ALL":
            print("\nâŒ Cancelled")
            return

        # Delete everything
        print("\nðŸ—‘ï¸  Deleting ALL data...")

        session.run("MATCH (n) DETACH DELETE n")
        print(f"   âœ… Deleted {node_count} nodes and {rel_count} relationships")

        print("\nâœ… All data deleted!")


def show_stats(driver):
    """Show current Neo4j statistics"""
    print("\n" + "="*80)
    print("NEO4J DATABASE STATISTICS")
    print("="*80)

    with driver.session() as session:
        # Total nodes
        result = session.run("MATCH (n) RETURN count(n) as count")
        total_nodes = result.single()['count']

        # Memory nodes
        result = session.run("""
            MATCH (n)
            WHERE n.user_id IS NOT NULL
            RETURN count(n) as count
        """)
        memory_nodes = result.single()['count']

        # Code structure nodes
        code_nodes = total_nodes - memory_nodes

        # Node types
        result = session.run("""
            MATCH (n)
            RETURN DISTINCT labels(n)[0] as label, count(*) as count
            ORDER BY count DESC
            LIMIT 10
        """)

        print(f"\nðŸ“Š Total nodes: {total_nodes}")
        print(f"   - Memory data (with user_id): {memory_nodes}")
        print(f"   - Code structure data: {code_nodes}")

        print(f"\nðŸ“‹ Top node types:")
        for record in result:
            label = record['label']
            count = record['count']
            print(f"   - {label}: {count} nodes")

        # Users
        result = session.run("""
            MATCH (n)
            WHERE n.user_id IS NOT NULL
            RETURN DISTINCT n.user_id as user_id, count(*) as count
            ORDER BY count DESC
        """)

        users = list(result)
        if users:
            print(f"\nðŸ‘¥ Memory users:")
            for record in users:
                print(f"   - {record['user_id']}: {record['count']} nodes")


def main():
    parser = argparse.ArgumentParser(description="Clean Neo4j memory data")
    parser.add_argument(
        '--mode',
        choices=['memory', 'all', 'user', 'stats'],
        default='stats',
        help='Cleaning mode: memory (safe), all (dangerous), user (specific), stats (show only)'
    )
    parser.add_argument(
        '--user-id',
        type=str,
        help='User ID to clean (required for --mode user)'
    )
    parser.add_argument(
        '--execute',
        action='store_true',
        help='Actually execute deletion (default is dry-run)'
    )

    args = parser.parse_args()

    # Get connection
    url, username, password = get_neo4j_connection()

    print("="*80)
    print("NEO4J MEMORY CLEANER")
    print("="*80)
    print(f"Connecting to: {url}")
    print(f"Username: {username}")

    try:
        driver = GraphDatabase.driver(url, auth=(username, password))
        driver.verify_connectivity()
        print("âœ… Connected successfully")

        # Show stats first
        if args.mode == 'stats':
            show_stats(driver)
        elif args.mode == 'memory':
            show_stats(driver)
            clean_memory_only(driver, dry_run=not args.execute)
        elif args.mode == 'user':
            if not args.user_id:
                print("\nâŒ Error: --user-id required for --mode user")
                return
            show_stats(driver)
            clean_specific_user(driver, args.user_id, dry_run=not args.execute)
        elif args.mode == 'all':
            show_stats(driver)
            clean_all(driver, dry_run=not args.execute)

        driver.close()

    except Exception as e:
        print(f"\nâŒ Error: {e}")
        print("\nTroubleshooting:")
        print("  1. Check Neo4j is running")
        print("  2. Check environment variables: NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD")
        print("  3. Check credentials are correct")


if __name__ == "__main__":
    main()

```


================================================================================
=== FILE: utils\clean_neo4j_repo.py ===
================================================================================

```python
#!/usr/bin/env python3
"""
Clear code structure nodes and relationships in Neo4j (research-server data)
This script only deletes nodes WITHOUT user_id property (code structure).
Memory data from conversational system (nodes with user_id) will be preserved.
"""
import asyncio
import sys
import os
from pathlib import Path

def get_project_root():
    """Get the project root directory by looking for .git directory"""
    current_path = Path(__file__).resolve()
    # Navigate up to find project root (directory containing .git)
    while not (current_path / ".git").exists() and current_path.parent != current_path:
        current_path = current_path.parent
    if (current_path / ".git").exists():
        return current_path
    else:
        raise FileNotFoundError("Could not find project root (no .git directory found)")

def read_neo4j_env_vars():
    """Read Neo4j environment variables from environment or .env file"""
    # First try to get from environment variables
    neo4j_uri = os.environ.get('NEO4J_URI')
    neo4j_user = os.environ.get('NEO4J_USER')
    neo4j_password = os.environ.get('NEO4J_PASSWORD')
    
    # If not found in environment, try .env file
    if not all([neo4j_uri, neo4j_user, neo4j_password]):
        project_root = get_project_root()
        env_path = project_root / "mcp_servers_and_tools/research_server" / ".env"
        
        if env_path.exists():
            with open(env_path, "r") as f:
                for line in f:
                    line = line.strip()
                    if line.startswith("NEO4J_URI=") and not neo4j_uri:
                        neo4j_uri = line.split("=", 1)[1].strip()
                    elif line.startswith("NEO4J_USER=") and not neo4j_user:
                        neo4j_user = line.split("=", 1)[1].strip()
                    elif line.startswith("NEO4J_PASSWORD=") and not neo4j_password:
                        neo4j_password = line.split("=", 1)[1].strip()
    
    return neo4j_uri, neo4j_user, neo4j_password

async def clean_neo4j_all():
    neo4j_uri, neo4j_user, neo4j_password = read_neo4j_env_vars()
    
    if not all([neo4j_uri, neo4j_user, neo4j_password]):
        raise Exception("Neo4j environment variables not found. Please set NEO4J_URI, NEO4J_USER, and NEO4J_PASSWORD in your environment or .env file.")
    
    print(f"Connecting to Neo4j: {neo4j_uri}")
    project_root = get_project_root()
    sys.path.append(str(project_root / "mcp_servers_and_tools/research_server" / "knowledge_graphs"))
    from parse_repo_into_neo4j import DirectNeo4jExtractor
    extractor = DirectNeo4jExtractor(neo4j_uri, neo4j_user, neo4j_password)
    await extractor.initialize()
    print("âœ“ Successfully connected to Neo4j")
    async with extractor.driver.session() as session:
        print("Starting to clear code structure nodes (without user_id property)...")
        await session.run("MATCH (n) WHERE n.user_id IS NULL DETACH DELETE n")
        print("âœ“ Code structure nodes and relationships have been cleared")
        print("âœ“ Memory data (nodes with user_id) has been preserved")
        print("\nðŸ’¡ To clean memory data from conversational system:")
        print("   â€¢ Check: python utils/clean_neo4j_memory.py --mode stats")
        print("   â€¢ Preview deletion: python utils/clean_neo4j_memory.py --mode memory")
        print("   â€¢ Actually clean: python utils/clean_neo4j_memory.py --mode memory --execute")
    await extractor.close()
    print("\nâœ“ Database connection closed")

def main():
    print("Clear Neo4j code structure data (research-server)")
    print("="*50)
    asyncio.run(clean_neo4j_all())

if __name__ == "__main__":
    main() 
```


================================================================================
=== FILE: utils\mcp_server_manager.py ===
================================================================================

```python
#!/usr/bin/env python3
"""
MCP Server Manager - Manages MCP server connections with reuse capability.
"""

import os
import asyncio
import psutil
import time
from typing import Dict, List, Optional, Tuple
from agents.mcp import MCPServerStdio
from .retry_utils import retry_mcp_server_connect

class MCPServerManager:
    """Manages MCP server connections with reuse capability."""
    
    def __init__(self):
        self._servers: Dict[str, MCPServerStdio] = {}
        self._server_info: Dict[str, Dict] = {}
        self._lock = asyncio.Lock()
        self._working_dir = os.getcwd()  # Track the working directory
    
    async def get_or_create_server(
        self, 
        server_name: str, 
        server_config: Dict,
        check_existing: bool = True,
        working_dir: str = None
    ) -> MCPServerStdio:
        """
        Get existing server or create new one.
        
        Args:
            server_name: Name of the server
            server_config: Configuration for creating the server
            check_existing: Whether to check for existing servers first
            working_dir: Working directory for isolation (defaults to current working directory)
        
        Returns:
            MCPServerStdio instance
        """
        if working_dir is None:
            working_dir = os.getcwd()
        
        # Create a unique key that includes the working directory
        server_key = f"{server_name}:{working_dir}"
        
        async with self._lock:
            # Check if we already have a server instance for this working directory
            if server_key in self._servers:
                server = self._servers[server_key]
                try:
                    # Test if the server is still responsive
                    await self._test_server_connection(server)
                    # print(f"âœ… Reusing existing {server_name} server (working_dir: {working_dir})")
                    return server
                except Exception as e:
                    print(f"âš ï¸  Existing {server_name} server not responsive: {e}")
                    # Remove the unresponsive server
                    del self._servers[server_key]
                    if server_key in self._server_info:
                        del self._server_info[server_key]
            
            # Disabled existing process detection to avoid confusion
            # Each agent will create its own servers, but agents within the same process can reuse them
            
            # Create new server
            # print(f"ðŸš€ Creating new {server_name} server (working_dir: {working_dir})...")
            server = MCPServerStdio(**server_config)
            await retry_mcp_server_connect(server, max_retries=3)
            
            # Store the server
            self._servers[server_key] = server
            self._server_info[server_key] = {
                'pid': await self._get_server_pid(server_name, server_config),
                'created_time': time.time(),
                'reused': False,
                'working_dir': working_dir
            }
            
            # print(f"âœ… Created new {server_name} server (working_dir: {working_dir})")
            return server
    
    async def _test_server_connection(self, server: MCPServerStdio) -> bool:
        """Test if a server connection is still responsive."""
        try:
            # Try to list tools as a connection test
            # This is a lightweight operation that should work if the server is responsive
            tools = await server.list_tools()
            return True
        except Exception:
            return False
    
    async def _find_existing_server_process(
        self, 
        server_name: str, 
        server_config: Dict,
        working_dir: str
    ) -> Optional[int]:
        """Find existing server process by name and configuration."""
        try:
            current_pid = os.getpid()
            current_process = psutil.Process(current_pid)
            
            for proc in psutil.process_iter(['pid', 'name', 'cmdline', 'create_time', 'ppid']):
                try:
                    if not proc.info['cmdline']:
                        continue
                    
                    cmdline = ' '.join(proc.info['cmdline'])
                    
                    # Check if this is the right type of server
                    if not self._matches_server_type(server_name, cmdline, server_config):
                        continue
                    
                    proc_obj = psutil.Process(proc.info['pid'])
                    
                    # Check if it's a recent process (started within last 5 minutes)
                    current_create_time = current_process.create_time()
                    proc_create_time = proc_obj.create_time()
                    time_diff = abs(proc_create_time - current_create_time)
                    
                    if time_diff > 300:  # 5 minutes
                        continue
                    
                    # Check if it's in the same process group, is a child, or is a sibling process
                    try:
                        current_pgid = os.getpgid(current_pid)
                        proc_pgid = os.getpgid(proc.info['pid'])
                        same_process_group = current_pgid == proc_pgid
                        is_child = proc.info['ppid'] == current_pid
                        
                        # For MCP servers, we want to detect servers started by any process in the same working directory
                        # This allows reuse across different single_question_runner processes
                        try:
                            proc_cwd = proc_obj.cwd()
                            if proc_cwd == working_dir:
                                # Accept if same working directory, regardless of process group
                                return proc.info['pid']
                        except (psutil.NoSuchProcess, psutil.AccessDenied):
                            # Fall back to process group check if we can't get working directory
                            if same_process_group or is_child:
                                return proc.info['pid']
                    except (OSError, ProcessLookupError):
                        continue
                        
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    continue
                    
        except Exception as e:
            print(f"âš ï¸  Error finding existing server process: {e}")
        
        return None
    
    def _matches_server_type(self, server_name: str, cmdline: str, server_config: Dict) -> bool:
        """Check if a process matches the server type we're looking for."""
        cmdline_lower = cmdline.lower()
        
        if server_name == "mcp_servers_and_tools/workspace_server":
            return "mcp_servers_and_tools/workspace_server" in cmdline_lower and "index.js" in cmdline_lower
        elif server_name == "tavily-search":
            return "tavily-mcp" in cmdline_lower
        elif server_name == "mcp_servers_and_tools/research_server":
            return "research_mcp.py" in cmdline_lower
        else:
            return server_name.lower() in cmdline_lower
    
    async def _connect_to_existing_process(
        self, 
        server_name: str, 
        server_config: Dict, 
        pid: int
    ) -> MCPServerStdio:
        """Try to connect to an existing server process."""
        # For now, we'll create a new connection
        # In the future, we could implement direct connection to existing processes
        server = MCPServerStdio(**server_config)
        await retry_mcp_server_connect(server, max_retries=2)  # Fewer retries for existing processes
        return server
    
    async def _get_server_pid(self, server_name: str, server_config: Dict) -> Optional[int]:
        """Get the PID of a newly created server."""
        try:
            # Wait a moment for the server to start
            await asyncio.sleep(0.5)
            
            # Find the most recent process that matches our server type
            current_pid = os.getpid()
            current_process = psutil.Process(current_pid)
            
            for proc in psutil.process_iter(['pid', 'name', 'cmdline', 'create_time']):
                try:
                    if not proc.info['cmdline']:
                        continue
                    
                    cmdline = ' '.join(proc.info['cmdline'])
                    
                    if not self._matches_server_type(server_name, cmdline, server_config):
                        continue
                    
                    proc_obj = psutil.Process(proc.info['pid'])
                    
                    # Check if it's a recent process (started within last 10 seconds)
                    current_create_time = current_process.create_time()
                    proc_create_time = proc_obj.create_time()
                    time_diff = abs(proc_create_time - current_create_time)
                    
                    if time_diff <= 10:  # 10 seconds
                        return proc.info['pid']
                        
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    continue
                    
        except Exception as e:
            print(f"âš ï¸  Error getting server PID: {e}")
        
        return None
    
    def get_server_info(self) -> Dict[str, Dict]:
        """Get information about all managed servers."""
        return self._server_info.copy()
    
    async def cleanup(self):
        """Clean up all managed servers."""
        async with self._lock:
            for server_name, server in self._servers.items():
                try:
                    # Close the connection
                    if hasattr(server, 'close'):
                        await server.close()
                except Exception as e:
                    print(f"âš ï¸  Error closing {server_name}: {e}")
            
            self._servers.clear()
            self._server_info.clear()

# Global instance
_mcp_manager = MCPServerManager()

async def get_or_create_mcp_server(server_name: str, server_config: Dict, working_dir: str = None) -> MCPServerStdio:
    """Global function to get or create MCP server."""
    return await _mcp_manager.get_or_create_server(server_name, server_config, working_dir=working_dir)

async def cleanup_mcp_servers():
    """Global function to cleanup MCP servers."""
    await _mcp_manager.cleanup()

def get_mcp_server_info() -> Dict[str, Dict]:
    """Global function to get MCP server information."""
    return _mcp_manager.get_server_info()

```


================================================================================
=== FILE: utils\quiet_utils.py ===
================================================================================

```python
import os
import sys
import io
import logging
import atexit
import signal
import contextlib

_SHUTDOWN_DETECTED = False

def _set_shutdown_flag():
    """Set the global shutdown flag on exit."""
    global _SHUTDOWN_DETECTED
    _SHUTDOWN_DETECTED = True

def _aggressive_exit_handler(sig, frame):
    """Signal handler for SIGINT/SIGTERM. Sets the global shutdown flag."""
    global _SHUTDOWN_DETECTED
    print("\nSignal received, initiating shutdown...")
    _SHUTDOWN_DETECTED = True
    sys.exit(0)

class _SilentStderr(io.TextIOWrapper):
    """
    Suppresses stderr output once shutdown begins.
    Detects relevant error patterns to avoid noisy output during shutdown.
    """
    def __init__(self, target: io.TextIOBase):
        super().__init__(target.buffer, target.encoding, target.errors)
        self._target = target
        self._shutdown_patterns = {
            "asyncio.run() shutdown",
            "Attempted to exit cancel scope",
            "unhandled exception during",
            "an error occurred during closing",
            "BaseExceptionGroup",
            "RuntimeError: Attempted to exit cancel scope",
            "GeneratorExit",
            "CancelledError",
            "TaskGroup",
            "async_generator_athrow",
            "stdio_client",
            "anyio/_backends/_asyncio.py",
            "mcp/client/stdio/__init__.py",
            "Loop that handles pid",
            "is closed",
            "Exception Group Traceback",
            "During handling of the above exception",
            "Traceback (most recent call last)",
            "yield",
            "await",
            "async with",
            "self.close()",
            "call_soon(",
            "_check_closed()",
            "BaseSubprocessTransport.__del__",
            "Event loop unavailable",
            "Exception ignored",
            "RuntimeError: Event loop is closed",
            "Task group cancellation",
            "cancel scope in a different task",
            "/asyncio/",
            "Loop <_UnixSelectorEventLoop running=False closed=True debug=False>",
            "that handles pid",
        }

    def write(self, text):  # type: ignore[override]
        global _SHUTDOWN_DETECTED
        if _SHUTDOWN_DETECTED:
            return len(text)
        text_lower = text.lower()
        if any(pattern.lower() in text_lower for pattern in self._shutdown_patterns):
            _SHUTDOWN_DETECTED = True
            return len(text)
        return self._target.write(text)

    def flush(self):  # type: ignore[override]
        if not _SHUTDOWN_DETECTED:
            self._target.flush()

def configure_logging():
    """Configure logging and environment variables for the application."""
    os.environ["MCP_QUIET"] = "1"
    os.environ["NODE_ENV"] = "production"
    os.environ["PYTHONWARNINGS"] = "ignore"
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    os.environ["OPENAI_LOG"] = "none"
    os.environ["PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD"] = "1"
    os.environ["PYTHONUNBUFFERED"] = "1"
    os.environ["PYTHONIOENCODING"] = "utf-8"
    os.environ["PYTHONHASHSEED"] = "0"
    os.environ["PYTHONPATH"] = os.getcwd()
    # Additional MCP quiet settings
    os.environ["MCP_LOG_LEVEL"] = "ERROR"
    os.environ["MCP_VERBOSE"] = "0"
    # Neo4j quiet settings
    os.environ["NEO4J_LOG_LEVEL"] = "ERROR"
    # Configure OpenAI Agents SDK tracing for full detail capture
    os.environ["OPENAI_AGENTS_DISABLE_TRACING"] = "0"  # Keep tracing enabled
    # Remove trace filtering to allow full granularity for MLflow autolog
    os.environ.pop("OPENAI_AGENTS_TRACE_FILTER", None)
    os.environ.pop("OPENAI_AGENTS_TRACE_LEVEL", None)
    os.environ["OPENAI_LOG_LEVEL"] = "WARNING"  # Reduce OpenAI SDK logging noise

    # Set root logger to ERROR level and configure basic logging
    logging.basicConfig(level=logging.ERROR, format='%(levelname)s:%(name)s:%(message)s')
    
    # Configure specific noisy loggers to ERROR level (expanded list)
    noisy_loggers = [
        # HTTP and network
        "httpx", "uvicorn", "urllib3", "requests",
        
        # OpenAI and AI libraries
        "openai", "openai._client", "_client",
        
        # MCP related
        "mcp", "mcp.client", "mcp.server", "mcp.utils", "mcp.rag",
        "mcp.client.stdio", "mcp.server.lowlevel", "mcp.server.lowlevel.server",
        "mcp_servers_and_tools/workspace_server",
        
        # Async and concurrency
        "asyncio", "anyio", "concurrent.futures",
        
        # ML and transformers
        "torch", "transformers", "tokenizers", "numpy", "pandas",
        "sentence_transformers", "sentence_transformers.cross_encoder", 
        "sentence_transformers.cross_encoder.CrossEncoder",
        
        # Neo4j and knowledge graph (more comprehensive)
        "neo4j", "neo4j.notifications", "neo4j.driver", "neo4j.work", "neo4j.pool",
        "neo4j.io", "neo4j.time", "neo4j.spatial", "neo4j.graph",
        "knowledge_graph_validator", "parse_repo_into_neo4j", "ai_hallucination_detector",
        "ai_script_analyzer", "hallucination_reporter",
        
        # Web scraping and automation
        "selenium", "crawl4ai", "Tavily", "server",
        
        # Database
        "supabase",
        
        # Other noisy loggers
        "CrossEncoder",
    ]
    
    # More aggressive silencing - set to CRITICAL level for extremely noisy loggers
    critical_silent_loggers = [
        "sentence_transformers.cross_encoder.CrossEncoder",
        "knowledge_graph_validator", 
        "parse_repo_into_neo4j",
        "neo4j.notifications",
        "mcp.server.lowlevel.server",
        "httpx",
        "ai_hallucination_detector",
    ]
    
    for logger_name in noisy_loggers:
        logger = logging.getLogger(logger_name)
        if logger_name in critical_silent_loggers:
            logger.setLevel(logging.CRITICAL)  # Even more silent
        else:
            logger.setLevel(logging.ERROR)
        logger.propagate = False
        # Remove all existing handlers to ensure our settings take effect
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)
    
    # Override getLogger to automatically configure noisy loggers (enhanced)
    original_getLogger = logging.getLogger
    
    def quiet_getLogger(name=None):
        logger = original_getLogger(name)
        if name:
            name_lower = name.lower()
            # Enhanced pattern matching for automatic silencing
            silent_patterns = [
                "mcp", "sentence_transformers", "crossencoder", "tavily", "neo4j",
                "httpx", "uvicorn", "openai", "asyncio", "anyio", "selenium", "urllib3",
                "knowledge_graph", "parse_repo", "supabase", "transformers", "torch",
                "hallucination", "ai_script"
            ]
            
            if any(pattern in name_lower for pattern in silent_patterns):
                # Extra quiet for the most noisy ones
                if any(critical in name_lower for critical in ["sentence_transformers", "neo4j", "mcp.server", "knowledge_graph", "parse_repo", "hallucination"]):
                    logger.setLevel(logging.CRITICAL)
                else:
                    logger.setLevel(logging.ERROR)
                logger.propagate = False
                # Remove existing handlers
                for handler in logger.handlers[:]:
                    logger.removeHandler(handler)
        return logger
    
    logging.getLogger = quiet_getLogger
    
    # Additional step: Silence the root logger's handlers
    root_logger = logging.getLogger()
    for handler in root_logger.handlers:
        handler.setLevel(logging.ERROR)
    
    # Force Neo4j logging to be quiet
    import warnings
    warnings.filterwarnings("ignore", category=UserWarning, module="neo4j")
    warnings.filterwarnings("ignore", message=".*Neo4j.*")
    warnings.filterwarnings("ignore", message=".*neo4j.*")

def _get_noisy_logger_names():
    """
    Return logger names known to emit irrelevant noise during shutdown.
    """
    return (
        # MCP related
        "MCP", "mcp", "mcp.client", "mcp.server", "mcp.utils", "mcp.rag", 
        "mcp.server.lowlevel", "mcp.server.lowlevel.server",
        
        # Async and system
        "anyio", "memgpt", "asyncio", "concurrent.futures",
        
        # HTTP and network
        "httpx", "uvicorn", "urllib3", "requests",
        
        # ML and AI
        "CrossEncoder", "sentence_transformers", "sentence_transformers.cross_encoder",
        "sentence_transformers.cross_encoder.CrossEncoder", "transformers", "torch",
        "tokenizers", "numpy", "pandas",
        
        # Neo4j and knowledge graph (comprehensive)
        "neo4j", "neo4j.notifications", "neo4j.driver", "neo4j.work", "neo4j.pool",
        "neo4j.io", "neo4j.time", "neo4j.spatial", "neo4j.graph",
        "knowledge_graph_validator", "parse_repo_into_neo4j", "ai_hallucination_detector",
        "ai_script_analyzer", "hallucination_reporter",
        
        # Web and automation
        "Tavily", "selenium", "crawl4ai",
        
        # Database
        "supabase",
        
        # OpenAI
        "openai", "openai._client", "_client"
    )

@contextlib.contextmanager
def silence_external_output(enabled=True):
    """
    Temporarily redirect stdout/stderr and raise loggers to ERROR level.
    Used to suppress noisy output from external libraries.
    """
    if not enabled:
        yield
        return
    new_out, new_err = io.StringIO(), io.StringIO()
    with contextlib.redirect_stdout(new_out), contextlib.redirect_stderr(new_err):
        saved = {}
        for name in _get_noisy_logger_names():
            lg = logging.getLogger(name)
            saved[name] = lg.level
            lg.setLevel(logging.ERROR)
        try:
            yield
        finally:
            for name, lvl in saved.items():
                logging.getLogger(name).setLevel(lvl)

def setup_quiet_mode():
    """
    Set up stderr filtering, logging, signal handlers, and environment variables
    to suppress noisy output and warnings during shutdown.
    Should be called as early as possible in the main script.
    """
    # Install stderr filter (only once)
    if not isinstance(sys.stderr, _SilentStderr):
        sys.stderr = _SilentStderr(sys.stderr)
    # Register shutdown handlers
    atexit.register(_set_shutdown_flag)
    signal.signal(signal.SIGINT, _aggressive_exit_handler)
    signal.signal(signal.SIGTERM, _aggressive_exit_handler)
    # Configure logging and environment
    configure_logging()
    
    # Proactively silence any existing loggers that match our patterns
    import logging
    
    # Get all existing loggers
    existing_loggers = [logging.getLogger(name) for name in logging.Logger.manager.loggerDict]
    
    for logger in existing_loggers:
        if logger.name:
            name_lower = logger.name.lower()
            silent_patterns = [
                "mcp", "sentence_transformers", "crossencoder", "tavily", "neo4j",
                "httpx", "uvicorn", "openai", "asyncio", "anyio", "selenium", "urllib3",
                "knowledge_graph", "parse_repo", "supabase", "transformers", "torch"
            ]
            
            if any(pattern in name_lower for pattern in silent_patterns):
                # Extra quiet for the most noisy ones
                if any(critical in name_lower for critical in ["sentence_transformers", "neo4j", "mcp.server", "knowledge_graph", "parse_repo"]):
                    logger.setLevel(logging.CRITICAL)
                else:
                    logger.setLevel(logging.ERROR)
                logger.propagate = False
                # Remove existing handlers
                for handler in logger.handlers[:]:
                    logger.removeHandler(handler)

def reapply_quiet_mode():
    """
    Reapply quiet mode to any newly created loggers.
    Call this periodically during long-running processes.
    """
    import logging
    
    # Get all existing loggers
    existing_loggers = [logging.getLogger(name) for name in logging.Logger.manager.loggerDict]
    
    for logger in existing_loggers:
        if logger.name:
            name_lower = logger.name.lower()
            silent_patterns = [
                "mcp", "sentence_transformers", "crossencoder", "tavily", "neo4j",
                "httpx", "uvicorn", "openai", "asyncio", "anyio", "selenium", "urllib3",
                "knowledge_graph", "parse_repo", "supabase", "transformers", "torch"
            ]
            
            if any(pattern in name_lower for pattern in silent_patterns):
                # Extra quiet for the most noisy ones
                if any(critical in name_lower for critical in ["sentence_transformers", "neo4j", "mcp.server", "knowledge_graph", "parse_repo"]):
                    logger.setLevel(logging.CRITICAL)
                else:
                    logger.setLevel(logging.ERROR)
                logger.propagate = False
                # Remove existing handlers
                for handler in logger.handlers[:]:
                    logger.removeHandler(handler)

def is_shutdown():
    """Return True if shutdown has been detected."""
    return _SHUTDOWN_DETECTED

```


================================================================================
=== FILE: utils\retry_utils.py ===
================================================================================

```python
import asyncio
from typing import Any, Callable, TypeVar

T = TypeVar('T')

async def retry_with_backoff(
    func: Callable[..., Any],
    max_retries: int = 3,
    initial_delay: float = 2.0,
    max_delay: float = 32.0,
    backoff_factor: float = 2.0,
    *args: Any,
    **kwargs: Any
) -> Any:
    """
    Retry a function with exponential backoff.
    """
    last_exception = None
    delay = initial_delay
    
    for attempt in range(max_retries):
        try:
            return await func(*args, **kwargs)
        except Exception as e:
            last_exception = e
            if attempt < max_retries - 1:
                print(f"âŒ Attempt {attempt + 1} failed: {str(e)}")
                print(f"ðŸ”„ Retrying in {delay} seconds... (attempt {attempt + 2}/{max_retries})")
                await asyncio.sleep(delay)
                delay = min(delay * backoff_factor, max_delay)
            else:
                print(f"âŒ Final attempt failed: {str(e)}")
                print(f"ðŸš« All {max_retries} attempts exhausted")
                raise last_exception

async def call_tool_with_retry(
    tool: Any, 
    *args: Any,
    max_retries: int = 3,
    initial_delay: float = 2.0,
    max_delay: float = 32.0,
    backoff_factor: float = 2.0,
    **kwargs: Any
) -> Any:
    """
    Call a tool with retry mechanism, supporting different tool interfaces.
    """
    # Remove retry-related parameters from kwargs to avoid duplicate passing
    tool_kwargs = {k: v for k, v in kwargs.items() 
                   if k not in ['max_retries', 'initial_delay', 'max_delay', 'backoff_factor']}
    
    # If tool is a function or class method, wrap it directly
    if callable(tool):
        # For instance methods, we need to bind the first argument (self)
        if hasattr(tool, '__self__'):
            async def wrapped_func(*func_args, **func_kwargs):
                return await tool(*func_args, **func_kwargs)
            return await retry_with_backoff(
                wrapped_func, 
                max_retries,
                initial_delay,
                max_delay,
                backoff_factor,
                *args, 
                **tool_kwargs
            )
        else:
            # For regular functions, we can call them directly
            return await retry_with_backoff(
                tool,
                max_retries,
                initial_delay,
                max_delay,
                backoff_factor,
                *args, 
                **tool_kwargs
            )
    
    # Otherwise, check for tool interfaces
    if hasattr(tool, '__call__') and callable(getattr(tool, '__call__')):
        return await retry_with_backoff(
            tool,
            max_retries,
            initial_delay,
            max_delay,
            backoff_factor,
            *args, 
            **tool_kwargs
        )
    elif hasattr(tool, 'call') and callable(getattr(tool, 'call')):
        return await retry_with_backoff(
            tool.call,
            max_retries,
            initial_delay,
            max_delay,
            backoff_factor,
            *args, 
            **tool_kwargs
        )
    elif hasattr(tool, 'execute') and callable(getattr(tool, 'execute')):
        return await retry_with_backoff(
            tool.execute,
            max_retries,
            initial_delay,
            max_delay,
            backoff_factor,
            *args, 
            **tool_kwargs
        )
    else:
        raise Exception(f"Tool {tool} has no callable interface")

async def retry_mcp_server_connect(server, max_retries: int = 3) -> None:
    """
    Retry MCP server connection with exponential backoff.
    Only shows output when there are errors.
    """
    last_exception = None
    delay = 2.0
    
    for attempt in range(max_retries):
        try:
            # Call the connect method directly - silent on success
            return await server.connect()
        except Exception as e:
            last_exception = e
            error_type = type(e).__name__
            
            if attempt < max_retries - 1:
                print(f"âŒ MCP server '{server.name}' connection failed: {error_type} - {str(e)}")
                print(f"ðŸ”„ Retrying connection to '{server.name}' in {delay} seconds... (attempt {attempt + 2}/{max_retries})")
                await asyncio.sleep(delay)
                delay = min(delay * 2.0, 32.0)  # Exponential backoff
            else:
                print(f"âŒ MCP server '{server.name}' final connection attempt failed: {error_type} - {str(e)}")
                print(f"ðŸš« All {max_retries} connection attempts to '{server.name}' failed")
                raise last_exception
```


================================================================================
=== FILE: utils\supabase_utils.py ===
================================================================================

```python
"""
Supabase utilities for research agent.
"""

import os
from supabase import create_client, Client

def get_supabase_client() -> Client:
    """
    Get a Supabase client instance.
    
    Returns:
        Client: A Supabase client instance
    """
    url = os.getenv("SUPABASE_URL")
    key = os.getenv("SUPABASE_SERVICE_KEY")
    
    if not url or not key:
        raise ValueError("SUPABASE_URL and SUPABASE_SERVICE_KEY environment variables must be set")
    
    return create_client(url, key) 

def clear_supabase_tables(supabase_client):
    """Clear the extracted_code table in Supabase."""
    print("ðŸ§¹ Clearing Supabase tables...")
    
    tables_to_clear = [
        # ("crawled_pages", "id", 0),
        # ("code_examples", "id", 0), 
        # ("sources", "source_id", ""),
        ("extracted_code", "id", 0)
    ]
    
    for table_name, id_column, neq_value in tables_to_clear:
        try:
            result = supabase_client.table(table_name).delete().neq(id_column, neq_value).execute()
            count = len(result.data) if result.data else 0
            print(f"   âœ… Cleared {count} records from {table_name} table")
        except Exception as e:
            print(f"   âš ï¸  Error clearing {table_name} table: {e}")
            try:
                if id_column == "source_id":
                    result = supabase_client.table(table_name).delete().gte("created_at", "1970-01-01").execute()
                else:
                    result = supabase_client.table(table_name).delete().gte(id_column, 0).execute()
                count = len(result.data) if result.data else 0
                print(f"   âœ… Cleared {table_name} table using alternative method ({count} records)")
            except Exception as e2:
                print(f"   âŒ Failed to clear {table_name} table: {e2}")

def view_supabase_tables(supabase_client, limit: int = 10):
    """
    View the contents of tables in Supabase.
    
    Args:
        supabase_client: Supabase client instance
        limit: Maximum number of records to show per table (default: 10)
    """
    print("ðŸ“Š Viewing Supabase tables contents...")
    
    # Define table configurations with their primary key columns
    table_configs = [
        ("crawled_pages", "id"),
        ("code_examples", "id"), 
        ("sources", "source_id"),  # sources table uses source_id as primary key
        ("extracted_code", "url")  # extracted_code table uses url as primary key
    ]
    
    for table_name, primary_key in table_configs:
        print(f"\n" + "="*60)
        print(f"Table: {table_name}")
        print("="*60)
        
        try:
            # Get total count using the correct primary key
            count_result = supabase_client.table(table_name).select(primary_key, count="exact").execute()
            total_count = count_result.count if hasattr(count_result, 'count') else len(count_result.data)
            
            print(f"Total records: {total_count}")
            
            if total_count > 0:
                # Get sample records
                result = supabase_client.table(table_name).select("*").limit(limit).execute()
                
                if result.data:
                    print(f"Showing first {len(result.data)} records:")
                    for i, record in enumerate(result.data, 1):
                        print(f"\n--- Record {i} ---")
                        for key, value in record.items():
                            # Skip embedding output to reduce verbosity
                            if key == 'embedding':
                                print(f"  {key}: [vector with {len(value)} dimensions]")
                            elif key == 'content' and isinstance(value, str) and len(value) > 200:
                                print(f"  {key}: {value[:200]}...")
                            elif key == 'code' and isinstance(value, str) and len(value) > 200:
                                print(f"  {key}: {value[:200]}...")
                            else:
                                print(f"  {key}: {value}")
                else:
                    print("No records found")
            else:
                print("Table is empty")
                
        except Exception as e:
            print(f"âŒ Error viewing {table_name} table: {e}")

def get_table_statistics(supabase_client):
    """
    Get statistics about the contents of tables.
    
    Args:
        supabase_client: Supabase client instance
    """
    print("ðŸ“ˆ Supabase tables statistics:")
    print("="*50)
    
    # Define table configurations with their primary key columns
    table_configs = [
        ("crawled_pages", "id"),
        ("code_examples", "id"), 
        ("sources", "source_id"),  # sources table uses source_id as primary key
        ("extracted_code", "url")  # extracted_code table uses url as primary key
    ]
    
    for table_name, primary_key in table_configs:
        try:
            count_result = supabase_client.table(table_name).select(primary_key, count="exact").execute()
            total_count = count_result.count if hasattr(count_result, 'count') else len(count_result.data)
            print(f"  {table_name}: {total_count} records")
        except Exception as e:
            print(f"  {table_name}: Error - {e}")
```


================================================================================
=== FILE: utils\__init__.py ===
================================================================================

```python
#!/usr/bin/env python3
"""
Utils Package
Shared utilities for MCP server management, retry logic, and database operations.
"""

from .mcp_server_manager import get_or_create_mcp_server
from .retry_utils import retry_with_backoff, call_tool_with_retry, retry_mcp_server_connect
from .quiet_utils import setup_quiet_mode, silence_external_output, reapply_quiet_mode
from .supabase_utils import get_supabase_client, clear_supabase_tables

__all__ = [
    # MCP server management
    "get_or_create_mcp_server",
    # Retry utilities
    "retry_with_backoff",
    "call_tool_with_retry",
    "retry_mcp_server_connect",
    # Output suppression
    "setup_quiet_mode",
    "silence_external_output",
    "reapply_quiet_mode",
    # Supabase utilities
    "get_supabase_client",
    "clear_supabase_tables",
]

# Other utilities available via direct import:
# from utils.check_neo4j import ...
# from utils.clean_neo4j_repo import ...
# from utils.clean_neo4j_memory import ...
# from utils.supabase_utils import view_supabase_tables, get_table_statistics
# from utils.check_supabase_connection import ...

```


################################################################################
# MANIFEST
# Included files:
#  - docker-compose.yml
#  - LICENSE
#  - project_context.txt
#  - project_to_txt.py
#  - README.md
#  - conversational_system\ADAPTIVE_AGENTS.md
#  - conversational_system\launch.sh
#  - conversational_system\MLFLOW_TRACING.md
#  - conversational_system\__init__.py
#  - conversational_system\config\prompts.py
#  - conversational_system\config\__init__.py
#  - conversational_system\core\deep_solver.py
#  - conversational_system\core\orchestrator.py
#  - conversational_system\core\__init__.py
#  - conversational_system\deep_solver_with_memory\adaptive_code_agent.py
#  - conversational_system\deep_solver_with_memory\adaptive_debug_agent.py
#  - conversational_system\deep_solver_with_memory\adaptive_solution_researcher.py
#  - conversational_system\deep_solver_with_memory\code_agent.py
#  - conversational_system\deep_solver_with_memory\debug_agent.py
#  - conversational_system\deep_solver_with_memory\output_processor_agent.py
#  - conversational_system\deep_solver_with_memory\output_types.py
#  - conversational_system\deep_solver_with_memory\solution_researcher.py
#  - conversational_system\deep_solver_with_memory\__init__.py
#  - conversational_system\frontend\session_manager.py
#  - conversational_system\frontend\streamlit_app.py
#  - conversational_system\frontend\__init__.py
#  - docker\Dockerfile
#  - docker\entrypoint.sh
#  - docker\requirements-docker.txt
#  - mcp_servers_and_tools\__init__.py
#  - mcp_servers_and_tools\direct_tools\custom_memory_prompts.py
#  - mcp_servers_and_tools\direct_tools\memory_tools.py
#  - mcp_servers_and_tools\direct_tools\research_tools.py
#  - mcp_servers_and_tools\direct_tools\search_tool.py
#  - mcp_servers_and_tools\direct_tools\workspace_tools.py
#  - mcp_servers_and_tools\direct_tools\__init__.py
#  - mcp_servers_and_tools\memory_server\pyproject.toml
#  - mcp_servers_and_tools\memory_server\README.md
#  - mcp_servers_and_tools\memory_server\test_memory_server.py
#  - mcp_servers_and_tools\memory_server\test_memory_server_basic.py
#  - mcp_servers_and_tools\memory_server\src\memory_mcp.py
#  - mcp_servers_and_tools\research_server\extracted_code.sql
#  - mcp_servers_and_tools\research_server\pyproject.toml
#  - mcp_servers_and_tools\research_server\README.md
#  - mcp_servers_and_tools\research_server\test_research_server_extract_code_from_url_and_retrieve.py
#  - mcp_servers_and_tools\research_server\test_research_server_introspect.py
#  - mcp_servers_and_tools\research_server\test_research_server_parse_local_package_and_query_KG.py
#  - mcp_servers_and_tools\research_server\test_research_server_probe.py
#  - mcp_servers_and_tools\research_server\introspection_and_probe\quick_introspect_core.py
#  - mcp_servers_and_tools\research_server\introspection_and_probe\runtime_probe.py
#  - mcp_servers_and_tools\research_server\knowledge_graphs\parse_repo_into_neo4j.py
#  - mcp_servers_and_tools\research_server\src\research_mcp.py
#  - mcp_servers_and_tools\research_server\src\research_server_utils.py
#  - mcp_servers_and_tools\workspace_server\Dockerfile
#  - mcp_servers_and_tools\workspace_server\LICENSE
#  - mcp_servers_and_tools\workspace_server\package.json
#  - mcp_servers_and_tools\workspace_server\README.md
#  - mcp_servers_and_tools\workspace_server\smithery.yaml
#  - mcp_servers_and_tools\workspace_server\test_workspace_security.py
#  - mcp_servers_and_tools\workspace_server\test_workspace_server.py
#  - mcp_servers_and_tools\workspace_server\tsconfig.json
#  - mcp_servers_and_tools\workspace_server\src\index.ts
#  - utils\check_neo4j.py
#  - utils\check_supabase_connection.py
#  - utils\clean_neo4j_memory.py
#  - utils\clean_neo4j_repo.py
#  - utils\mcp_server_manager.py
#  - utils\quiet_utils.py
#  - utils\retry_utils.py
#  - utils\supabase_utils.py
#  - utils\__init__.py
#
# Skips summary:
#  dir: 4
#  hidden: 10
#  size: 0
#  binary: 0
#  ext: 0
#  other: 1
# END
